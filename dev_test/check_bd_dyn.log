[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass RemoveUnusedFunctions
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential RemoveUnusedFunctions
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) {
  nn.softmax(%x)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass ToBasicBlockNormalForm
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential ToBasicBlockNormalForm
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) {
  nn.softmax(%x)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass sequential
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Legalize
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Legalize
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential sequential
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Legalize
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass EtaExpand
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential EtaExpand
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass SimplifyInference
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential SimplifyInference
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass EliminateCommonSubexpr
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass SimplifyExpr
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential SimplifyExpr
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InlinePrimitives
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Inline
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Inline
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass DeadCodeElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential DeadCodeElimination
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InlinePrimitives
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass CombineParallelConv2d
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass CombineParallelDense
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass CombineParallelBatchMatmul
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FoldConstant
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FoldConstant
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FoldScaleAxis
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BackwardFoldScaleAxis
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass ForwardFoldScaleAxis
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FoldConstant
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FoldConstant
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FoldScaleAxis
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass CanonicalizeCast
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass CanonicalizeOps
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass AlterOpLayout
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FastMath
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FoldConstant
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FoldConstant
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FuseOps
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  nn.softmax(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FuseOps
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %0(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass ToANormalForm
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential ToANormalForm
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %x1 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  let %x2 = %x1(%x);
  %x2
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InferType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InferType
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %x1: fn (Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  let %x2: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %x1(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %x2
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass LambdaLift
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential LambdaLift
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %x1: fn (Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  let %x2: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %x1(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %x2
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InlinePrimitives
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Inline
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Inline
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  let %x1: fn (Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %0;
  let %x2: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %0(%x);
  %x2
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass DeadCodeElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential DeadCodeElimination
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  let %x1: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %0(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %x1
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InlinePrimitives
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  let %x1: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %0(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %x1
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InlineGlobals
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InlineGlobals
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  let %x1: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %0(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %x1
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass sequential
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass RemoveUnusedFunctions
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential RemoveUnusedFunctions
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  let %x1: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %0(%x) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %x1
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass ManifestAlloc
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/driver/driver_api.cc:139: Lowering shape_func_nn_softmax
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:319: List all Stages here
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: compute
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i0, range(min=0, ext=4))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:327: END List

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, range(min=0, ext=4)): i0: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, ): i0: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext=4)): range(min=0, ext=4)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, ): range(min=0, ext=4)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectPrefetch
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectPrefetch
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  attr [compute] "realize_scope" = "";
  realize(compute, [0:4], True {
    for (i0: int32, 0, 4) {
      compute[i0] = placeholder[i0]
    }
  })
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageFlatten
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageFlatten
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (i0: int32 >= 0)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (i0: int32 < 4)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (int64*)placeholder: Pointer(int64)[i0: int32]
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (int64*)placeholder: Pointer(int64)[i0: int32]
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify i0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify i0: int32
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (i0: int32 >= 0)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (i0: int32 < 4)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (int64*)placeholder: Pointer(int64)[i0: int32]
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (int64*)placeholder: Pointer(int64)[i0: int32]
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify i0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify i0: int32
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential ManifestAlloc
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  %3 = add(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */;
  %4 = prod(%shape_func_out_0) /* ty=int64 */;
  %5 = divide(%3, 8 /* ty=int64 */) /* ty=int64 */;
  %6 = multiply(%4, %5) /* ty=int64 */;
  let %storage_01: Storage[] = memory.alloc_storage(%6, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %7 = (%x,);
  %8 = (%out_0,);
  let %x1: () = vm.invoke_tvm_op(%0, %7, %8) /* ty=() */;
  let %x2: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x2
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FoldConstant
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FuseOps
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
type Storage {
  
}

def @main() -> int64 {
  add(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FuseOps
type Storage {
  
}

def @main() -> int64 {
  %0 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  %0(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass ToANormalForm
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential ToANormalForm
type Storage {
  
}

def @main() -> int64 {
  let %x = 32 /* ty=int64 */;
  let %x1 = 7 /* ty=int64 */;
  let %x2 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3 = %x2(%x, %x1);
  %x3
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InferType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass EtaExpand
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential EtaExpand
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1)
  };
  let %x3: int64 = %x2(%x, %x1);
  %x3
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InferType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:319: List all Stages here
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: T_add
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:327: END List

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectPrefetch
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectPrefetch
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [T_add] "realize_scope" = "";
  realize(T_add, [], True {
    T_add[] = (placeholder[] + placeholder_1[])
  })
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageFlatten
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageFlatten
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0] + (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0] + (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0] + (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0] + (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.HoistIfThenElse
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerReduction
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerReduction
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.PlanAndUpdateBufferAllocationLocation
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.PlanAndUpdateBufferAllocationLocation
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ConvertBlocksToOpaque
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ConvertBlocksToOpaque
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.CompactBufferAllocation
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.CompactBufferAllocation
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.FlattenBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.FlattenBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0] + (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0] + (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0] + (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0] + (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.HoistIfThenElse
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VerifyMemory
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VerifyMemory
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Apply
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Apply
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InferFragment
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InferFragment
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerThreadAllreduce
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerThreadAllreduce
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.MakePackedAPI
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg2: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg2: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg2: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg2: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (dev_id: int32 == @tir.tvm_struct_get(arg2: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (dev_id: int32 == @tir.tvm_struct_get(arg2: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.SplitHostDevice
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerWarpMemory
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerWarpMemory

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Apply
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Apply
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerTVMBuiltin
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 3)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg2.code: int32 == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg2: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg2: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg2: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg2: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg2: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg2: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.CombineContextCall
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 3)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg2.code: int32 == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg2: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg2: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg2: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg2: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg2: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg2: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FuseOps
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
type Storage {
  
}

def @main() -> int64 {
  divide(39 /* ty=int64 */, 8 /* ty=int64 */) /* ty=int64 */
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FuseOps
type Storage {
  
}

def @main() -> int64 {
  %0 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  %0(39 /* ty=int64 */, 8 /* ty=int64 */) /* ty=int64 */
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass ToANormalForm
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential ToANormalForm
type Storage {
  
}

def @main() -> int64 {
  let %x = 39 /* ty=int64 */;
  let %x1 = 8 /* ty=int64 */;
  let %x2 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3 = %x2(%x, %x1);
  %x3
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InferType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass EtaExpand
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential EtaExpand
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1)
  };
  let %x3: int64 = %x2(%x, %x1);
  %x3
}

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InferType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:319: List all Stages here
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: T_divide
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:327: END List

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectPrefetch
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectPrefetch
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  attr [T_divide] "realize_scope" = "";
  realize(T_divide, [], True {
    T_divide[] = (placeholder[] / placeholder_1[])
  })
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageFlatten
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageFlatten
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0] / (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0] / (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0] / (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0] / (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.HoistIfThenElse
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerReduction
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerReduction
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.PlanAndUpdateBufferAllocationLocation
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.PlanAndUpdateBufferAllocationLocation
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ConvertBlocksToOpaque
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ConvertBlocksToOpaque
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.CompactBufferAllocation
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.CompactBufferAllocation
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.FlattenBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.FlattenBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0] / (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0] / (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0] / (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0] / (int64*)placeholder_1: Pointer(int64)[0])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.HoistIfThenElse
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VerifyMemory
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VerifyMemory
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Apply
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Apply
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InferFragment
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InferFragment
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerThreadAllreduce
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerThreadAllreduce
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.MakePackedAPI
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg2: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg2: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg2: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg2: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (dev_id: int32 == @tir.tvm_struct_get(arg2: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (dev_id: int32 == @tir.tvm_struct_get(arg2: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.SplitHostDevice
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerWarpMemory
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerWarpMemory

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Apply
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Apply
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerTVMBuiltin
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 3)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg2.code: int32 == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg2: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg2: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg2: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg2: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg2: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg2: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.CombineContextCall
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 3)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg2.code: int32 == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg2: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg2: handle, 0, 4, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg2: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg2: handle, 0, 8, dtype=uint64))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg2: handle, 0, 10, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg2: handle, 0, 9, dtype=int32))
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FoldConstant
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  %3 = prod(%shape_func_out_0) /* ty=int64 */;
  %4 = multiply(%3, 4 /* ty=int64 */) /* ty=int64 */;
  let %storage_01: Storage[] = memory.alloc_storage(%4, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %5 = (%x,);
  %6 = (%out_0,);
  let %x1: () = vm.invoke_tvm_op(%0, %5, %6) /* ty=() */;
  let %x2: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x2
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FuseOps
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  %3 = prod(%shape_func_out_0) /* ty=int64 */;
  %4 = multiply(%3, 4 /* ty=int64 */) /* ty=int64 */;
  let %storage_01: Storage[] = memory.alloc_storage(%4, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %5 = (%x,);
  %6 = (%out_0,);
  let %x1: () = vm.invoke_tvm_op(%0, %5, %6) /* ty=() */;
  let %x2: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x2
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FuseOps
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  %3 = fn (%p02: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p02) /* ty=int64 */
  };
  %4 = %3(%shape_func_out_0) /* ty=int64 */;
  %5 = fn (%p01: int64, Primitive=1) -> int64 {
    multiply(%p01, 4 /* ty=int64 */) /* ty=int64 */
  };
  %6 = %5(%4) /* ty=int64 */;
  let %storage_01: Storage[] = memory.alloc_storage(%6, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %7 = (%x,);
  %8 = (%out_0,);
  let %x1: () = vm.invoke_tvm_op(%0, %7, %8) /* ty=() */;
  let %x2: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x2
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass ManifestAlloc
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential ManifestAlloc
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FuseOps
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FuseOps
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FoldConstant
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FoldConstant
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FuseOps
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:468: InferType;
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:470: After this dependency pass
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FuseOps
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass ManifestAlloc
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential ManifestAlloc
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass FoldConstant
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential FoldConstant
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential sequential
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass InferType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential InferType
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1) -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass LabelOps
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential LabelOps
type Storage {
  
}

def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], hash="e101bcf67246f662") -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
  let %in_shape_0: Tensor[(4), int64] = vm.shape_of(%x, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(32 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(4), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(4), int64] */;
  let %shape_func_out_0: Tensor[(4), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32], Primitive=1, hash="45be4dd2fe63b572") -> Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] {
    nn.softmax(%p0) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */
  };
  %1 = (%in_shape_0,);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(4), int64], Primitive=1, hash="75ec56f3169a1497") -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x1: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1, hash="e2e2680f0ff08f46") -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x2: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] */;
  %9 = (%x,);
  %10 = (%out_0,);
  let %x3: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x4: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32] = %out_0;
  %x4
}


[17:51:58] /workspace/home/codes/tvm/src/driver/driver_api.cc:139: Lowering shape_func_nn_softmax_1
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:319: List all Stages here
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: compute
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i0, range(min=0, ext=4))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:327: END List

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, range(min=0, ext=4)): i0: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, ): i0: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext=4)): range(min=0, ext=4)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, ): range(min=0, ext=4)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectPrefetch
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectPrefetch
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  attr [compute] "realize_scope" = "";
  realize(compute, [0:4], True {
    for (i0: int32, 0, 4) {
      compute[i0] = placeholder[i0]
    }
  })
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageFlatten
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageFlatten
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (i0: int32 >= 0)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (i0: int32 < 4)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (int64*)placeholder: Pointer(int64)[i0: int32]
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (int64*)placeholder: Pointer(int64)[i0: int32]
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify i0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify i0: int32
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (i0: int32 >= 0)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (i0: int32 < 4)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (int64*)placeholder: Pointer(int64)[i0: int32]
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (int64*)placeholder: Pointer(int64)[i0: int32]
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify i0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify i0: int32
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:319: List all Stages here
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder_red
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(k0, range(min=0, ext=4))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:327: END List

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k0, range(min=0, ext=4)): k0: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(singleton, range(min=0, ext=1)): 0

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k0, ): k0: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(singleton, range(min=0, ext=1)): range(min=0, ext=1)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k0, ): range(min=0, ext=4)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k0, range(min=0, ext=4)): range(min=0, ext=4)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k0, range(min=0, ext=4)): k0: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(singleton, range(min=0, ext=1)): 0

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k0, ): k0: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(singleton, range(min=0, ext=1)): range(min=0, ext=1)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k0, ): range(min=0, ext=4)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k0, range(min=0, ext=4)): range(min=0, ext=4)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectPrefetch
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectPrefetch
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  attr [placeholder_red] "realize_scope" = "";
  realize(placeholder_red, [], True {
    placeholder_red[] = 1i64
    for (k0: int32, 0, 4) {
      placeholder_red[] = (placeholder_red[]*placeholder[k0])
    }
  })
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageFlatten
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageFlatten
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k0: int32 >= 0)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k0: int32 < 4)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder_red: Pointer(int64)[0]*(int64*)placeholder: Pointer(int64)[k0: int32])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder_red: Pointer(int64)[0]*(int64*)placeholder: Pointer(int64)[k0: int32])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k0: int32 >= 0)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k0: int32 < 4)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder_red: Pointer(int64)[0]*(int64*)placeholder: Pointer(int64)[k0: int32])
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder_red: Pointer(int64)[0]*(int64*)placeholder: Pointer(int64)[k0: int32])
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.HoistIfThenElse
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.HoistIfThenElse
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:319: List all Stages here
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: compile_engine_const
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: T_multiply
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:327: END List

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectPrefetch
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectPrefetch
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  attr [T_multiply] "realize_scope" = "";
  realize(T_multiply, [], True {
    T_multiply[] = (placeholder[]*4i64)
  })
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageFlatten
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageFlatten
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0]*4i64)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0]*4i64)
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((int64*)placeholder: Pointer(int64)[0]*4i64)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((int64*)placeholder: Pointer(int64)[0]*4i64)
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.HoistIfThenElse
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.HoistIfThenElse
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d0: int32 + -1)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d0: int32 - 1)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d1: int32 + -1)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d1: int32 - 1)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d2: int32 + -1)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d2: int32 - 1)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d3: int32 + -1)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d3: int32 - 1)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d0: int32*d1: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d0: int32*d1: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d0: int32*d1: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d0: int32*d1: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d0: int32*d1: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d0: int32*d1: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d0: int32*d1: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d0: int32*d1: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:319: List all Stages here
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: placeholder
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: T_softmax_maxelem
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i0, range(min=0, ext={d0|d0>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i1, range(min=0, ext={d1|d1>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i2, range(min=0, ext={d2|d2>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(k, range(min=0, ext={d3|d3>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: T_softmax_exp
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i0, range(min=0, ext={d0|d0>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i1, range(min=0, ext={d1|d1>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i2, range(min=0, ext={d2|d2>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i3, range(min=0, ext={d3|d3>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: T_softmax_expsum
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i0, range(min=0, ext={d0|d0>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i1, range(min=0, ext={d1|d1>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i2, range(min=0, ext={d2|d2>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(k, range(min=0, ext={d3|d3>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:321: Name: T_softmax_norm
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:322: Root Iters: 
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i0, range(min=0, ext={d0|d0>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i1, range(min=0, ext={d1|d1>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i2, range(min=0, ext={d2|d2>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:324: iter_var(i3, range(min=0, ext={d3|d3>=0}))
[17:51:58] /workspace/home/codes/tvm/src/te/schedule/schedule_ops.cc:327: END List

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, range(min=0, ext={d0|d0>=0})): floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i2, range(min=0, ext={d2|d2>=0})): floormod(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): threadIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused, ): floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): blockIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused, ): floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i1, range(min=0, ext={d1|d1>=0})): floormod(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.i3.fused, ): (threadIdx.x: int32 + (blockIdx.x: int32*512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i3, range(min=0, ext={d3|d3>=0})): floormod((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i1, range(min=0, ext={d1|d1>=0})): floormod(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i2, range(min=0, ext={d2|d2>=0})): floormod((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused, ): floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.inner, ): threadIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.outer, ): blockIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k, range(min=0, ext={d3|d3>=0})): k: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, range(min=0, ext={d0|d0>=0})): floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k, ): k: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused, ): (threadIdx.x: int32 + (blockIdx.x: int32*512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i1, range(min=0, ext={d1|d1>=0})): floormod(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i2, range(min=0, ext={d2|d2>=0})): floormod((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused, ): floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.inner, ): threadIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.outer, ): blockIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k, range(min=0, ext={d3|d3>=0})): k: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, range(min=0, ext={d0|d0>=0})): floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k, ): k: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused, ): (threadIdx.x: int32 + (blockIdx.x: int32*512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i1, range(min=0, ext={d1|d1>=0})): floormod(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i2, range(min=0, ext={d2|d2>=0})): floormod(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): threadIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): blockIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i3, range(min=0, ext={d3|d3>=0})): floormod((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, range(min=0, ext={d0|d0>=0})): floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.i3.fused, ): (threadIdx.x: int32 + (blockIdx.x: int32*512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused, ): floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused, ): floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, range(min=0, ext={d0|d0>=0})): floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused, ): floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.inner, ): threadIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.outer, ): blockIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused, ): (threadIdx.x: int32 + (blockIdx.x: int32*512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k, ): k: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i1, range(min=0, ext={d1|d1>=0})): floormod(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i2, range(min=0, ext={d2|d2>=0})): floormod((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k, range(min=0, ext={d3|d3>=0})): k: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:532: [[[Show value map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0, range(min=0, ext={d0|d0>=0})): floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused, ): floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.inner, ): threadIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused.outer, ): blockIdx.x: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i0.i1.fused.i2.fused, ): (threadIdx.x: int32 + (blockIdx.x: int32*512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k, ): k: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i1, range(min=0, ext={d1|d1>=0})): floormod(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(i2, range(min=0, ext={d2|d2>=0})): floormod((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:535: iter_var(k, range(min=0, ext={d3|d3>=0})): k: int32

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:538: [[[Show value map end]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:540: [[[Show dom map here]]]

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.outer, ): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i3, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.i3.fused, ): range(min=0, ext=((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i1, range(min=0, ext={d1|d1>=0})): range(min=0, ext={d1|d1>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.inner, ): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(threadIdx.x, , threadIdx.x): range(min=0, ext=512)

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i2, range(min=0, ext={d2|d2>=0})): range(min=0, ext={d2|d2>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(blockIdx.x, , blockIdx.x): range(min=0, ext=floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0, range(min=0, ext={d0|d0>=0})): range(min=0, ext={d0|d0>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused, ): range(min=0, ext=({d0|d0>=0}*{d1|d1>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused, ): range(min=0, ext=(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, ): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(i0.i1.fused.i2.fused.outer, ): range(min=0, ext=floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512))

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:543: iter_var(k, range(min=0, ext={d3|d3>=0})): range(min=0, ext={d3|d3>=0})

[17:51:58] /workspace/home/codes/tvm/src/te/schedule/message_passing.cc:546: [[[Show dom map end]]]

[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectPrefetch
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectPrefetch
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Buffer(T_softmax_maxelem_1: Pointer(float32), float32, [d0, d1, d2], [])] "realize_scope" = "";
  realize(T_softmax_maxelem, [0:d0, 0:d1, 0:d2], True {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
      if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
            T_softmax_maxelem[floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1), floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1), floormod((threadIdx.x + (blockIdx.x*512)), d2)] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
              T_softmax_maxelem[floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1), floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1), floormod((threadIdx.x + (blockIdx.x*512)), d2)] = max(T_softmax_maxelem[floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1), floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1), floormod((threadIdx.x + (blockIdx.x*512)), d2)], placeholder[floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1), floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1), floormod((threadIdx.x + (blockIdx.x*512)), d2), k])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Buffer(T_softmax_exp_1: Pointer(float32), float32, [d0, d1, d2, d3], [])] "realize_scope" = "";
    realize(T_softmax_exp, [0:d0, 0:d1, 0:d2, 0:d3], True {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
            if @tir.likely(((threadIdx.x_1 + (blockIdx.x_1*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
              T_softmax_exp[floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1), floormod(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1), floormod(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), floormod((threadIdx.x_1 + (blockIdx.x_1*512)), d3)] = @tir.exp((placeholder[floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1), floormod(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1), floormod(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), floormod((threadIdx.x_1 + (blockIdx.x_1*512)), d3)] - T_softmax_maxelem[floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1), floormod(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1), floormod(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2)]), dtype=float32)
            }
          }
        }
      }
      attr [T_softmax_expsum: Buffer(T_softmax_expsum_1: Pointer(float32), float32, [d0, d1, d2], [])] "realize_scope" = "";
      realize(T_softmax_expsum, [0:d0, 0:d1, 0:d2], True {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
          if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
              if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                T_softmax_expsum[floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1), floormod(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1), floormod((threadIdx.x_2 + (blockIdx.x_2*512)), d2)] = 0f32
              }
            }
          }
          for (k_1: int32, 0, d3) {
            if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
              if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
                if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                  T_softmax_expsum[floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1), floormod(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1), floormod((threadIdx.x_2 + (blockIdx.x_2*512)), d2)] = (T_softmax_expsum[floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1), floormod(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1), floormod((threadIdx.x_2 + (blockIdx.x_2*512)), d2)] + T_softmax_exp[floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1), floormod(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1), floormod((threadIdx.x_2 + (blockIdx.x_2*512)), d2), k_1])
                }
              }
            }
          }
        }
        attr [T_softmax_norm] "realize_scope" = "";
        realize(T_softmax_norm, [0:d0, 0:d1, 0:d2, 0:d3], True {
          attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
          attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
          if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1) < d0), dtype=bool) {
            if @tir.likely((floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2) < (d0*d1)), dtype=bool) {
              if @tir.likely((floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
                if @tir.likely(((threadIdx.x_3 + (blockIdx.x_3*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
                  T_softmax_norm[floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1), floormod(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1), floormod(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), floormod((threadIdx.x_3 + (blockIdx.x_3*512)), d3)] = (T_softmax_exp[floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1), floormod(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1), floormod(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), floormod((threadIdx.x_3 + (blockIdx.x_3*512)), d3)] / T_softmax_expsum[floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1), floormod(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1), floormod(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2)])
                }
              }
            }
          }
        })
      })
    })
  })
}


[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageFlatten
[17:51:58] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)*d1) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod((threadIdx.x + (blockIdx.x*512)), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod(((blockIdx.x*512) + threadIdx.x), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)*d1) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod((threadIdx.x + (blockIdx.x*512)), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod(((blockIdx.x*512) + threadIdx.x), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)*stride: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*stride: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1)*stride_1: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod((threadIdx.x + (blockIdx.x*512)), d2)*stride_2: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)*d1) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod((threadIdx.x + (blockIdx.x*512)), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod(((blockIdx.x*512) + threadIdx.x), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)*stride: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2), d1)*stride_1: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2)*stride_2: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod((threadIdx.x + (blockIdx.x*512)), d3)*stride_3: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32)*d2) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32)*d2) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32)*d2) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32)*d2) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32)*d3) + floormod((threadIdx.x + (blockIdx.x*512)), d3))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32)*d3) + floormod(((blockIdx.x*512) + threadIdx.x), d3))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)*d1) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod((threadIdx.x + (blockIdx.x*512)), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod(((blockIdx.x*512) + threadIdx.x), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)*d1) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod((threadIdx.x + (blockIdx.x*512)), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod(((blockIdx.x*512) + threadIdx.x), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)*d1) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod((threadIdx.x + (blockIdx.x*512)), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod(((blockIdx.x*512) + threadIdx.x), d2))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((threadIdx.x: int32 + (blockIdx.x: int32*512))*d3: int32) + k: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((blockIdx.x: int32*512) + threadIdx.x: int32)*d3: int32) + k: int32)
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32)*d1) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1))
[17:51:58] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod((threadIdx.x + (blockIdx.x*512)), d2))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d2: int32)*d2) + floormod(((blockIdx.x*512) + threadIdx.x), d2))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2), d1))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32)*d2) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32)*d2) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32)*d3) + floormod((threadIdx.x + (blockIdx.x*512)), d3))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32)*d3) + floormod(((blockIdx.x*512) + threadIdx.x), d3))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2), d1))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*d1) + floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32)*d2) + floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32)*d2) + floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32)*stride: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2), d1)*stride_1: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv((threadIdx.x + (blockIdx.x*512)), d3), d2)*stride_2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod((threadIdx.x + (blockIdx.x*512)), d3)*stride_3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageFlatten
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
      if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
            T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
              T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = max((float32*)T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
            if @tir.likely(((threadIdx.x_1 + (blockIdx.x_1*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
              T_softmax_exp[(threadIdx.x_1 + (blockIdx.x_1*512))] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
          if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
              if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = 0f32
              }
            }
          }
          for (k_1: int32, 0, d3) {
            if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
              if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
                if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                  T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = ((float32*)T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_1)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
              if @tir.likely(((threadIdx.x_3 + (blockIdx.x_3*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
                T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[(threadIdx.x_3 + (blockIdx.x_3*512))] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Legalize
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16Promote
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Promote
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
      if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
            T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
              T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = max((float32*)T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
            if @tir.likely(((threadIdx.x_1 + (blockIdx.x_1*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
              T_softmax_exp[(threadIdx.x_1 + (blockIdx.x_1*512))] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
          if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
              if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = 0f32
              }
            }
          }
          for (k_1: int32, 0, d3) {
            if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
              if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
                if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                  T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = ((float32*)T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_1)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
              if @tir.likely(((threadIdx.x_3 + (blockIdx.x_3*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
                T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[(threadIdx.x_3 + (blockIdx.x_3*512))] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16CastElimination
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16CastElimination
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
      if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
            T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
              T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = max((float32*)T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
            if @tir.likely(((threadIdx.x_1 + (blockIdx.x_1*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
              T_softmax_exp[(threadIdx.x_1 + (blockIdx.x_1*512))] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
          if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
              if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = 0f32
              }
            }
          }
          for (k_1: int32, 0, d3) {
            if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
              if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
                if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                  T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = ((float32*)T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_1)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
              if @tir.likely(((threadIdx.x_3 + (blockIdx.x_3*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
                T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[(threadIdx.x_3 + (blockIdx.x_3*512))] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.BF16TypeLowering
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16TypeLowering
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
      if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
            T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
              T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = max((float32*)T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
            if @tir.likely(((threadIdx.x_1 + (blockIdx.x_1*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
              T_softmax_exp[(threadIdx.x_1 + (blockIdx.x_1*512))] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
          if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
              if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = 0f32
              }
            }
          }
          for (k_1: int32, 0, d3) {
            if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
              if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
                if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                  T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = ((float32*)T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_1)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
              if @tir.likely(((threadIdx.x_3 + (blockIdx.x_3*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
                T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[(threadIdx.x_3 + (blockIdx.x_3*512))] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.BF16Legalize
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
      if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
            T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
              T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = max((float32*)T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
            if @tir.likely(((threadIdx.x_1 + (blockIdx.x_1*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
              T_softmax_exp[(threadIdx.x_1 + (blockIdx.x_1*512))] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
          if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
              if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = 0f32
              }
            }
          }
          for (k_1: int32, 0, d3) {
            if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
              if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
                if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                  T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = ((float32*)T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_1)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
              if @tir.likely(((threadIdx.x_3 + (blockIdx.x_3*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
                T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[(threadIdx.x_3 + (blockIdx.x_3*512))] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.NarrowDataType
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.NarrowDataType
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
      if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
            T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if @tir.likely((floordiv(floordiv((threadIdx.x + (blockIdx.x*512)), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < ((d0*d1)*d2)), dtype=bool) {
              T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))] = max((float32*)T_softmax_maxelem[(threadIdx.x + (blockIdx.x*512))], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2), d1) < d0), dtype=bool) {
        if @tir.likely((floordiv(floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3), d2) < (d0*d1)), dtype=bool) {
          if @tir.likely((floordiv((threadIdx.x_1 + (blockIdx.x_1*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
            if @tir.likely(((threadIdx.x_1 + (blockIdx.x_1*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
              T_softmax_exp[(threadIdx.x_1 + (blockIdx.x_1*512))] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
          if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
              if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = 0f32
              }
            }
          }
          for (k_1: int32, 0, d3) {
            if @tir.likely((floordiv(floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2), d1) < d0), dtype=bool) {
              if @tir.likely((floordiv((threadIdx.x_2 + (blockIdx.x_2*512)), d2) < (d0*d1)), dtype=bool) {
                if @tir.likely(((threadIdx.x_2 + (blockIdx.x_2*512)) < ((d0*d1)*d2)), dtype=bool) {
                  T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] = ((float32*)T_softmax_expsum[(threadIdx.x_2 + (blockIdx.x_2*512))] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_1)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if @tir.likely((floordiv(floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2), d1) < d0), dtype=bool) {
          if @tir.likely((floordiv(floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3), d2) < (d0*d1)), dtype=bool) {
            if @tir.likely((floordiv((threadIdx.x_3 + (blockIdx.x_3*512)), d3) < ((d0*d1)*d2)), dtype=bool) {
              if @tir.likely(((threadIdx.x_3 + (blockIdx.x_3*512)) < (((d0*d1)*d2)*d3)), dtype=bool) {
                T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[(threadIdx.x_3 + (blockIdx.x_3*512))] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32) < d0: int32), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32) < (d0: int32*d1: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify -3.40282e+38f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify -3.40282e+38f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (threadIdx.x: int32 + (blockIdx.x: int32*512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32) < d0: int32), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32) < (d0: int32*d1: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify max((float32*)T_softmax_maxelem: Pointer(float32)[(threadIdx.x: int32 + (blockIdx.x: int32*512))], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (threadIdx.x: int32 + (blockIdx.x: int32*512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32) < d0: int32), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely((floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32) < (d0: int32*d1: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d3: int32*(d2: int32*(d0: int32*d1: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d3: int32*((d0: int32*d1: int32)*d2: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (((d0: int32*d1: int32)*d2: int32)*d3: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (((d0: int32*d1: int32)*d2: int32)*d3: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (threadIdx.x: int32 + (blockIdx.x: int32*512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d0: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d0: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d1: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d1: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d2: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d2: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32) < d0: int32), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32) < (d0: int32*d1: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify 0f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (threadIdx.x: int32 + (blockIdx.x: int32*512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32), d1: int32) < d0: int32), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d2: int32) < (d0: int32*d1: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_expsum: Pointer(float32)[(threadIdx.x: int32 + (blockIdx.x: int32*512))] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_expsum: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (threadIdx.x: int32 + (blockIdx.x: int32*512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv(floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32), d1: int32) < d0: int32), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely((floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32) < (d2: int32*(d1: int32*d0: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv(floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32), d2: int32) < (d0: int32*d1: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d3: int32*(d2: int32*(d0: int32*d1: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32)))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely((floordiv((threadIdx.x: int32 + (blockIdx.x: int32*512)), d3: int32) < ((d0: int32*d1: int32)*d2: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (d3: int32*((d0: int32*d1: int32)*d2: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32))), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (((d0: int32*d1: int32)*d2: int32)*d3: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.likely(((threadIdx.x: int32 + (blockIdx.x: int32*512)) < (((d0: int32*d1: int32)*d2: int32)*d3: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.likely((((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32)), dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_exp: Pointer(float32)[(threadIdx.x: int32 + (blockIdx.x: int32*512))] / (float32*)T_softmax_expsum: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_expsum: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
      if @tir.likely((((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))), dtype=bool) {
        if @tir.likely((((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))), dtype=bool) {
          if @tir.likely((((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)), dtype=bool) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if @tir.likely((((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))), dtype=bool) {
          if @tir.likely((((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))), dtype=bool) {
            if @tir.likely((((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)), dtype=bool) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if @tir.likely((((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))), dtype=bool) {
        if @tir.likely((((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))), dtype=bool) {
          if @tir.likely((((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))), dtype=bool) {
            if @tir.likely((((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)), dtype=bool) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512 {
          if @tir.likely((((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))), dtype=bool) {
            if @tir.likely((((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))), dtype=bool) {
              if @tir.likely((((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)), dtype=bool) {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_1: int32, 0, d3) {
            if @tir.likely((((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))), dtype=bool) {
              if @tir.likely((((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))), dtype=bool) {
                if @tir.likely((((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)), dtype=bool) {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_1)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if @tir.likely((((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))), dtype=bool) {
          if @tir.likely((((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))), dtype=bool) {
            if @tir.likely((((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))), dtype=bool) {
              if @tir.likely((((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)), dtype=bool) {
                T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LoopPartition
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (floordiv(min(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), (d2*(d1*d0))), (d2*(d0*d1))), ((d0*d1)*d2)), 512) + -1)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (floordiv(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512) - 1)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify neg_inf: handle
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify neg_inf: handle
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512)
[17:51:59] /workspace/home/codes/tvm/src/tir/transforms/loop_partition.cc:554: Warning: Cannot prove: ((((floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512) - 1) - floordiv(min(min(min((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}), ({d2|d2>=0}*({d0|d0>=0}*{d1|d1>=0}))), ({d2|d2>=0}*({d1|d1>=0}*{d0|d0>=0}))), (({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})), 512)) + 1) >= 0), when generating the post doubt loop
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512) + -1)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512) - 1)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify neg_inf: handle
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify neg_inf: handle
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512)
[17:51:59] /workspace/home/codes/tvm/src/tir/transforms/loop_partition.cc:554: Warning: Cannot prove: ((((floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512) - 1) - floordiv(min(min(min(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}), ({d3|d3>=0}*(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))), ({d3|d3>=0}*({d2|d2>=0}*({d1|d1>=0}*{d0|d0>=0})))), ({d3|d3>=0}*({d2|d2>=0}*({d0|d0>=0}*{d1|d1>=0})))), 512)) + 1) >= 0), when generating the post doubt loop
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (floordiv(min(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), (d2*(d1*d0))), (d2*(d0*d1))), ((d0*d1)*d2)), 512) + -1)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (floordiv(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512) - 1)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify neg_inf: handle
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify neg_inf: handle
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512)
[17:51:59] /workspace/home/codes/tvm/src/tir/transforms/loop_partition.cc:554: Warning: Cannot prove: ((((floordiv(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}) + 511), 512) - 1) - floordiv(min(min(min((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}), ({d2|d2>=0}*({d0|d0>=0}*{d1|d1>=0}))), ({d2|d2>=0}*({d1|d1>=0}*{d0|d0>=0}))), (({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})), 512)) + 1) >= 0), when generating the post doubt loop
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512) + -1)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512) - 1)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify neg_inf: handle
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify neg_inf: handle
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512)
[17:51:59] /workspace/home/codes/tvm/src/tir/transforms/loop_partition.cc:554: Warning: Cannot prove: ((((floordiv((((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}) + 511), 512) - 1) - floordiv(min(min(min(((({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0})*{d3|d3>=0}), ({d3|d3>=0}*({d2|d2>=0}*({d1|d1>=0}*{d0|d0>=0})))), ({d3|d3>=0}*({d2|d2>=0}*({d0|d0>=0}*{d1|d1>=0})))), ({d3|d3>=0}*(({d0|d0>=0}*{d1|d1>=0})*{d2|d2>=0}))), 512)) + 1) >= 0), when generating the post doubt loop
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LoopPartition
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
      if True {
        if True {
          if True {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if True {
          if True {
            if True {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_1 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
        if True {
          if True {
            if True {
              if True {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      } else {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_2 < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_2: int32, 0, d3) {
            if True {
              if True {
                if True {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_3: int32, 0, d3) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
                if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_3 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                if True {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
              if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
                if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VectorizeLoop
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VectorizeLoop
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
      if True {
        if True {
          if True {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if True {
          if True {
            if True {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_1 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
        if True {
          if True {
            if True {
              if True {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      } else {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_2 < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_2: int32, 0, d3) {
            if True {
              if True {
                if True {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_3: int32, 0, d3) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
                if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_3 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                if True {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
              if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
                if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectVirtualThread
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectVirtualThread
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
      if True {
        if True {
          if True {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if True {
          if True {
            if True {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_1 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
        if True {
          if True {
            if True {
              if True {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      } else {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_2 < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_2: int32, 0, d3) {
            if True {
              if True {
                if True {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_3: int32, 0, d3) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
                if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_3 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                if True {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
              if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
                if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InjectDoubleBuffer
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InjectDoubleBuffer
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [d0, d1, d2]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
      if True {
        if True {
          if True {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if True {
          if True {
            if True {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [d0, d1, d2, d3]) {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_1 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
        if True {
          if True {
            if True {
              if True {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      } else {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      }
      attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "global";
      allocate(T_softmax_expsum, float32, [d0, d1, d2]) {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_2 < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_2: int32, 0, d3) {
            if True {
              if True {
                if True {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_3: int32, 0, d3) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
                if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                  T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_expsum[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_3 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                if True {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
              if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
                if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_expsum[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.StorageRewrite
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.StorageRewrite
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
      if True {
        if True {
          if True {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if True {
          if True {
            if True {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
     {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_1 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
        if True {
          if True {
            if True {
              if True {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      } else {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      }
       {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_2 < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_2: int32, 0, d3) {
            if True {
              if True {
                if True {
                  T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_3: int32, 0, d3) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
                if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                  T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_3 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                if True {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
              if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
                if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.UnrollLoop
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.UnrollLoop
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
      if True {
        if True {
          if True {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k: int32, 0, d3) {
        if True {
          if True {
            if True {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
            }
          }
        }
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
     {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_1 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
        if True {
          if True {
            if True {
              if True {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      } else {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      }
       {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_2 < min(floordiv(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_2: int32, 0, d3) {
            if True {
              if True {
                if True {
                  T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_3: int32, 0, d3) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
                if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                  T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_3 < min(floordiv(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1))) {
          if True {
            if True {
              if True {
                if True {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        } else {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
              if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
                if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d0: int32*d1: int32)*d2: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify "global"
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((d0: int32*d1: int32)*d2: int32)*d3: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < min(floordiv(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify -3.40282e+38f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify -3.40282e+38f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512) <= blockIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify -3.40282e+38f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify -3.40282e+38f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < min(floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512) <= blockIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < min(floordiv(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), 512), ((floordiv((((d0*d1)*d2) + 511), 512) - 1) + 1)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify 0f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512) <= blockIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify 0f32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < min(floordiv(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), 512), ((floordiv(((((d0*d1)*d2)*d3) + 511), 512) - 1) + 1)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition True
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify conditionTrue
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint True
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512) <= blockIdx.x: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
     {
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
        T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
      } else {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      }
       {
        attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
        attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
          T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          for (k_2: int32, 0, d3) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
          }
        } else {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
              }
            }
          }
          for (k_3: int32, 0, d3) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
                if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                  T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
                }
              }
            }
          }
        }
        attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
        attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
        if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
          T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
        } else {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
              if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
                if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
                  T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
                }
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RemoveNoOp
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RemoveNoOp
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.RewriteUnsafeSelect
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.HoistIfThenElse
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.HoistIfThenElse
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1151: CODEGEN!
[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1162: Lower cached function shape_func_nn_softmax_1 in target llvm -keys=cpu -link-params=0
[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1162: Lower cached function fused_prod in target llvm -keys=cpu -link-params=0
[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1162: Lower cached function fused_multiply in target llvm -keys=cpu -link-params=0
[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1162: Lower cached function fused_nn_softmax in target cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32
[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1184: FUNCS MAP
[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1186: Target: llvm -keys=cpu -link-params=0, primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1186: Target: llvm -keys=cpu -link-params=0, primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1186: Target: llvm -keys=cpu -link-params=0, primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1186: Target: cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VerifyMemory
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VerifyMemory
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InferFragment
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InferFragment
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerThreadAllreduce
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerThreadAllreduce
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [4], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 4) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.MakePackedAPI
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (4 == cast(int32, (int64*)arg0.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (4 == cast(int32, (int64*)arg0.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (4 == cast(int32, (int64*)arg1.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (4 == cast(int32, (int64*)arg1.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.SplitHostDevice
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerTVMBuiltin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 2)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == cast(int32, (int64*)arg0.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition!@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == cast(int32, (int64*)arg0.strides: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == cast(int32, (int64*)arg1.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = !@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = !@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition !@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition!@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == cast(int32, (int64*)arg1.strides: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.CombineContextCall
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((4 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (4 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 4) {
        compute[i0] = (int64*)placeholder[i0]
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerWarpMemory
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerWarpMemory

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VerifyMemory
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VerifyMemory
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InferFragment
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InferFragment
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerThreadAllreduce
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerThreadAllreduce
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [4], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 4) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.MakePackedAPI
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (4 == cast(int32, (int64*)arg0.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (4 == cast(int32, (int64*)arg0.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.SplitHostDevice
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerTVMBuiltin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 2)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == cast(int32, (int64*)arg0.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition!@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == cast(int32, (int64*)arg0.strides: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.CombineContextCall
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((4 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (4 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 4) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerWarpMemory
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerWarpMemory

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VerifyMemory
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VerifyMemory
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InferFragment
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InferFragment
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerThreadAllreduce
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerThreadAllreduce
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.MakePackedAPI
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.SplitHostDevice
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerTVMBuiltin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 2)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.CombineContextCall
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args == 2), "fused_multiply: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder[0]*4i64)
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerWarpMemory
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerWarpMemory

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo

[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.VerifyMemory
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.VerifyMemory
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.ThreadSync
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.ThreadSync
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.InferFragment
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.InferFragment
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerThreadAllreduce
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerThreadAllreduce
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [d0: int32, d1: int32, d2: int32, d3: int32], [stride: int32, stride_1: int32, stride_2: int32, stride_3: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [d0, d1, d2, d3], [stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
  allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
      for (k: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k*stride_7))])
      }
    } else {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
          }
        }
      }
      for (k_1: int32, 0, d3) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder_2[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_4) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_5)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_6)) + (k_1*stride_7))])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
    } else {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
              T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_2[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride_7))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
      T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
      for (k_2: int32, 0, d3) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
      }
    } else {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
          }
        }
      }
      for (k_3: int32, 0, d3) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
            }
          }
        }
      }
    }
    attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
    attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
      T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
    } else {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
              T_softmax_norm_2[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_1)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_2)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_3))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.MakePackedAPI
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d3: int32*d2: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d3: int32*d2: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d3: int32*d2: int32)*d1: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d3: int32*d2: int32)*d1: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((d3: int32*d2: int32)*d1: int32)*d0: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((d3: int32*d2: int32)*d1: int32)*d0: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (2 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (2 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d0: int32 == cast(int32, (int64*)arg1.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d0: int32 == cast(int32, (int64*)arg1.shape: handle[0]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d1: int32 == cast(int32, (int64*)arg1.shape: handle[1]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d1: int32 == cast(int32, (int64*)arg1.shape: handle[1]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d2: int32 == cast(int32, (int64*)arg1.shape: handle[2]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d2: int32 == cast(int32, (int64*)arg1.shape: handle[2]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d3: int32 == cast(int32, (int64*)arg1.shape: handle[3]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d3: int32 == cast(int32, (int64*)arg1.shape: handle[3]))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (d3: int32*d2: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (d3: int32*d2: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((d3: int32*d2: int32)*d1: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((d3: int32*d2: int32)*d1: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((d3: int32*d2: int32)*d1: int32)*d0: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((d3: int32*d2: int32)*d1: int32)*d0: int32)
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (2 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (2 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_2: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_6: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
    @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
      attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
        T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        for (k: int32, 0, d3) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_3) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_2)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_1)) + (k*stride))])
        }
      } else {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
            if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
            }
          }
        }
        for (k_1: int32, 0, d3) {
          if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
            if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
              if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
                T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_3) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_2)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_1)) + (k_1*stride))])
              }
            }
          }
        }
      }
      attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_1 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
        T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_3) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_2)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_1)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
      } else {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0*d1)*d2)*d3)) {
                T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_3) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2), d1)*stride_2)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3), d2)*stride_1)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3)*stride))] - (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3)]), dtype=float32)
              }
            }
          }
        }
      }
      attr [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
      attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_2 < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
        T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        for (k_2: int32, 0, d3) {
          T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_2)])
        }
      } else {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
              T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
            }
          }
        }
        for (k_3: int32, 0, d3) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d1*d0))) {
            if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2*(d0*d1))) {
              if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0*d1)*d2)) {
                T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp[((((blockIdx.x_2*512) + threadIdx.x_2)*d3) + k_3)])
              }
            }
          }
        }
      }
      attr [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0*d1)*d2)*d3) + 511), 512);
      attr [IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if (blockIdx.x_3 < floordiv(min(min(min(min((((d0*d1)*d2)*d3), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512)) {
        T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_7) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_6)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_5)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_4))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
      } else {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d1*d0)))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*(d2*(d0*d1)))) {
            if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3*((d0*d1)*d2))) {
              if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0*d1)*d2)*d3)) {
                T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_7) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2), d1)*stride_6)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3), d2)*stride_5)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3)*stride_4))] = ((float32*)T_softmax_exp[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3)])
              }
            }
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.SplitHostDevice
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.SplitHostDevice
primfn(T_softmax_maxelem: Pointer(float32), placeholder: Pointer(float32), d0: int32, d1: int32, d2: int32, d3: int32, stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
    T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
    for (k: int32, 0, d3) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k*stride_3))])
    }
  } else {
    if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
        if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        }
      }
    }
    for (k_1: int32, 0, d3) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k_1*stride_3))])
          }
        }
      }
    }
  }
}

primfn(T_softmax_exp: Pointer(float32), placeholder_1: Pointer(float32), T_softmax_maxelem_1: Pointer(float32), d0_1: int32, d1_1: int32, d2_1: int32, d3_1: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel1", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_1*d1_1)*d2_1)*d3_1) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_1 < floordiv(min(min(min(min((((d0_1*d1_1)*d2_1)*d3_1), (d3_1*((d0_1*d1_1)*d2_1))), (d3_1*(d2_1*(d1_1*d0_1)))), (d3_1*(d2_1*(d0_1*d1_1)))), ((((d0_1*d1_1)*d2_1)*d3_1) + 511)), 512)) {
    T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
  } else {
    if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d1_1*d0_1)))) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d0_1*d1_1)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*((d0_1*d1_1)*d2_1))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0_1*d1_1)*d2_1)*d3_1)) {
            T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
          }
        }
      }
    }
  }
}

primfn(T_softmax_maxelem_2: Pointer(float32), T_softmax_exp_1: Pointer(float32), d0_2: int32, d1_2: int32, d2_2: int32, d3_2: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel2", "tir.device_thread_axis": [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_2, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0_2*d1_2)*d2_2) + 511), 512);
  attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_2 < floordiv(min(min(min(min(((d0_2*d1_2)*d2_2), (d2_2*(d0_2*d1_2))), (d2_2*(d1_2*d0_2))), ((d0_2*d1_2)*d2_2)), (((d0_2*d1_2)*d2_2) + 511)), 512)) {
    T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
    for (k_2: int32, 0, d3_2) {
      T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_2)])
    }
  } else {
    if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
          T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        }
      }
    }
    for (k_3: int32, 0, d3_2) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
            T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_3)])
          }
        }
      }
    }
  }
}

primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0_3: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1_3: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2_3: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3_3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride_8: int32 = @tir.if_then_else((d3_3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_9: int32 = @tir.if_then_else((d2_3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3_3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_10: int32 = @tir.if_then_else((d1_3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3_3*d2_3), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_11: int32 = @tir.if_then_else((d0_3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3_3*d2_3)*d1_3), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_12: int32 = @tir.if_then_else((d3_3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_13: int32 = @tir.if_then_else((d2_3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3_3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_14: int32 = @tir.if_then_else((d1_3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3_3*d2_3), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_15: int32 = @tir.if_then_else((d0_3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3_3*d2_3)*d1_3), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0_3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1_3 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2_3 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3_3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
    @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem_3: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_maxelem_3, float32, [((d0_3*d1_3)*d2_3)]);
    attr [T_softmax_exp_2: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp_2, float32, [(((d0_3*d1_3)*d2_3)*d3_3)]) {
      @tir.tvm_call_packed("fused_nn_softmax_kernel0", T_softmax_maxelem_3, placeholder_2, d0_3, d1_3, d2_3, d3_3, stride_11, stride_10, stride_9, stride_8, floordiv((((d0_3*d1_3)*d2_3) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel1", T_softmax_exp_2, placeholder_2, T_softmax_maxelem_3, d0_3, d1_3, d2_3, d3_3, stride_11, stride_10, stride_9, stride_8, floordiv(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel2", T_softmax_maxelem_3, T_softmax_exp_2, d0_3, d1_3, d2_3, d3_3, floordiv((((d0_3*d1_3)*d2_3) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel3", T_softmax_norm, T_softmax_exp_2, T_softmax_maxelem_3, d0_3, d1_3, d2_3, d3_3, stride_15, stride_14, stride_13, stride_12, floordiv(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 512), 512, dtype=int32)
    }
  }
}

primfn(T_softmax_norm_1: Pointer(float32), T_softmax_exp_3: Pointer(float32), T_softmax_maxelem_4: Pointer(float32), d0_4: int32, d1_4: int32, d2_4: int32, d3_4: int32, stride_16: int32, stride_17: int32, stride_18: int32, stride_19: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel3", "tir.device_thread_axis": [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_3, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_4*d1_4)*d2_4)*d3_4) + 511), 512);
  attr [IterVar(threadIdx.x_3, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_3 < floordiv(min(min(min(min((((d0_4*d1_4)*d2_4)*d3_4), (d3_4*(d2_4*(d1_4*d0_4)))), (d3_4*(d2_4*(d0_4*d1_4)))), (d3_4*((d0_4*d1_4)*d2_4))), ((((d0_4*d1_4)*d2_4)*d3_4) + 511)), 512)) {
    T_softmax_norm_1[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_4), d2_4), d1_4)*stride_16) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_4), d2_4), d1_4)*stride_17)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_4), d2_4)*stride_18)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_4)*stride_19))] = ((float32*)T_softmax_exp_3[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_4[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_4)])
  } else {
    if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_4*(d2_4*(d1_4*d0_4)))) {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_4*(d2_4*(d0_4*d1_4)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_4*((d0_4*d1_4)*d2_4))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0_4*d1_4)*d2_4)*d3_4)) {
            T_softmax_norm_1[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_4), d2_4), d1_4)*stride_16) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_4), d2_4), d1_4)*stride_17)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_4), d2_4)*stride_18)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_4)*stride_19))] = ((float32*)T_softmax_exp_3[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_4[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_4)])
          }
        }
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_2: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_6: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
    @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
      @tir.tvm_call_packed("fused_nn_softmax_kernel0", T_softmax_maxelem, placeholder, d0, d1, d2, d3, stride_3, stride_2, stride_1, stride, floordiv((((d0*d1)*d2) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel1", T_softmax_exp, placeholder, T_softmax_maxelem, d0, d1, d2, d3, stride_3, stride_2, stride_1, stride, floordiv(((((d0*d1)*d2)*d3) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel2", T_softmax_maxelem, T_softmax_exp, d0, d1, d2, d3, floordiv((((d0*d1)*d2) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel3", T_softmax_norm, T_softmax_exp, T_softmax_maxelem, d0, d1, d2, d3, stride_7, stride_6, stride_5, stride_4, floordiv(((((d0*d1)*d2)*d3) + 511), 512), 512, dtype=int32)
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_2: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_6: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
    @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_maxelem, float32, [((d0*d1)*d2)]);
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
    allocate(T_softmax_exp, float32, [(((d0*d1)*d2)*d3)]) {
      @tir.tvm_call_packed("fused_nn_softmax_kernel0", T_softmax_maxelem, placeholder, d0, d1, d2, d3, stride_3, stride_2, stride_1, stride, floordiv((((d0*d1)*d2) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel1", T_softmax_exp, placeholder, T_softmax_maxelem, d0, d1, d2, d3, stride_3, stride_2, stride_1, stride, floordiv(((((d0*d1)*d2)*d3) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel2", T_softmax_maxelem, T_softmax_exp, d0, d1, d2, d3, floordiv((((d0*d1)*d2) + 511), 512), 512, dtype=int32)
      @tir.tvm_call_packed("fused_nn_softmax_kernel3", T_softmax_norm, T_softmax_exp, T_softmax_maxelem, d0, d1, d2, d3, stride_7, stride_6, stride_5, stride_4, floordiv(((((d0*d1)*d2)*d3) + 511), 512), 512, dtype=int32)
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerTVMBuiltin
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 14, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 14, dtype=handle)
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_2: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_6: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode[0] = 0
      @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
      stack_tcode[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
    attr [T_softmax_maxelem] "storage_alignment" = 128 {
      let T_softmax_maxelem = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*((d0*d1)*d2))), 2, 32, dtype=handle)
       {
        if @tir.isnullptr(T_softmax_maxelem, dtype=bool) {
          @tir.tvm_throw_last_error(, dtype=int32)
        }
        attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
        attr [T_softmax_exp] "storage_alignment" = 128 {
          let T_softmax_exp = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*(((d0*d1)*d2)*d3))), 2, 32, dtype=handle)
           {
            if @tir.isnullptr(T_softmax_exp, dtype=bool) {
              @tir.tvm_throw_last_error(, dtype=int32)
            }
             {
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, floordiv((((d0*d1)*d2) + 511), 512)), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, 512), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value, stack_tcode, 0, 12, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_exp, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, floordiv(((((d0*d1)*d2)*d3) + 511), 512)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel1", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, floordiv((((d0*d1)*d2) + 511), 512)), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, 512), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel2", stack_value, stack_tcode, 0, 8, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_norm, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_7), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_6), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_5), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride_4), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, floordiv(((((d0*d1)*d2)*d3) + 511), 512)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel3", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
            }
          }
          if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_exp, dtype=int32) != 0) {
            @tir.tvm_throw_last_error(, dtype=int32)
          }
        }
      }
      if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_maxelem, dtype=int32) != 0) {
        @tir.tvm_throw_last_error(, dtype=int32)
      }
    }
  }
}


[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:51:59] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 14, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 14, dtype=handle)
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_2: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_6: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode[0] = 0
      @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
      stack_tcode[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
    attr [T_softmax_maxelem] "storage_alignment" = 128 {
      let T_softmax_maxelem = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*((d0*d1)*d2))), 2, 32, dtype=handle)
       {
        if @tir.isnullptr(T_softmax_maxelem, dtype=bool) {
          @tir.tvm_throw_last_error(, dtype=int32)
        }
        attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
        attr [T_softmax_exp] "storage_alignment" = 128 {
          let T_softmax_exp = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*(((d0*d1)*d2)*d3))), 2, 32, dtype=handle)
           {
            if @tir.isnullptr(T_softmax_exp, dtype=bool) {
              @tir.tvm_throw_last_error(, dtype=int32)
            }
             {
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, floordiv((((d0*d1)*d2) + 511), 512)), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, 512), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value, stack_tcode, 0, 12, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_exp, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, floordiv(((((d0*d1)*d2)*d3) + 511), 512)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel1", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, floordiv((((d0*d1)*d2) + 511), 512)), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, 512), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel2", stack_value, stack_tcode, 0, 8, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_norm, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_7), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_6), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_5), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride_4), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, floordiv(((((d0*d1)*d2)*d3) + 511), 512)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel3", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
            }
          }
          if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_exp, dtype=int32) != 0) {
            @tir.tvm_throw_last_error(, dtype=int32)
          }
        }
      }
      if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_maxelem, dtype=int32) != 0) {
        @tir.tvm_throw_last_error(, dtype=int32)
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 2)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d3: int32 == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d3: int32 != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d2: int32 == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d2: int32 != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d1: int32 == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d1: int32 != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d0: int32 == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d0: int32 != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg0.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d3: int32 == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d3: int32 != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d2: int32 == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d2: int32 != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d1: int32 == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d1: int32 != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d0: int32 == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d0: int32 != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(arg1.strides: handle, dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (2 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d0: int32 == cast(int32, (int64*)arg1.shape: handle[0]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d1: int32 == cast(int32, (int64*)arg1.shape: handle[1]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d2: int32 == cast(int32, (int64*)arg1.shape: handle[2]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d3: int32 == cast(int32, (int64*)arg1.shape: handle[3]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (2 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.isnullptr(T_softmax_maxelem: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.isnullptr(T_softmax_maxelem: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition @tir.isnullptr(T_softmax_maxelem: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition@tir.isnullptr(T_softmax_maxelem: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(T_softmax_maxelem: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = @tir.isnullptr(T_softmax_exp: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = @tir.isnullptr(T_softmax_exp: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition @tir.isnullptr(T_softmax_exp: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition@tir.isnullptr(T_softmax_exp: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(T_softmax_exp: Pointer(float32), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_exp: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_exp: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_exp: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_exp: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_exp: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_maxelem: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_maxelem: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_maxelem: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_maxelem: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (@tir.TVMBackendFreeWorkspace(2, dev_id: int32, T_softmax_maxelem: Pointer(float32), dtype=int32) != 0)
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 14, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 14, dtype=handle)
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_2: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_6: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode[0] = 0
      @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
      stack_tcode[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
    attr [T_softmax_maxelem] "storage_alignment" = 128 {
      let T_softmax_maxelem = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*((d0*d1)*d2))), 2, 32, dtype=handle)
       {
        if @tir.isnullptr(T_softmax_maxelem, dtype=bool) {
          @tir.tvm_throw_last_error(, dtype=int32)
        }
        attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
        attr [T_softmax_exp] "storage_alignment" = 128 {
          let T_softmax_exp = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*(((d0*d1)*d2)*d3))), 2, 32, dtype=handle)
           {
            if @tir.isnullptr(T_softmax_exp, dtype=bool) {
              @tir.tvm_throw_last_error(, dtype=int32)
            }
             {
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, @tir.shift_right((((d0*d1)*d2) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, 512), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value, stack_tcode, 0, 12, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_exp, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, @tir.shift_right(((((d0*d1)*d2)*d3) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel1", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, @tir.shift_right((((d0*d1)*d2) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, 512), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel2", stack_value, stack_tcode, 0, 8, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_norm, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_7), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_6), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_5), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride_4), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, @tir.shift_right(((((d0*d1)*d2)*d3) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel3", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
            }
          }
          if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_exp, dtype=int32) != 0) {
            @tir.tvm_throw_last_error(, dtype=int32)
          }
        }
      }
      if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_maxelem, dtype=int32) != 0) {
        @tir.tvm_throw_last_error(, dtype=int32)
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 14, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 14, dtype=handle)
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_2: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_6: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode[0] = 0
      @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
      stack_tcode[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
    attr [T_softmax_maxelem] "storage_alignment" = 128 {
      let T_softmax_maxelem = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*((d0*d1)*d2))), 2, 32, dtype=handle)
       {
        if @tir.isnullptr(T_softmax_maxelem, dtype=bool) {
          @tir.tvm_throw_last_error(, dtype=int32)
        }
        attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
        attr [T_softmax_exp] "storage_alignment" = 128 {
          let T_softmax_exp = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*(((d0*d1)*d2)*d3))), 2, 32, dtype=handle)
           {
            if @tir.isnullptr(T_softmax_exp, dtype=bool) {
              @tir.tvm_throw_last_error(, dtype=int32)
            }
             {
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, @tir.shift_right((((d0*d1)*d2) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, 512), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value, stack_tcode, 0, 12, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_exp, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, @tir.shift_right(((((d0*d1)*d2)*d3) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel1", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, @tir.shift_right((((d0*d1)*d2) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, 512), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel2", stack_value, stack_tcode, 0, 8, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_norm, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_7), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_6), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_5), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride_4), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, @tir.shift_right(((((d0*d1)*d2)*d3) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel3", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
            }
          }
          if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_exp, dtype=int32) != 0) {
            @tir.tvm_throw_last_error(, dtype=int32)
          }
        }
      }
      if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_maxelem, dtype=int32) != 0) {
        @tir.tvm_throw_last_error(, dtype=int32)
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.CombineContextCall
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 14, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 14, dtype=handle)
  assert((num_args == 2), "fused_nn_softmax: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let d0: int32 = cast(int32, (int64*)arg0.shape[0])
  let d1: int32 = cast(int32, (int64*)arg0.shape[1])
  let d2: int32 = cast(int32, (int64*)arg0.shape[2])
  let d3: int32 = cast(int32, (int64*)arg0.shape[3])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[3]), dtype=int32), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), d3, cast(int32, (int64*)arg0.strides[2]), dtype=int32), dtype=int32)
  let stride_2: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg0.strides[1]), dtype=int32), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else((d3 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[3]), dtype=int32), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((d2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), d3, cast(int32, (int64*)arg1.strides[2]), dtype=int32), dtype=int32)
  let stride_6: int32 = @tir.if_then_else((d1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), (d3*d2), cast(int32, (int64*)arg1.strides[1]), dtype=int32), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((d0 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), ((d3*d2)*d1), cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((4 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 4")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((d0 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: ({d0|d0>=0} == int32(arg1.shape[0]))")
  assert((d1 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: ({d1|d1>=0} == int32(arg1.shape[1]))")
  assert((d2 == cast(int32, (int64*)arg1.shape[2])), "Argument arg1.shape[2] has an unsatisfied constraint: ({d2|d2>=0} == int32(arg1.shape[2]))")
  assert((d3 == cast(int32, (int64*)arg1.shape[3])), "Argument arg1.shape[3] has an unsatisfied constraint: ({d3|d3>=0} == int32(arg1.shape[3]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode[0] = 0
      @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
      stack_tcode[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "global";
    attr [T_softmax_maxelem] "storage_alignment" = 128 {
      let T_softmax_maxelem = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*((d0*d1)*d2))), 2, 32, dtype=handle)
       {
        if @tir.isnullptr(T_softmax_maxelem, dtype=bool) {
          @tir.tvm_throw_last_error(, dtype=int32)
        }
        attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "global";
        attr [T_softmax_exp] "storage_alignment" = 128 {
          let T_softmax_exp = @tir.TVMBackendAllocWorkspace(2, dev_id, cast(uint64, (4*(((d0*d1)*d2)*d3))), 2, 32, dtype=handle)
           {
            if @tir.isnullptr(T_softmax_exp, dtype=bool) {
              @tir.tvm_throw_last_error(, dtype=int32)
            }
             {
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, @tir.shift_right((((d0*d1)*d2) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, 512), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value, stack_tcode, 0, 12, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_exp, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_1), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, @tir.shift_right(((((d0*d1)*d2)*d3) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel1", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, cast(int64, d0), dtype=int32)
                stack_tcode[2] = 0
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d1), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d2), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d3), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, @tir.shift_right((((d0*d1)*d2) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, 512), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel2", stack_value, stack_tcode, 0, 8, dtype=int32)
              }
               {
                @tir.tvm_struct_set(stack_value, 0, 12, T_softmax_norm, dtype=int32)
                stack_tcode[0] = 3
                @tir.tvm_struct_set(stack_value, 1, 12, T_softmax_exp, dtype=int32)
                stack_tcode[1] = 3
                @tir.tvm_struct_set(stack_value, 2, 12, T_softmax_maxelem, dtype=int32)
                stack_tcode[2] = 3
                @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, d0), dtype=int32)
                stack_tcode[3] = 0
                @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, d1), dtype=int32)
                stack_tcode[4] = 0
                @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, d2), dtype=int32)
                stack_tcode[5] = 0
                @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, d3), dtype=int32)
                stack_tcode[6] = 0
                @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_7), dtype=int32)
                stack_tcode[7] = 0
                @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_6), dtype=int32)
                stack_tcode[8] = 0
                @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, stride_5), dtype=int32)
                stack_tcode[9] = 0
                @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, stride_4), dtype=int32)
                stack_tcode[10] = 0
                @tir.tvm_struct_set(stack_value, 11, 12, cast(int64, @tir.shift_right(((((d0*d1)*d2)*d3) + 511), 9, dtype=int32)), dtype=int32)
                stack_tcode[11] = 0
                @tir.tvm_struct_set(stack_value, 12, 12, cast(int64, 512), dtype=int32)
                stack_tcode[12] = 0
                @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel3", stack_value, stack_tcode, 0, 13, dtype=int32)
              }
            }
          }
          if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_exp, dtype=int32) != 0) {
            @tir.tvm_throw_last_error(, dtype=int32)
          }
        }
      }
      if (@tir.TVMBackendFreeWorkspace(2, dev_id, T_softmax_maxelem, dtype=int32) != 0) {
        @tir.tvm_throw_last_error(, dtype=int32)
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass Filter
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential Filter
primfn(T_softmax_maxelem: Pointer(float32), placeholder: Pointer(float32), d0: int32, d1: int32, d2: int32, d3: int32, stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
    T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
    for (k: int32, 0, d3) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k*stride_3))])
    }
  } else {
    if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
        if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        }
      }
    }
    for (k_1: int32, 0, d3) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k_1*stride_3))])
          }
        }
      }
    }
  }
}

primfn(T_softmax_exp: Pointer(float32), placeholder_1: Pointer(float32), T_softmax_maxelem_1: Pointer(float32), d0_1: int32, d1_1: int32, d2_1: int32, d3_1: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel1", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_1*d1_1)*d2_1)*d3_1) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_1 < floordiv(min(min(min(min((((d0_1*d1_1)*d2_1)*d3_1), (d3_1*((d0_1*d1_1)*d2_1))), (d3_1*(d2_1*(d1_1*d0_1)))), (d3_1*(d2_1*(d0_1*d1_1)))), ((((d0_1*d1_1)*d2_1)*d3_1) + 511)), 512)) {
    T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
  } else {
    if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d1_1*d0_1)))) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d0_1*d1_1)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*((d0_1*d1_1)*d2_1))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0_1*d1_1)*d2_1)*d3_1)) {
            T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
          }
        }
      }
    }
  }
}

primfn(T_softmax_maxelem_2: Pointer(float32), T_softmax_exp_1: Pointer(float32), d0_2: int32, d1_2: int32, d2_2: int32, d3_2: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel2", "tir.device_thread_axis": [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_2, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0_2*d1_2)*d2_2) + 511), 512);
  attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_2 < floordiv(min(min(min(min(((d0_2*d1_2)*d2_2), (d2_2*(d0_2*d1_2))), (d2_2*(d1_2*d0_2))), ((d0_2*d1_2)*d2_2)), (((d0_2*d1_2)*d2_2) + 511)), 512)) {
    T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
    for (k_2: int32, 0, d3_2) {
      T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_2)])
    }
  } else {
    if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
          T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        }
      }
    }
    for (k_3: int32, 0, d3_2) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
            T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_3)])
          }
        }
      }
    }
  }
}

primfn(T_softmax_norm: Pointer(float32), T_softmax_exp_2: Pointer(float32), T_softmax_maxelem_3: Pointer(float32), d0_3: int32, d1_3: int32, d2_3: int32, d3_3: int32, stride_8: int32, stride_9: int32, stride_10: int32, stride_11: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel3", "tir.device_thread_axis": [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_3, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 512);
  attr [IterVar(threadIdx.x_3, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_3 < floordiv(min(min(min(min((((d0_3*d1_3)*d2_3)*d3_3), (d3_3*(d2_3*(d1_3*d0_3)))), (d3_3*(d2_3*(d0_3*d1_3)))), (d3_3*((d0_3*d1_3)*d2_3))), ((((d0_3*d1_3)*d2_3)*d3_3) + 511)), 512)) {
    T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
  } else {
    if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d1_3*d0_3)))) {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d0_3*d1_3)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*((d0_3*d1_3)*d2_3))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0_3*d1_3)*d2_3)*d3_3)) {
            T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
          }
        }
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass BindTarget
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential BindTarget
primfn(T_softmax_maxelem: Pointer(float32), placeholder: Pointer(float32), d0: int32, d1: int32, d2: int32, d3: int32, stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
    T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
    for (k: int32, 0, d3) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k*stride_3))])
    }
  } else {
    if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
        if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        }
      }
    }
    for (k_1: int32, 0, d3) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k_1*stride_3))])
          }
        }
      }
    }
  }
}

primfn(T_softmax_exp: Pointer(float32), placeholder_1: Pointer(float32), T_softmax_maxelem_1: Pointer(float32), d0_1: int32, d1_1: int32, d2_1: int32, d3_1: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel1", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_1*d1_1)*d2_1)*d3_1) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_1 < floordiv(min(min(min(min((((d0_1*d1_1)*d2_1)*d3_1), (d3_1*((d0_1*d1_1)*d2_1))), (d3_1*(d2_1*(d1_1*d0_1)))), (d3_1*(d2_1*(d0_1*d1_1)))), ((((d0_1*d1_1)*d2_1)*d3_1) + 511)), 512)) {
    T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
  } else {
    if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d1_1*d0_1)))) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d0_1*d1_1)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*((d0_1*d1_1)*d2_1))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0_1*d1_1)*d2_1)*d3_1)) {
            T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
          }
        }
      }
    }
  }
}

primfn(T_softmax_maxelem_2: Pointer(float32), T_softmax_exp_1: Pointer(float32), d0_2: int32, d1_2: int32, d2_2: int32, d3_2: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel2", "tir.device_thread_axis": [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_2, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0_2*d1_2)*d2_2) + 511), 512);
  attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_2 < floordiv(min(min(min(min(((d0_2*d1_2)*d2_2), (d2_2*(d0_2*d1_2))), (d2_2*(d1_2*d0_2))), ((d0_2*d1_2)*d2_2)), (((d0_2*d1_2)*d2_2) + 511)), 512)) {
    T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
    for (k_2: int32, 0, d3_2) {
      T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_2)])
    }
  } else {
    if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
          T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        }
      }
    }
    for (k_3: int32, 0, d3_2) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
            T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_3)])
          }
        }
      }
    }
  }
}

primfn(T_softmax_norm: Pointer(float32), T_softmax_exp_2: Pointer(float32), T_softmax_maxelem_3: Pointer(float32), d0_3: int32, d1_3: int32, d2_3: int32, d3_3: int32, stride_8: int32, stride_9: int32, stride_10: int32, stride_11: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel3", "tir.device_thread_axis": [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_3, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 512);
  attr [IterVar(threadIdx.x_3, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_3 < floordiv(min(min(min(min((((d0_3*d1_3)*d2_3)*d3_3), (d3_3*(d2_3*(d1_3*d0_3)))), (d3_3*(d2_3*(d0_3*d1_3)))), (d3_3*((d0_3*d1_3)*d2_3))), ((((d0_3*d1_3)*d2_3)*d3_3) + 511)), 512)) {
    T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
  } else {
    if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d1_3*d0_3)))) {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d0_3*d1_3)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*((d0_3*d1_3)*d2_3))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0_3*d1_3)*d2_3)*d3_3)) {
            T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
          }
        }
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerWarpMemory
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerWarpMemory
primfn(T_softmax_maxelem: Pointer(float32), placeholder: Pointer(float32), d0: int32, d1: int32, d2: int32, d3: int32, stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
    T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
    for (k: int32, 0, d3) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k*stride_3))])
    }
  } else {
    if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
        if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        }
      }
    }
    for (k_1: int32, 0, d3) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k_1*stride_3))])
          }
        }
      }
    }
  }
}

primfn(T_softmax_exp: Pointer(float32), placeholder_1: Pointer(float32), T_softmax_maxelem_1: Pointer(float32), d0_1: int32, d1_1: int32, d2_1: int32, d3_1: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel1", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_1*d1_1)*d2_1)*d3_1) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_1 < floordiv(min(min(min(min((((d0_1*d1_1)*d2_1)*d3_1), (d3_1*((d0_1*d1_1)*d2_1))), (d3_1*(d2_1*(d1_1*d0_1)))), (d3_1*(d2_1*(d0_1*d1_1)))), ((((d0_1*d1_1)*d2_1)*d3_1) + 511)), 512)) {
    T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
  } else {
    if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d1_1*d0_1)))) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d0_1*d1_1)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*((d0_1*d1_1)*d2_1))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0_1*d1_1)*d2_1)*d3_1)) {
            T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
          }
        }
      }
    }
  }
}

primfn(T_softmax_maxelem_2: Pointer(float32), T_softmax_exp_1: Pointer(float32), d0_2: int32, d1_2: int32, d2_2: int32, d3_2: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel2", "tir.device_thread_axis": [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_2, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0_2*d1_2)*d2_2) + 511), 512);
  attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_2 < floordiv(min(min(min(min(((d0_2*d1_2)*d2_2), (d2_2*(d0_2*d1_2))), (d2_2*(d1_2*d0_2))), ((d0_2*d1_2)*d2_2)), (((d0_2*d1_2)*d2_2) + 511)), 512)) {
    T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
    for (k_2: int32, 0, d3_2) {
      T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_2)])
    }
  } else {
    if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
          T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        }
      }
    }
    for (k_3: int32, 0, d3_2) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
            T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_3)])
          }
        }
      }
    }
  }
}

primfn(T_softmax_norm: Pointer(float32), T_softmax_exp_2: Pointer(float32), T_softmax_maxelem_3: Pointer(float32), d0_3: int32, d1_3: int32, d2_3: int32, d3_3: int32, stride_8: int32, stride_9: int32, stride_10: int32, stride_11: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel3", "tir.device_thread_axis": [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_3, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 512);
  attr [IterVar(threadIdx.x_3, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_3 < floordiv(min(min(min(min((((d0_3*d1_3)*d2_3)*d3_3), (d3_3*(d2_3*(d1_3*d0_3)))), (d3_3*(d2_3*(d0_3*d1_3)))), (d3_3*((d0_3*d1_3)*d2_3))), ((((d0_3*d1_3)*d2_3)*d3_3) + 511)), 512)) {
    T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
  } else {
    if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d1_3*d0_3)))) {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d0_3*d1_3)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*((d0_3*d1_3)*d2_3))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0_3*d1_3)*d2_3)*d3_3)) {
            T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
          }
        }
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.Simplify
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify -3.40282e+38f32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify -3.40282e+38f32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512) <= blockIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify -3.40282e+38f32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify -3.40282e+38f32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify max((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)], (float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2: int32)) + (k: int32*stride_3: int32))])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512) <= blockIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify @tir.exp(((float32*)placeholder: Pointer(float32)[((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))] - (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3)]), dtype=float32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv((((d0: int32*d1: int32)*d2: int32) + 511), 512)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0f32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify 0f32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512) <= blockIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify 0f32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify 0f32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 >= 0)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (k: int32 < d3: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify d3: int32
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify d3: int32
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_maxelem: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] + (float32*)T_softmax_exp: Pointer(float32)[((((blockIdx.x*512) + threadIdx.x)*d3: int32) + k: int32)])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((blockIdx.x: int32*512) + threadIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify floordiv(((((d0: int32*d1: int32)*d2: int32)*d3: int32) + 511), 512)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512) <= blockIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((float32*)T_softmax_exp: Pointer(float32)[((blockIdx.x: int32*512) + threadIdx.x: int32)] / (float32*)T_softmax_maxelem: Pointer(float32)[floordiv(((blockIdx.x*512) + threadIdx.x), d3: int32)])
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:131: After rewrite simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:134: After canonical simplify ((((floordiv(floordiv(floordiv(((blockIdx.x: int32*512) + threadIdx.x: int32), d3: int32), d2: int32), d1: int32)*stride: int32) + (floormod(floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2), d1)*stride_1: int32)) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d3), d2)*stride_2: int32)) + (floormod(((blockIdx.x*512) + threadIdx.x), d3)*stride_3: int32))
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.Simplify
primfn(T_softmax_maxelem: Pointer(float32), placeholder: Pointer(float32), d0: int32, d1: int32, d2: int32, d3: int32, stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
    T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
    for (k: int32, 0, d3) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k*stride_3))])
    }
  } else {
    if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
        if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        }
      }
    }
    for (k_1: int32, 0, d3) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k_1*stride_3))])
          }
        }
      }
    }
  }
}

primfn(T_softmax_exp: Pointer(float32), placeholder_1: Pointer(float32), T_softmax_maxelem_1: Pointer(float32), d0_1: int32, d1_1: int32, d2_1: int32, d3_1: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel1", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_1*d1_1)*d2_1)*d3_1) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_1 < floordiv(min(min(min(min((((d0_1*d1_1)*d2_1)*d3_1), (d3_1*((d0_1*d1_1)*d2_1))), (d3_1*(d2_1*(d1_1*d0_1)))), (d3_1*(d2_1*(d0_1*d1_1)))), ((((d0_1*d1_1)*d2_1)*d3_1) + 511)), 512)) {
    T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
  } else {
    if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d1_1*d0_1)))) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d0_1*d1_1)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*((d0_1*d1_1)*d2_1))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0_1*d1_1)*d2_1)*d3_1)) {
            T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
          }
        }
      }
    }
  }
}

primfn(T_softmax_maxelem_2: Pointer(float32), T_softmax_exp_1: Pointer(float32), d0_2: int32, d1_2: int32, d2_2: int32, d3_2: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel2", "tir.device_thread_axis": [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_2, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0_2*d1_2)*d2_2) + 511), 512);
  attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_2 < floordiv(min(min(min(min(((d0_2*d1_2)*d2_2), (d2_2*(d0_2*d1_2))), (d2_2*(d1_2*d0_2))), ((d0_2*d1_2)*d2_2)), (((d0_2*d1_2)*d2_2) + 511)), 512)) {
    T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
    for (k_2: int32, 0, d3_2) {
      T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_2)])
    }
  } else {
    if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
          T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        }
      }
    }
    for (k_3: int32, 0, d3_2) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
            T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_3)])
          }
        }
      }
    }
  }
}

primfn(T_softmax_norm: Pointer(float32), T_softmax_exp_2: Pointer(float32), T_softmax_maxelem_3: Pointer(float32), d0_3: int32, d1_3: int32, d2_3: int32, d3_3: int32, stride_8: int32, stride_9: int32, stride_10: int32, stride_11: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel3", "tir.device_thread_axis": [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_3, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 512);
  attr [IterVar(threadIdx.x_3, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_3 < floordiv(min(min(min(min((((d0_3*d1_3)*d2_3)*d3_3), (d3_3*(d2_3*(d1_3*d0_3)))), (d3_3*(d2_3*(d0_3*d1_3)))), (d3_3*((d0_3*d1_3)*d2_3))), ((((d0_3*d1_3)*d2_3)*d3_3) + 511)), 512)) {
    T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
  } else {
    if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d1_3*d0_3)))) {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d0_3*d1_3)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*((d0_3*d1_3)*d2_3))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0_3*d1_3)*d2_3)*d3_3)) {
            T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
          }
        }
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerCustomDatatypes
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerCustomDatatypes
primfn(T_softmax_maxelem: Pointer(float32), placeholder: Pointer(float32), d0: int32, d1: int32, d2: int32, d3: int32, stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0*d1)*d2) + 511), 512);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < floordiv(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512)) {
    T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
    for (k: int32, 0, d3) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k*stride_3))])
    }
  } else {
    if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
        if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        }
      }
    }
    for (k_1: int32, 0, d3) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[((((floordiv(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride) + (floormod(floordiv(((blockIdx.x*512) + threadIdx.x), d2), d1)*stride_1)) + (floormod(((blockIdx.x*512) + threadIdx.x), d2)*stride_2)) + (k_1*stride_3))])
          }
        }
      }
    }
  }
}

primfn(T_softmax_exp: Pointer(float32), placeholder_1: Pointer(float32), T_softmax_maxelem_1: Pointer(float32), d0_1: int32, d1_1: int32, d2_1: int32, d3_1: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel1", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_1*d1_1)*d2_1)*d3_1) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_1 < floordiv(min(min(min(min((((d0_1*d1_1)*d2_1)*d3_1), (d3_1*((d0_1*d1_1)*d2_1))), (d3_1*(d2_1*(d1_1*d0_1)))), (d3_1*(d2_1*(d0_1*d1_1)))), ((((d0_1*d1_1)*d2_1)*d3_1) + 511)), 512)) {
    T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
  } else {
    if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d1_1*d0_1)))) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d0_1*d1_1)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*((d0_1*d1_1)*d2_1))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0_1*d1_1)*d2_1)*d3_1)) {
            T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.exp(((float32*)placeholder_1[((((floordiv(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_4) + (floormod(floordiv(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1), d1_1)*stride_5)) + (floormod(floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1), d2_1)*stride_6)) + (floormod(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[floordiv(((blockIdx.x_1*512) + threadIdx.x_1), d3_1)]), dtype=float32)
          }
        }
      }
    }
  }
}

primfn(T_softmax_maxelem_2: Pointer(float32), T_softmax_exp_1: Pointer(float32), d0_2: int32, d1_2: int32, d2_2: int32, d3_2: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel2", "tir.device_thread_axis": [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_2, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((((d0_2*d1_2)*d2_2) + 511), 512);
  attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_2 < floordiv(min(min(min(min(((d0_2*d1_2)*d2_2), (d2_2*(d0_2*d1_2))), (d2_2*(d1_2*d0_2))), ((d0_2*d1_2)*d2_2)), (((d0_2*d1_2)*d2_2) + 511)), 512)) {
    T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
    for (k_2: int32, 0, d3_2) {
      T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_2)])
    }
  } else {
    if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
          T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        }
      }
    }
    for (k_3: int32, 0, d3_2) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
            T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_3)])
          }
        }
      }
    }
  }
}

primfn(T_softmax_norm: Pointer(float32), T_softmax_exp_2: Pointer(float32), T_softmax_maxelem_3: Pointer(float32), d0_3: int32, d1_3: int32, d2_3: int32, d3_3: int32, stride_8: int32, stride_9: int32, stride_10: int32, stride_11: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel3", "tir.device_thread_axis": [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_3, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 512);
  attr [IterVar(threadIdx.x_3, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_3 < floordiv(min(min(min(min((((d0_3*d1_3)*d2_3)*d3_3), (d3_3*(d2_3*(d1_3*d0_3)))), (d3_3*(d2_3*(d0_3*d1_3)))), (d3_3*((d0_3*d1_3)*d2_3))), ((((d0_3*d1_3)*d2_3)*d3_3) + 511)), 512)) {
    T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
  } else {
    if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d1_3*d0_3)))) {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d0_3*d1_3)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*((d0_3*d1_3)*d2_3))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0_3*d1_3)*d2_3)*d3_3)) {
            T_softmax_norm[((((floordiv(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_8) + (floormod(floordiv(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3), d1_3)*stride_9)) + (floormod(floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3), d2_3)*stride_10)) + (floormod(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[floordiv(((blockIdx.x_3*512) + threadIdx.x_3), d3_3)])
          }
        }
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerIntrin
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < @tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < @tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < @tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < @tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (@tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32) <= blockIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < @tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < @tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < @tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < @tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (@tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*((d0*d1)*d2))), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32) <= blockIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < @tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < @tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < @tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < @tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (@tir.shift_right(min(min(min(min(((d0: int32*d1: int32)*d2: int32), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32) <= blockIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d1: int32*d0: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d2: int32*(d0: int32*d1: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < ((d0: int32*d1: int32)*d2: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (blockIdx.x: int32 < floordiv(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 512))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (blockIdx.x: int32 < @tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (blockIdx.x: int32 < @tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(blockIdx.x: int32 < @tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (blockIdx.x: int32 < @tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (@tir.shift_right(min(min(min(min((((d0: int32*d1: int32)*d2: int32)*d3: int32), (d3*(d2*(d1*d0)))), (d3*(d2*(d0*d1)))), (d3*((d0*d1)*d2))), ((((d0*d1)*d2)*d3) + 511)), 9, dtype=int32) <= blockIdx.x: int32)
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d1: int32*d0: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*(d2: int32*(d0: int32*d1: int32))))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (d3: int32*((d0: int32*d1: int32)*d2: int32)))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:57: Before VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:59: After VisitExpr, condition = (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:69: Judge condition (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/ir_mutator_with_analyzer.cc:70: After simplify condition(((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((blockIdx.x: int32*512) + threadIdx.x: int32) < (((d0: int32*d1: int32)*d2: int32)*d3: int32))
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerIntrin
primfn(T_softmax_maxelem: Pointer(float32), placeholder: Pointer(float32), d0: int32, d1: int32, d2: int32, d3: int32, stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right((((d0*d1)*d2) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < @tir.shift_right(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32)) {
    T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
    for (k: int32, 0, d3) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[(((((((((blockIdx.x*512) + threadIdx.x) / d2) / d1) + @tir.shift_right(((((blockIdx.x*512) + threadIdx.x) / d2) % d1), 31, dtype=int32))*stride) + ((((((blockIdx.x*512) + threadIdx.x) / d2) % d1) + @tir.bitwise_and(d1, @tir.shift_right(((((blockIdx.x*512) + threadIdx.x) / d2) % d1), 31, dtype=int32), dtype=int32))*stride_1)) + ((((blockIdx.x*512) + threadIdx.x) % d2)*stride_2)) + (k*stride_3))])
    }
  } else {
    if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
        if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        }
      }
    }
    for (k_1: int32, 0, d3) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[(((((((((blockIdx.x*512) + threadIdx.x) / d2) / d1) + @tir.shift_right(((((blockIdx.x*512) + threadIdx.x) / d2) % d1), 31, dtype=int32))*stride) + ((((((blockIdx.x*512) + threadIdx.x) / d2) % d1) + @tir.bitwise_and(d1, @tir.shift_right(((((blockIdx.x*512) + threadIdx.x) / d2) % d1), 31, dtype=int32), dtype=int32))*stride_1)) + ((((blockIdx.x*512) + threadIdx.x) % d2)*stride_2)) + (k_1*stride_3))])
          }
        }
      }
    }
  }
}

primfn(T_softmax_exp: Pointer(float32), placeholder_1: Pointer(float32), T_softmax_maxelem_1: Pointer(float32), d0_1: int32, d1_1: int32, d2_1: int32, d3_1: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel1", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right(((((d0_1*d1_1)*d2_1)*d3_1) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_1 < @tir.shift_right(min(min(min(min((((d0_1*d1_1)*d2_1)*d3_1), (d3_1*((d0_1*d1_1)*d2_1))), (d3_1*(d2_1*(d1_1*d0_1)))), (d3_1*(d2_1*(d0_1*d1_1)))), ((((d0_1*d1_1)*d2_1)*d3_1) + 511)), 9, dtype=int32)) {
    T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.call_pure_extern("__expf", ((float32*)placeholder_1[(((((((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) / d1_1) + @tir.shift_right(((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1), 31, dtype=int32))*stride_4) + ((((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1) + @tir.bitwise_and(d1_1, @tir.shift_right(((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1), 31, dtype=int32), dtype=int32))*stride_5)) + ((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1) + @tir.bitwise_and(d2_1, @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32), dtype=int32))*stride_6)) + ((((blockIdx.x_1*512) + threadIdx.x_1) % d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[(((blockIdx.x_1*512) + threadIdx.x_1) / d3_1)]), dtype=float32)
  } else {
    if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d1_1*d0_1)))) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d0_1*d1_1)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*((d0_1*d1_1)*d2_1))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0_1*d1_1)*d2_1)*d3_1)) {
            T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.call_pure_extern("__expf", ((float32*)placeholder_1[(((((((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) / d1_1) + @tir.shift_right(((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1), 31, dtype=int32))*stride_4) + ((((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1) + @tir.bitwise_and(d1_1, @tir.shift_right(((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1), 31, dtype=int32), dtype=int32))*stride_5)) + ((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1) + @tir.bitwise_and(d2_1, @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32), dtype=int32))*stride_6)) + ((((blockIdx.x_1*512) + threadIdx.x_1) % d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[(((blockIdx.x_1*512) + threadIdx.x_1) / d3_1)]), dtype=float32)
          }
        }
      }
    }
  }
}

primfn(T_softmax_maxelem_2: Pointer(float32), T_softmax_exp_1: Pointer(float32), d0_2: int32, d1_2: int32, d2_2: int32, d3_2: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel2", "tir.device_thread_axis": [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_2, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right((((d0_2*d1_2)*d2_2) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_2 < @tir.shift_right(min(min(min(min(((d0_2*d1_2)*d2_2), (d2_2*(d0_2*d1_2))), (d2_2*(d1_2*d0_2))), ((d0_2*d1_2)*d2_2)), (((d0_2*d1_2)*d2_2) + 511)), 9, dtype=int32)) {
    T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
    for (k_2: int32, 0, d3_2) {
      T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_2)])
    }
  } else {
    if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
          T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        }
      }
    }
    for (k_3: int32, 0, d3_2) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
            T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_3)])
          }
        }
      }
    }
  }
}

primfn(T_softmax_norm: Pointer(float32), T_softmax_exp_2: Pointer(float32), T_softmax_maxelem_3: Pointer(float32), d0_3: int32, d1_3: int32, d2_3: int32, d3_3: int32, stride_8: int32, stride_9: int32, stride_10: int32, stride_11: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel3", "tir.device_thread_axis": [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_3, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x_3, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_3 < @tir.shift_right(min(min(min(min((((d0_3*d1_3)*d2_3)*d3_3), (d3_3*(d2_3*(d1_3*d0_3)))), (d3_3*(d2_3*(d0_3*d1_3)))), (d3_3*((d0_3*d1_3)*d2_3))), ((((d0_3*d1_3)*d2_3)*d3_3) + 511)), 9, dtype=int32)) {
    T_softmax_norm[(((((((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) / d1_3) + @tir.shift_right(((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3), 31, dtype=int32))*stride_8) + ((((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3) + @tir.bitwise_and(d1_3, @tir.shift_right(((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3), 31, dtype=int32), dtype=int32))*stride_9)) + ((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3) + @tir.bitwise_and(d2_3, @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32), dtype=int32))*stride_10)) + ((((blockIdx.x_3*512) + threadIdx.x_3) % d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[(((blockIdx.x_3*512) + threadIdx.x_3) / d3_3)])
  } else {
    if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d1_3*d0_3)))) {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d0_3*d1_3)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*((d0_3*d1_3)*d2_3))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0_3*d1_3)*d2_3)*d3_3)) {
            T_softmax_norm[(((((((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) / d1_3) + @tir.shift_right(((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3), 31, dtype=int32))*stride_8) + ((((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3) + @tir.bitwise_and(d1_3, @tir.shift_right(((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3), 31, dtype=int32), dtype=int32))*stride_9)) + ((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3) + @tir.bitwise_and(d2_3, @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32), dtype=int32))*stride_10)) + ((((blockIdx.x_3*512) + threadIdx.x_3) % d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[(((blockIdx.x_3*512) + threadIdx.x_3) / d3_3)])
          }
        }
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:463: DOING Pass tir.LowerDeviceStorageAccessInfo
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:466: DEPENDENCIES: 
[17:52:00] /workspace/home/codes/tvm/src/ir/transform.cc:473: After tir sequential tir.LowerDeviceStorageAccessInfo
primfn(T_softmax_maxelem: Pointer(float32), placeholder: Pointer(float32), d0: int32, d1: int32, d2: int32, d3: int32, stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right((((d0*d1)*d2) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < @tir.shift_right(min(min(min(min(((d0*d1)*d2), (d2*(d0*d1))), (d2*(d1*d0))), ((d0*d1)*d2)), (((d0*d1)*d2) + 511)), 9, dtype=int32)) {
    T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
    for (k: int32, 0, d3) {
      T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[(((((((((blockIdx.x*512) + threadIdx.x) / d2) / d1) + @tir.shift_right(((((blockIdx.x*512) + threadIdx.x) / d2) % d1), 31, dtype=int32))*stride) + ((((((blockIdx.x*512) + threadIdx.x) / d2) % d1) + @tir.bitwise_and(d1, @tir.shift_right(((((blockIdx.x*512) + threadIdx.x) / d2) % d1), 31, dtype=int32), dtype=int32))*stride_1)) + ((((blockIdx.x*512) + threadIdx.x) % d2)*stride_2)) + (k*stride_3))])
    }
  } else {
    if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
        if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
          T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = -3.40282e+38f32
        }
      }
    }
    for (k_1: int32, 0, d3) {
      if (((blockIdx.x*512) + threadIdx.x) < (d2*(d1*d0))) {
        if (((blockIdx.x*512) + threadIdx.x) < (d2*(d0*d1))) {
          if (((blockIdx.x*512) + threadIdx.x) < ((d0*d1)*d2)) {
            T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)] = max((float32*)T_softmax_maxelem[((blockIdx.x*512) + threadIdx.x)], (float32*)placeholder[(((((((((blockIdx.x*512) + threadIdx.x) / d2) / d1) + @tir.shift_right(((((blockIdx.x*512) + threadIdx.x) / d2) % d1), 31, dtype=int32))*stride) + ((((((blockIdx.x*512) + threadIdx.x) / d2) % d1) + @tir.bitwise_and(d1, @tir.shift_right(((((blockIdx.x*512) + threadIdx.x) / d2) % d1), 31, dtype=int32), dtype=int32))*stride_1)) + ((((blockIdx.x*512) + threadIdx.x) % d2)*stride_2)) + (k_1*stride_3))])
          }
        }
      }
    }
  }
}

primfn(T_softmax_exp: Pointer(float32), placeholder_1: Pointer(float32), T_softmax_maxelem_1: Pointer(float32), d0_1: int32, d1_1: int32, d2_1: int32, d3_1: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel1", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right(((((d0_1*d1_1)*d2_1)*d3_1) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_1 < @tir.shift_right(min(min(min(min((((d0_1*d1_1)*d2_1)*d3_1), (d3_1*((d0_1*d1_1)*d2_1))), (d3_1*(d2_1*(d1_1*d0_1)))), (d3_1*(d2_1*(d0_1*d1_1)))), ((((d0_1*d1_1)*d2_1)*d3_1) + 511)), 9, dtype=int32)) {
    T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.call_pure_extern("__expf", ((float32*)placeholder_1[(((((((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) / d1_1) + @tir.shift_right(((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1), 31, dtype=int32))*stride_4) + ((((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1) + @tir.bitwise_and(d1_1, @tir.shift_right(((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1), 31, dtype=int32), dtype=int32))*stride_5)) + ((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1) + @tir.bitwise_and(d2_1, @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32), dtype=int32))*stride_6)) + ((((blockIdx.x_1*512) + threadIdx.x_1) % d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[(((blockIdx.x_1*512) + threadIdx.x_1) / d3_1)]), dtype=float32)
  } else {
    if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d1_1*d0_1)))) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*(d2_1*(d0_1*d1_1)))) {
        if (((blockIdx.x_1*512) + threadIdx.x_1) < (d3_1*((d0_1*d1_1)*d2_1))) {
          if (((blockIdx.x_1*512) + threadIdx.x_1) < (((d0_1*d1_1)*d2_1)*d3_1)) {
            T_softmax_exp[((blockIdx.x_1*512) + threadIdx.x_1)] = @tir.call_pure_extern("__expf", ((float32*)placeholder_1[(((((((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) / d1_1) + @tir.shift_right(((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1), 31, dtype=int32))*stride_4) + ((((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1) + @tir.bitwise_and(d1_1, @tir.shift_right(((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) / d2_1) + @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32)) % d1_1), 31, dtype=int32), dtype=int32))*stride_5)) + ((((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1) + @tir.bitwise_and(d2_1, @tir.shift_right(((((blockIdx.x_1*512) + threadIdx.x_1) / d3_1) % d2_1), 31, dtype=int32), dtype=int32))*stride_6)) + ((((blockIdx.x_1*512) + threadIdx.x_1) % d3_1)*stride_7))] - (float32*)T_softmax_maxelem_1[(((blockIdx.x_1*512) + threadIdx.x_1) / d3_1)]), dtype=float32)
          }
        }
      }
    }
  }
}

primfn(T_softmax_maxelem_2: Pointer(float32), T_softmax_exp_1: Pointer(float32), d0_2: int32, d1_2: int32, d2_2: int32, d3_2: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel2", "tir.device_thread_axis": [IterVar(blockIdx.x_2: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_2, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right((((d0_2*d1_2)*d2_2) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_2 < @tir.shift_right(min(min(min(min(((d0_2*d1_2)*d2_2), (d2_2*(d0_2*d1_2))), (d2_2*(d1_2*d0_2))), ((d0_2*d1_2)*d2_2)), (((d0_2*d1_2)*d2_2) + 511)), 9, dtype=int32)) {
    T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
    for (k_2: int32, 0, d3_2) {
      T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_2)])
    }
  } else {
    if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
          T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = 0f32
        }
      }
    }
    for (k_3: int32, 0, d3_2) {
      if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d1_2*d0_2))) {
        if (((blockIdx.x_2*512) + threadIdx.x_2) < (d2_2*(d0_2*d1_2))) {
          if (((blockIdx.x_2*512) + threadIdx.x_2) < ((d0_2*d1_2)*d2_2)) {
            T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] = ((float32*)T_softmax_maxelem_2[((blockIdx.x_2*512) + threadIdx.x_2)] + (float32*)T_softmax_exp_1[((((blockIdx.x_2*512) + threadIdx.x_2)*d3_2) + k_3)])
          }
        }
      }
    }
  }
}

primfn(T_softmax_norm: Pointer(float32), T_softmax_exp_2: Pointer(float32), T_softmax_maxelem_3: Pointer(float32), d0_3: int32, d1_3: int32, d2_3: int32, d3_3: int32, stride_8: int32, stride_9: int32, stride_10: int32, stride_11: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel3", "tir.device_thread_axis": [IterVar(blockIdx.x_3: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_3: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_3, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right(((((d0_3*d1_3)*d2_3)*d3_3) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x_3, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x_3 < @tir.shift_right(min(min(min(min((((d0_3*d1_3)*d2_3)*d3_3), (d3_3*(d2_3*(d1_3*d0_3)))), (d3_3*(d2_3*(d0_3*d1_3)))), (d3_3*((d0_3*d1_3)*d2_3))), ((((d0_3*d1_3)*d2_3)*d3_3) + 511)), 9, dtype=int32)) {
    T_softmax_norm[(((((((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) / d1_3) + @tir.shift_right(((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3), 31, dtype=int32))*stride_8) + ((((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3) + @tir.bitwise_and(d1_3, @tir.shift_right(((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3), 31, dtype=int32), dtype=int32))*stride_9)) + ((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3) + @tir.bitwise_and(d2_3, @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32), dtype=int32))*stride_10)) + ((((blockIdx.x_3*512) + threadIdx.x_3) % d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[(((blockIdx.x_3*512) + threadIdx.x_3) / d3_3)])
  } else {
    if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d1_3*d0_3)))) {
      if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*(d2_3*(d0_3*d1_3)))) {
        if (((blockIdx.x_3*512) + threadIdx.x_3) < (d3_3*((d0_3*d1_3)*d2_3))) {
          if (((blockIdx.x_3*512) + threadIdx.x_3) < (((d0_3*d1_3)*d2_3)*d3_3)) {
            T_softmax_norm[(((((((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) / d1_3) + @tir.shift_right(((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3), 31, dtype=int32))*stride_8) + ((((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3) + @tir.bitwise_and(d1_3, @tir.shift_right(((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) / d2_3) + @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32)) % d1_3), 31, dtype=int32), dtype=int32))*stride_9)) + ((((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3) + @tir.bitwise_and(d2_3, @tir.shift_right(((((blockIdx.x_3*512) + threadIdx.x_3) / d3_3) % d2_3), 31, dtype=int32), dtype=int32))*stride_10)) + ((((blockIdx.x_3*512) + threadIdx.x_3) % d3_3)*stride_11))] = ((float32*)T_softmax_exp_2[((blockIdx.x_3*512) + threadIdx.x_3)] / (float32*)T_softmax_maxelem_3[(((blockIdx.x_3*512) + threadIdx.x_3) / d3_3)])
          }
        }
      }
    }
  }
}


[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 2)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == cast(int32, (int64*)arg0.shape: handle[0]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == cast(int32, (int64*)arg0.strides: handle[0]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == cast(int32, (int64*)arg1.shape: handle[0]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == cast(int32, (int64*)arg1.strides: handle[0]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 2)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == cast(int32, (int64*)arg0.shape: handle[0]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == cast(int32, (int64*)arg0.strides: handle[0]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 2)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (1 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (num_args: int32 == 2)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 0, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[3]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[2]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[1]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) == 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (cast(int32, (int64*)arg0.shape: handle[0]) != 1)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint @tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint !@tir.isnullptr(@tir.tvm_struct_get(@tir.tvm_struct_get(args: handle, 1, 12, dtype=handle), 0, 3, dtype=handle), dtype=bool)
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg0.code: int32 == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint ((((arg1.code: int32 == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == @tir.tvm_struct_get(arg0: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg0: handle, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg0: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (2 == @tir.tvm_struct_get(arg0: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (4 == @tir.tvm_struct_get(arg1: handle, 0, 4, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (((@tir.tvm_struct_get(arg1: handle, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d0: int32 == cast(int32, (int64*)arg1.shape: handle[0]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d1: int32 == cast(int32, (int64*)arg1.shape: handle[1]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d2: int32 == cast(int32, (int64*)arg1.shape: handle[2]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (d3: int32 == cast(int32, (int64*)arg1.shape: handle[3]))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (0u64 == @tir.tvm_struct_get(arg1: handle, 0, 8, dtype=uint64))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (2 == @tir.tvm_struct_get(arg1: handle, 0, 10, dtype=int32))
[17:52:00] /workspace/home/codes/tvm/src/arith/analyzer.cc:69: enter constraint (dev_id: int32 == @tir.tvm_struct_get(arg1: handle, 0, 9, dtype=int32))
Raw module: 
def @main(%x: Tensor[(meta[tir.SizeVar][0], meta[tir.SizeVar][1], meta[tir.SizeVar][2], meta[tir.SizeVar][3]), float32]) {
  nn.softmax(%x)
}


Running on (cuda, cuda(0))
lowering fused_add
lowering fused_divide
lowering fused_prod
lowering fused_multiply
lowering fused_nn_softmax
Finish in 17.46798 ms
