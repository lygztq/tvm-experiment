[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass RemoveUnusedFunctions
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) {
  %0 = add(%a, %b);
  %1 = nn.softmax(%0);
  add(%1, %b)
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToBasicBlockNormalForm
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) {
  %0 = add(%a, %b);
  %1 = nn.softmax(%0);
  add(%1, %b)
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass sequential
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass SimplifyInference
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass DynamicToStatic
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass SimplifyExpr
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldScaleAxis
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(?, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:914: LOWER START

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass RemoveUnusedFunctions
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToBasicBlockNormalForm
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b);
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1);
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b)
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass sequential
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass EtaExpand
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11)
  };
  %1 = %0(%a, %b);
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01)
  };
  %3 = %2(%1);
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1)
  };
  %4(%3, %b)
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass SimplifyInference
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass SimplifyExpr
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Inline
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b);
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1);
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b)
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass DeadCodeElimination
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InlinePrimitives
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldScaleAxis
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToANormalForm
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %x = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  let %x1 = %x(%a, %b);
  let %x2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  let %x3 = %x2(%x1);
  let %x4 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  let %x5 = %x4(%x3, %b);
  %x5
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %x: fn (Tensor[(?, 4), float32], Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  let %x1: Tensor[(?, 4), float32] = %x(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  let %x2: fn (Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  let %x3: Tensor[(?, 4), float32] = %x2(%x1) /* ty=Tensor[(?, 4), float32] */;
  let %x4: fn (Tensor[(?, 4), float32], Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  let %x5: Tensor[(?, 4), float32] = %x4(%x3, %b) /* ty=Tensor[(?, 4), float32] */;
  %x5
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass LambdaLift
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %x: fn (Tensor[(?, 4), float32], Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  let %x1: Tensor[(?, 4), float32] = %x(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  let %x2: fn (Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  let %x3: Tensor[(?, 4), float32] = %x2(%x1) /* ty=Tensor[(?, 4), float32] */;
  let %x4: fn (Tensor[(?, 4), float32], Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  let %x5: Tensor[(?, 4), float32] = %x4(%x3, %b) /* ty=Tensor[(?, 4), float32] */;
  %x5
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Inline
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  let %x: fn (Tensor[(?, 4), float32], Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = %0;
  let %x1: Tensor[(?, 4), float32] = %0(%a, %b);
  %1 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  let %x2: fn (Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = %1;
  let %x3: Tensor[(?, 4), float32] = %1(%x1);
  %2 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  let %x4: fn (Tensor[(?, 4), float32], Tensor[(?, 4), float32]) -> Tensor[(?, 4), float32] = %2;
  let %x5: Tensor[(?, 4), float32] = %2(%x3, %b);
  %x5
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass DeadCodeElimination
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  let %x: Tensor[(?, 4), float32] = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  let %x1: Tensor[(?, 4), float32] = %1(%x) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  let %x2: Tensor[(?, 4), float32] = %2(%x1, %b) /* ty=Tensor[(?, 4), float32] */;
  %x2
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InlinePrimitives
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  let %x: Tensor[(?, 4), float32] = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  let %x1: Tensor[(?, 4), float32] = %1(%x) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  let %x2: Tensor[(?, 4), float32] = %2(%x1, %b) /* ty=Tensor[(?, 4), float32] */;
  %x2
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InlineGlobals
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  let %x: Tensor[(?, 4), float32] = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  let %x1: Tensor[(?, 4), float32] = %1(%x) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  let %x2: Tensor[(?, 4), float32] = %2(%x1, %b) /* ty=Tensor[(?, 4), float32] */;
  %x2
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass RemoveUnusedFunctions
def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  let %x: Tensor[(?, 4), float32] = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %1 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  let %x1: Tensor[(?, 4), float32] = %1(%x) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  let %x2: Tensor[(?, 4), float32] = %2(%x1, %b) /* ty=Tensor[(?, 4), float32] */;
  %x2
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [buf__broadcast_shape_func] "realize_scope" = "";
  realize(buf__broadcast_shape_func, [0:2], True {
    attr [0] "extern_scope" = 0 {
      if (placeholder[1] == placeholder_1[1]) {
        buf__broadcast_shape_func[1] = placeholder[1]
      } else {
        if (placeholder[1] == 1i64) {
          buf__broadcast_shape_func[1] = placeholder_1[1]
        } else {
          assert((placeholder_1[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
          0
          buf__broadcast_shape_func[1] = placeholder[1]
        }
      }
      if (placeholder[0] == placeholder_1[0]) {
        buf__broadcast_shape_func[0] = placeholder[0]
      } else {
        if (placeholder[0] == 1i64) {
          buf__broadcast_shape_func[0] = placeholder_1[0]
        } else {
          assert((placeholder_1[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
          0
          buf__broadcast_shape_func[0] = placeholder[0]
        }
      }
    }
  })
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add", "tir.noalias": True}
  buffers = {buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  attr [compute] "realize_scope" = "";
  realize(compute, [0:2], True {
    for (i0: int32, 0, 2) {
      compute[i0] = placeholder[i0]
    }
  })
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ManifestAlloc
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  %3 = add(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */;
  %4 = prod(%shape_func_out_0) /* ty=int64 */;
  %5 = divide(%3, 8 /* ty=int64 */) /* ty=int64 */;
  %6 = multiply(%4, %5) /* ty=int64 */;
  let %storage_01: Storage[] = memory.alloc_storage(%6, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(?, 4), float32] */;
  %7 = (%a, %b);
  %8 = (%out_0,);
  let %x: () = vm.invoke_tvm_op(%0, %7, %8) /* ty=() */;
  let %x1: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x1, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_02: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(2), int64] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_01;
  %9 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %10 = (%in_shape_01,);
  %11 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%9, %10, %11, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  %12 = add(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */;
  %13 = prod(%shape_func_out_01) /* ty=int64 */;
  %14 = divide(%12, 8 /* ty=int64 */) /* ty=int64 */;
  %15 = multiply(%13, %14) /* ty=int64 */;
  let %storage_03: Storage[] = memory.alloc_storage(%15, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %16 = (%x1,);
  %17 = (%out_01,);
  let %x2: () = vm.invoke_tvm_op(%9, %16, %17) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_02;
  %18 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %19 = (%in_shape_02, %in_shape_11);
  %20 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%18, %19, %20, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  %21 = add(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */;
  %22 = prod(%shape_func_out_02) /* ty=int64 */;
  %23 = divide(%21, 8 /* ty=int64 */) /* ty=int64 */;
  %24 = multiply(%22, %23) /* ty=int64 */;
  let %storage_05: Storage[] = memory.alloc_storage(%24, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=Tensor[(?, 4), float32] */;
  %25 = (%x3, %b);
  %26 = (%out_02,);
  let %x4: () = vm.invoke_tvm_op(%18, %25, %26) /* ty=() */;
  let %x5: Tensor[(?, 4), float32] = %out_02;
  %x5
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main() -> int64 {
  %0 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  %0(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToANormalForm
type Storage {
  
}

def @main() -> int64 {
  let %x = 32 /* ty=int64 */;
  let %x1 = 7 /* ty=int64 */;
  let %x2 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3 = %x2(%x, %x1);
  %x3
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass EtaExpand
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1)
  };
  let %x3: int64 = %x2(%x, %x1);
  %x3
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [T_add] "realize_scope" = "";
  realize(T_add, [], True {
    T_add[] = (placeholder[] + placeholder_1[])
  })
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerReduction
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.PlanAndUpdateBufferAllocationLocation
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ConvertBlocksToOpaque
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.CompactBufferAllocation
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.FlattenBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VerifyMemory
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Apply
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InferFragment
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerThreadAllreduce
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  T_add_2[0] = ((int64*)placeholder_4[0] + (int64*)placeholder_5[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerWarpMemory

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin

[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Apply
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:25] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_add_compute_";
  T_add[0] = ((int64*)placeholder[0] + (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main() -> int64 {
  %0 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  %0(39 /* ty=int64 */, 8 /* ty=int64 */) /* ty=int64 */
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToANormalForm
type Storage {
  
}

def @main() -> int64 {
  let %x = 39 /* ty=int64 */;
  let %x1 = 8 /* ty=int64 */;
  let %x2 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass EtaExpand
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1)
  };
  let %x3: int64 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  attr [T_divide] "realize_scope" = "";
  realize(T_divide, [], True {
    T_divide[] = (placeholder[] / placeholder_1[])
  })
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerReduction
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.PlanAndUpdateBufferAllocationLocation
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ConvertBlocksToOpaque
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.CompactBufferAllocation
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.FlattenBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VerifyMemory
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Apply
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InferFragment
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerThreadAllreduce
primfn(placeholder_2: handle, placeholder_3: handle, T_divide_1: handle) -> ()
  attr = {"global_symbol": "fused_divide", "tir.noalias": True, "target": meta[Target][0], "tir.is_entry_func": True}
  buffers = {T_divide: Buffer(T_divide_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_4: Pointer(int64), int64, [], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_divide_1: T_divide} {
  T_divide_2[0] = ((int64*)placeholder_4[0] / (int64*)placeholder_5[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerWarpMemory

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Apply
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_divide", "tir.is_entry_func": True, "calling_conv": 1} {
  assert((num_args == 3), "fused_divide: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_divide: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_divide] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_divide: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_divide: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_divide: Expect arg[2] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
  attr [0] "compute_scope" = "fused_divide_compute_";
  T_divide[0] = ((int64*)placeholder[0] / (int64*)placeholder_1[0])
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main() -> int64 {
  %0 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  %0(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToANormalForm
type Storage {
  
}

def @main() -> int64 {
  let %x = 32 /* ty=int64 */;
  let %x1 = 7 /* ty=int64 */;
  let %x2 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass EtaExpand
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1)
  };
  let %x3: int64 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main() -> int64 {
  %0 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  %0(39 /* ty=int64 */, 8 /* ty=int64 */) /* ty=int64 */
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToANormalForm
type Storage {
  
}

def @main() -> int64 {
  let %x = 39 /* ty=int64 */;
  let %x1 = 8 /* ty=int64 */;
  let %x2 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass EtaExpand
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1)
  };
  let %x3: int64 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main() -> int64 {
  %0 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  %0(32 /* ty=int64 */, 7 /* ty=int64 */) /* ty=int64 */
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToANormalForm
type Storage {
  
}

def @main() -> int64 {
  let %x = 32 /* ty=int64 */;
  let %x1 = 7 /* ty=int64 */;
  let %x2 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass EtaExpand
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1)
  };
  let %x3: int64 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 32 /* ty=int64 */;
  let %x1: int64 = 7 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    add(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main() -> int64 {
  %0 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  %0(39 /* ty=int64 */, 8 /* ty=int64 */) /* ty=int64 */
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToANormalForm
type Storage {
  
}

def @main() -> int64 {
  let %x = 39 /* ty=int64 */;
  let %x1 = 8 /* ty=int64 */;
  let %x2 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass EtaExpand
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1)
  };
  let %x3: int64 = %x2(%x, %x1);
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main() -> int64 {
  let %x: int64 = 39 /* ty=int64 */;
  let %x1: int64 = 8 /* ty=int64 */;
  let %x2: fn (int64, int64) -> int64 = fn (%p0: int64, %p1: int64, Primitive=1) -> int64 {
    divide(%p0, %p1) /* ty=int64 */
  };
  let %x3: int64 = %x2(%x, %x1) /* ty=int64 */;
  %x3
}

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  %3 = prod(%shape_func_out_0) /* ty=int64 */;
  %4 = multiply(%3, 4 /* ty=int64 */) /* ty=int64 */;
  let %storage_01: Storage[] = memory.alloc_storage(%4, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(?, 4), float32] */;
  %5 = (%a, %b);
  %6 = (%out_0,);
  let %x: () = vm.invoke_tvm_op(%0, %5, %6) /* ty=() */;
  let %x1: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x1, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_02: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(2), int64] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_01;
  %7 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %8 = (%in_shape_01,);
  %9 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%7, %8, %9, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  %10 = prod(%shape_func_out_01) /* ty=int64 */;
  %11 = multiply(%10, 4 /* ty=int64 */) /* ty=int64 */;
  let %storage_03: Storage[] = memory.alloc_storage(%11, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %12 = (%x1,);
  %13 = (%out_01,);
  let %x2: () = vm.invoke_tvm_op(%7, %12, %13) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_02;
  %14 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %15 = (%in_shape_02, %in_shape_11);
  %16 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%14, %15, %16, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  %17 = prod(%shape_func_out_02) /* ty=int64 */;
  %18 = multiply(%17, 4 /* ty=int64 */) /* ty=int64 */;
  let %storage_05: Storage[] = memory.alloc_storage(%18, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=Tensor[(?, 4), float32] */;
  %19 = (%x3, %b);
  %20 = (%out_02,);
  let %x4: () = vm.invoke_tvm_op(%14, %19, %20) /* ty=() */;
  let %x5: Tensor[(?, 4), float32] = %out_02;
  %x5
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  %3 = fn (%p02: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p02) /* ty=int64 */
  };
  %4 = %3(%shape_func_out_0) /* ty=int64 */;
  %5 = fn (%p01: int64, Primitive=1) -> int64 {
    multiply(%p01, 4 /* ty=int64 */) /* ty=int64 */
  };
  %6 = %5(%4) /* ty=int64 */;
  let %storage_01: Storage[] = memory.alloc_storage(%6, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(?, 4), float32] */;
  %7 = (%a, %b);
  %8 = (%out_0,);
  let %x: () = vm.invoke_tvm_op(%0, %7, %8) /* ty=() */;
  let %x1: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x1, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_02: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(2), int64] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_01;
  %9 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %10 = (%in_shape_01,);
  %11 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%9, %10, %11, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  %12 = fn (%p05: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p05) /* ty=int64 */
  };
  %13 = %12(%shape_func_out_01) /* ty=int64 */;
  %14 = fn (%p04: int64, Primitive=1) -> int64 {
    multiply(%p04, 4 /* ty=int64 */) /* ty=int64 */
  };
  %15 = %14(%13) /* ty=int64 */;
  let %storage_03: Storage[] = memory.alloc_storage(%15, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %16 = (%x1,);
  %17 = (%out_01,);
  let %x2: () = vm.invoke_tvm_op(%9, %16, %17) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_02;
  %18 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %19 = (%in_shape_02, %in_shape_11);
  %20 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%18, %19, %20, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  %21 = fn (%p08: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p08) /* ty=int64 */
  };
  %22 = %21(%shape_func_out_02) /* ty=int64 */;
  %23 = fn (%p07: int64, Primitive=1) -> int64 {
    multiply(%p07, 4 /* ty=int64 */) /* ty=int64 */
  };
  %24 = %23(%22) /* ty=int64 */;
  let %storage_05: Storage[] = memory.alloc_storage(%24, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=Tensor[(?, 4), float32] */;
  %25 = (%x3, %b);
  %26 = (%out_02,);
  let %x4: () = vm.invoke_tvm_op(%18, %25, %26) /* ty=() */;
  let %x5: Tensor[(?, 4), float32] = %out_02;
  %x5
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ManifestAlloc
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1) -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1) -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1) -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1) -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1) -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1) -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1) -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1) -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ManifestAlloc
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1) -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1) -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1) -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1) -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass sequential
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1) -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1) -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1) -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1) -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1) -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1) -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass LabelOps
type Storage {
  
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="60fac443288c994e") -> Tensor[(?, 4), float32] {
  let %in_shape_0: Tensor[(2), int64] = vm.shape_of(%a, meta[relay.attrs.ShapeOfAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %in_shape_1: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][1]) /* ty=Tensor[(2), int64] */;
  let %storage_0: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(2), int64] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_0: Tensor[(2), int64] = %tensor_0;
  %0 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="da02eedf6be1490e") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = (%in_shape_0, %in_shape_1);
  %2 = (%shape_func_out_0,);
  let %shape_func: () = vm.shape_func(%0, %1, %2, meta[relay.attrs.ShapeFuncAttrs][0]) /* ty=() */;
  let %storage_01: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: int64 = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=int64 */;
  %3 = fn (%p01: Tensor[(2), int64], Primitive=1, hash="e04f03199b1f472a") -> int64 {
    prod(%p01) /* ty=int64 */
  };
  %4 = (%shape_func_out_0,);
  %5 = (%tensor_01,);
  let %x: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %storage_02: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: int64 = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=int64 */;
  %6 = fn (%p02: int64, Primitive=1, hash="e2e2680f0ff08f46") -> int64 {
    multiply(%p02, 4 /* ty=int64 */) /* ty=int64 */
  };
  %7 = (%tensor_01,);
  %8 = (%tensor_02,);
  let %x1: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %storage_03: Storage[] = memory.alloc_storage(%tensor_02, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][3]) /* ty=Storage[] */;
  let %out_0: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_03, 0 /* ty=int64 */, %shape_func_out_0, meta[relay.attrs.AllocTensorAttrs][3]) /* ty=Tensor[(?, 4), float32] */;
  %9 = (%a, %b);
  %10 = (%out_0,);
  let %x2: () = vm.invoke_tvm_op(%0, %9, %10) /* ty=() */;
  let %x3: Tensor[(?, 4), float32] = %out_0;
  let %in_shape_01: Tensor[(2), int64] = vm.shape_of(%x3, meta[relay.attrs.ShapeOfAttrs][2]) /* ty=Tensor[(2), int64] */;
  let %storage_04: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][4]) /* ty=Storage[] */;
  let %tensor_03: Tensor[(2), int64] = memory.alloc_tensor(%storage_04, 0 /* ty=int64 */, meta[relay.Constant][3] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_01: Tensor[(2), int64] = %tensor_03;
  %11 = fn (%p03: Tensor[(?, 4), float32], Primitive=1, hash="04957ee4fbd3de74") -> Tensor[(?, 4), float32] {
    nn.softmax(%p03) /* ty=Tensor[(?, 4), float32] */
  };
  %12 = (%in_shape_01,);
  %13 = (%shape_func_out_01,);
  let %shape_func1: () = vm.shape_func(%11, %12, %13, meta[relay.attrs.ShapeFuncAttrs][1]) /* ty=() */;
  let %storage_05: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][5]) /* ty=Storage[] */;
  let %tensor_04: int64 = memory.alloc_tensor(%storage_05, 0 /* ty=int64 */, meta[relay.Constant][4] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][5]) /* ty=int64 */;
  %14 = fn (%p04: Tensor[(2), int64], Primitive=1, hash="e04f03199b1f472a") -> int64 {
    prod(%p04) /* ty=int64 */
  };
  %15 = (%shape_func_out_01,);
  %16 = (%tensor_04,);
  let %x4: () = vm.invoke_tvm_op(%14, %15, %16) /* ty=() */;
  let %storage_06: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][6]) /* ty=Storage[] */;
  let %tensor_05: int64 = memory.alloc_tensor(%storage_06, 0 /* ty=int64 */, meta[relay.Constant][5] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][6]) /* ty=int64 */;
  %17 = fn (%p05: int64, Primitive=1, hash="e2e2680f0ff08f46") -> int64 {
    multiply(%p05, 4 /* ty=int64 */) /* ty=int64 */
  };
  %18 = (%tensor_04,);
  %19 = (%tensor_05,);
  let %x5: () = vm.invoke_tvm_op(%17, %18, %19) /* ty=() */;
  let %storage_07: Storage[] = memory.alloc_storage(%tensor_05, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][7]) /* ty=Storage[] */;
  let %out_01: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_07, 0 /* ty=int64 */, %shape_func_out_01, meta[relay.attrs.AllocTensorAttrs][7]) /* ty=Tensor[(?, 4), float32] */;
  %20 = (%x3,);
  %21 = (%out_01,);
  let %x6: () = vm.invoke_tvm_op(%11, %20, %21) /* ty=() */;
  let %x7: Tensor[(?, 4), float32] = %out_01;
  let %in_shape_02: Tensor[(2), int64] = vm.shape_of(%x7, meta[relay.attrs.ShapeOfAttrs][3]) /* ty=Tensor[(2), int64] */;
  let %in_shape_11: Tensor[(2), int64] = vm.shape_of(%b, meta[relay.attrs.ShapeOfAttrs][4]) /* ty=Tensor[(2), int64] */;
  let %storage_08: Storage[] = memory.alloc_storage(16 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][8]) /* ty=Storage[] */;
  let %tensor_06: Tensor[(2), int64] = memory.alloc_tensor(%storage_08, 0 /* ty=int64 */, meta[relay.Constant][6] /* ty=Tensor[(1), int64] */, meta[relay.attrs.AllocTensorAttrs][8]) /* ty=Tensor[(2), int64] */;
  let %shape_func_out_02: Tensor[(2), int64] = %tensor_06;
  %22 = fn (%p06: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="da02eedf6be1490e") -> Tensor[(?, 4), float32] {
    add(%p06, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %23 = (%in_shape_02, %in_shape_11);
  %24 = (%shape_func_out_02,);
  let %shape_func2: () = vm.shape_func(%22, %23, %24, meta[relay.attrs.ShapeFuncAttrs][2]) /* ty=() */;
  let %storage_09: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][9]) /* ty=Storage[] */;
  let %tensor_07: int64 = memory.alloc_tensor(%storage_09, 0 /* ty=int64 */, meta[relay.Constant][7] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][9]) /* ty=int64 */;
  %25 = fn (%p07: Tensor[(2), int64], Primitive=1, hash="e04f03199b1f472a") -> int64 {
    prod(%p07) /* ty=int64 */
  };
  %26 = (%shape_func_out_02,);
  %27 = (%tensor_07,);
  let %x8: () = vm.invoke_tvm_op(%25, %26, %27) /* ty=() */;
  let %storage_010: Storage[] = memory.alloc_storage(8 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][10]) /* ty=Storage[] */;
  let %tensor_08: int64 = memory.alloc_tensor(%storage_010, 0 /* ty=int64 */, meta[relay.Constant][8] /* ty=Tensor[(0), int64] */, meta[relay.attrs.AllocTensorAttrs][10]) /* ty=int64 */;
  %28 = fn (%p08: int64, Primitive=1, hash="e2e2680f0ff08f46") -> int64 {
    multiply(%p08, 4 /* ty=int64 */) /* ty=int64 */
  };
  %29 = (%tensor_07,);
  %30 = (%tensor_08,);
  let %x9: () = vm.invoke_tvm_op(%28, %29, %30) /* ty=() */;
  let %storage_011: Storage[] = memory.alloc_storage(%tensor_08, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][11]) /* ty=Storage[] */;
  let %out_02: Tensor[(?, 4), float32] = memory.alloc_tensor(%storage_011, 0 /* ty=int64 */, %shape_func_out_02, meta[relay.attrs.AllocTensorAttrs][11]) /* ty=Tensor[(?, 4), float32] */;
  %31 = (%x7, %b);
  %32 = (%out_02,);
  let %x10: () = vm.invoke_tvm_op(%22, %31, %32) /* ty=() */;
  let %x11: Tensor[(?, 4), float32] = %out_02;
  %x11
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [buf__broadcast_shape_func] "realize_scope" = "";
  realize(buf__broadcast_shape_func, [0:2], True {
    attr [0] "extern_scope" = 0 {
      if (placeholder[1] == placeholder_1[1]) {
        buf__broadcast_shape_func[1] = placeholder[1]
      } else {
        if (placeholder[1] == 1i64) {
          buf__broadcast_shape_func[1] = placeholder_1[1]
        } else {
          assert((placeholder_1[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
          0
          buf__broadcast_shape_func[1] = placeholder[1]
        }
      }
      if (placeholder[0] == placeholder_1[0]) {
        buf__broadcast_shape_func[0] = placeholder[0]
      } else {
        if (placeholder[0] == 1i64) {
          buf__broadcast_shape_func[0] = placeholder_1[0]
        } else {
          assert((placeholder_1[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
          0
          buf__broadcast_shape_func[0] = placeholder[0]
        }
      }
    }
  })
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  attr [placeholder_red] "realize_scope" = "";
  realize(placeholder_red, [], True {
    placeholder_red[] = 1i64
    for (k0: int32, 0, 2) {
      placeholder_red[] = (placeholder_red[]*placeholder[k0])
    }
  })
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_1: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_2[k0])
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  attr [T_multiply] "realize_scope" = "";
  realize(T_multiply, [], True {
    T_multiply[] = (placeholder[]*4i64)
  })
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_1: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_1: placeholder, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_2[0]*4i64)
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [T_add] "realize_scope" = "";
  realize(T_add, [0:max(any_dim, any_dim_1), 0:4], True {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
    if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), 4) < max(any_dim, any_dim_1)), dtype=bool) {
      if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < (max(any_dim, any_dim_1)*4)), dtype=bool) {
        T_add[floordiv((threadIdx.x + (blockIdx.x*512)), 4), floormod((threadIdx.x + (blockIdx.x*512)), 4)] = (placeholder[floordiv((threadIdx.x + (blockIdx.x*512)), 4), floormod((threadIdx.x + (blockIdx.x*512)), 4)] + placeholder_1[floordiv((threadIdx.x + (blockIdx.x*512)), 4), floormod((threadIdx.x + (blockIdx.x*512)), 4)])
      }
    }
  })
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), 4) < max(any_dim, any_dim_1)), dtype=bool) {
    if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < (max(any_dim, any_dim_1)*4)), dtype=bool) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), 4) < max(any_dim, any_dim_1)), dtype=bool) {
    if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < (max(any_dim, any_dim_1)*4)), dtype=bool) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), 4) < max(any_dim, any_dim_1)), dtype=bool) {
    if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < (max(any_dim, any_dim_1)*4)), dtype=bool) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), 4) < max(any_dim, any_dim_1)), dtype=bool) {
    if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < (max(any_dim, any_dim_1)*4)), dtype=bool) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), 4) < max(any_dim, any_dim_1)), dtype=bool) {
    if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < (max(any_dim, any_dim_1)*4)), dtype=bool) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if @tir.likely((floordiv((threadIdx.x + (blockIdx.x*512)), 4) < max(any_dim, any_dim_1)), dtype=bool) {
    if @tir.likely(((threadIdx.x + (blockIdx.x*512)) < (max(any_dim, any_dim_1)*4)), dtype=bool) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if @tir.likely(((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)), dtype=bool) {
    if @tir.likely((((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)), dtype=bool) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/tir/transforms/loop_partition.cc:554: Warning: Cannot prove: ((((floordiv(((max(any_dim, any_dim)*4) + 511), 512) - 1) - floordiv(max(any_dim, any_dim), 128)) + 1) >= 0), when generating the post doubt loop
[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < min(floordiv(max(any_dim, any_dim_1), 128), ((floordiv(((max(any_dim, any_dim_1)*4) + 511), 512) - 1) + 1))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if True {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < min(floordiv(max(any_dim, any_dim_1), 128), ((floordiv(((max(any_dim, any_dim_1)*4) + 511), 512) - 1) + 1))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if True {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < min(floordiv(max(any_dim, any_dim_1), 128), ((floordiv(((max(any_dim, any_dim_1)*4) + 511), 512) - 1) + 1))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if True {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < min(floordiv(max(any_dim, any_dim_1), 128), ((floordiv(((max(any_dim, any_dim_1)*4) + 511), 512) - 1) + 1))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if True {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < min(floordiv(max(any_dim, any_dim_1), 128), ((floordiv(((max(any_dim, any_dim_1)*4) + 511), 512) - 1) + 1))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if True {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if (blockIdx.x < min(floordiv(max(any_dim, any_dim_1), 128), ((floordiv(((max(any_dim, any_dim_1)*4) + 511), 512) - 1) + 1))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if True {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  attr [compute] "realize_scope" = "";
  realize(compute, [0:2], True {
    for (i0: int32, 0, 2) {
      compute[i0] = placeholder[i0]
    }
  })
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder: Buffer(placeholder_2: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_1: placeholder, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_2[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_norm] "realize_scope" = "";
  realize(T_softmax_norm, [0:any_dim, 0:4], True {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
    attr [T_softmax_exp: Buffer(T_softmax_exp_1: Pointer(float32), float32, [any_dim, 4], [])] "realize_scope" = "warp";
    realize(T_softmax_exp, [blockIdx.x:(blockIdx.x + 1), 0:4], True {
      attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
      attr [T_softmax_maxelem: Buffer(T_softmax_maxelem_1: Pointer(float32), float32, [any_dim], [])] "realize_scope" = "";
      realize(T_softmax_maxelem, [blockIdx.x:(blockIdx.x + 1)], True {
        attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
        allocate(normal_reduce_temp0, float32, [1]);
        attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
        allocate(reduce_temp0, float32, [1]) {
          normal_reduce_temp0[0] = -3.40282e+38f32
          if (threadIdx.x < 4) {
            if True {
              normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], placeholder[blockIdx.x, threadIdx.x])
            }
          }
          attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
          @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
          if True {
            T_softmax_maxelem[blockIdx.x] = (float32*)reduce_temp0[0]
          }
        }
        if @tir.likely((threadIdx.x < 4), dtype=bool) {
          T_softmax_exp[blockIdx.x, threadIdx.x] = @tir.exp((placeholder[blockIdx.x, threadIdx.x] - T_softmax_maxelem[blockIdx.x]), dtype=float32)
        }
      })
      attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
      attr [T_softmax_expsum: Buffer(T_softmax_expsum_1: Pointer(float32), float32, [any_dim], [])] "realize_scope" = "";
      realize(T_softmax_expsum, [blockIdx.x:(blockIdx.x + 1)], True {
        attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
        allocate(normal_reduce_temp0_1, float32, [1]);
        attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
        allocate(reduce_temp0_1, float32, [1]) {
          normal_reduce_temp0_1[0] = 0f32
          if (threadIdx.x < 4) {
            if True {
              normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + T_softmax_exp[blockIdx.x, threadIdx.x])
            }
          }
          attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
          @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
          if True {
            T_softmax_expsum[blockIdx.x] = (float32*)reduce_temp0_1[0]
          }
        }
        if @tir.likely((threadIdx.x < 4), dtype=bool) {
          T_softmax_norm[blockIdx.x, threadIdx.x] = (T_softmax_exp[blockIdx.x, threadIdx.x] / T_softmax_expsum[blockIdx.x])
        }
      })
    })
  })
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        reduce_temp0[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        reduce_temp0_1[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        reduce_temp0[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        reduce_temp0_1[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        0
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        0
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim: int32, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [any_dim, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*stride) + (threadIdx.x*stride_1))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:980: LOWER END

[14:32:26] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1155: CODEGEN START

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim_2: int32, 4], [stride_4: int32, stride_5: int32], type="auto"),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [any_dim_2, 4], [stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim_2;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*stride_4) + (threadIdx.x_1*stride_5))] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VerifyMemory
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim_2: int32, 4], [stride_4: int32, stride_5: int32], type="auto"),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [any_dim_2, 4], [stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim_2;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*stride_4) + (threadIdx.x_1*stride_5))] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim_2: int32, 4], [stride_4: int32, stride_5: int32], type="auto"),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [any_dim_2, 4], [stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim_2;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*stride_4) + (threadIdx.x_1*stride_5))] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim_2: int32, 4], [stride_4: int32, stride_5: int32], type="auto"),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [any_dim_2, 4], [stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim_2;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*stride_4) + (threadIdx.x_1*stride_5))] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InferFragment
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim_2: int32, 4], [stride_4: int32, stride_5: int32], type="auto"),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [any_dim_2, 4], [stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim_2;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*stride_4) + (threadIdx.x_1*stride_5))] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerThreadAllreduce
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [max(any_dim: int32, any_dim_1: int32), 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [any_dim, 4], [stride: int32, stride_1: int32], type="auto"),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [any_dim_1, 4], [stride_2: int32, stride_3: int32], type="auto")}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
        T_add_2[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride) + (floormod(threadIdx.x, 4)*stride_1))] + (float32*)placeholder_5[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_2) + (floormod(threadIdx.x, 4)*stride_3))])
      }
    }
  }
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [any_dim_2: int32, 4], [stride_4: int32, stride_5: int32], type="auto"),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [any_dim_2, 4], [stride_6: int32, stride_7: int32], type="auto")}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim_2;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*stride_6) + (threadIdx.x_1*stride_7))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*stride_4) + (threadIdx.x_1*stride_5))] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)red_buf0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"global_symbol": "fused_add_1", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
      @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
      attr [0] "compute_scope" = "fused_add_1_compute_";
      attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim, any_dim_1)*4) + 511), 512);
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
      if ((blockIdx.x < floordiv(max(any_dim, any_dim_1), 128)) && (blockIdx.x < floordiv(((max(any_dim, any_dim_1)*4) + 511), 512))) {
        if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
          T_add[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_1) + (floormod(threadIdx.x, 4)*stride))] + (float32*)placeholder_1[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_3) + (floormod(threadIdx.x, 4)*stride_2))])
        }
      } else {
        if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_1)) {
          if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim, any_dim_1)*4)) {
            T_add[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_1) + (floormod(threadIdx.x, 4)*stride))] + (float32*)placeholder_1[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_3) + (floormod(threadIdx.x, 4)*stride_2))])
          }
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
    @tir.tvm_call_packed("__tvm_set_device", 2, dev_id_1, dtype=int32)
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim_2;
    attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
    allocate(normal_reduce_temp0, float32, [1]);
    attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
    allocate(red_buf0, float32, [1]);
    attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
    allocate(T_softmax_exp, float32, [4]);
    attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
    allocate(normal_reduce_temp0_1, float32, [1]);
    attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
    allocate(red_buf0_1, float32, [1]) {
      attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x_1 < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x_1*stride_5) + (threadIdx.x_1*stride_4))])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        attr [mask: Pointer(uint32)] "storage_scope" = "local";
        allocate(mask, uint32, [1]);
        attr [t0: Pointer(float32)] "storage_scope" = "local";
        allocate(t0, float32, [1]) {
          red_buf0[0] = (float32*)normal_reduce_temp0[0]
          mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
          t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
          red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
          t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
          red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
          t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
          red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
          t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
          red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
          t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
          red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
          red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
        }
        if (threadIdx.x_1 < 4) {
          T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_2[((blockIdx.x_1*stride_5) + (threadIdx.x_1*stride_4))] - (float32*)red_buf0[0]), dtype=float32)
        }
      }
      attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x_1 < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
        allocate(mask_1, uint32, [1]);
        attr [t0_1: Pointer(float32)] "storage_scope" = "local";
        allocate(t0_1, float32, [1]) {
          red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
          mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
          t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
          red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
          t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
          red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
          t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
          red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
          t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
          red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
          t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
          red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
          red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
        }
        if (threadIdx.x_1 < 4) {
          T_softmax_norm[((blockIdx.x_1*stride_7) + (threadIdx.x_1*stride_6))] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)red_buf0_1[0])
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_add_1", "calling_conv": 1} {
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
      @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
      attr [0] "compute_scope" = "fused_add_1_compute_";
      @tir.tvm_call_packed("fused_add_1_kernel0", T_add, placeholder, placeholder_1, any_dim, any_dim_1, stride_1, stride, stride_3, stride_2, floordiv(((max(any_dim, any_dim_1)*4) + 511), 512), 512, dtype=int32)
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
    @tir.tvm_call_packed("__tvm_set_device", 2, dev_id_1, dtype=int32)
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    @tir.tvm_call_packed("fused_nn_softmax_kernel0", placeholder_2, T_softmax_norm, stride_5, stride_4, stride_7, stride_6, any_dim_2, 32, dtype=int32)
  }
}

primfn(T_add_1: Pointer(float32), placeholder_3: Pointer(float32), placeholder_4: Pointer(float32), any_dim_3: int32, any_dim_4: int32, stride_8: int32, stride_9: int32, stride_10: int32, stride_11: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_1_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim_3, any_dim_4)*4) + 511), 512);
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x < floordiv(max(any_dim_3, any_dim_4), 128)) && (blockIdx.x < floordiv(((max(any_dim_3, any_dim_4)*4) + 511), 512))) {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_3) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_4)) {
      T_add_1[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_3[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_8) + (floormod(threadIdx.x, 4)*stride_9))] + (float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_10) + (floormod(threadIdx.x, 4)*stride_11))])
    }
  } else {
    if ((((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_3) || (((blockIdx.x*128) + floordiv(threadIdx.x, 4)) < any_dim_4)) {
      if (((blockIdx.x*512) + threadIdx.x) < (max(any_dim_3, any_dim_4)*4)) {
        T_add_1[((blockIdx.x*512) + threadIdx.x)] = ((float32*)placeholder_3[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_8) + (floormod(threadIdx.x, 4)*stride_9))] + (float32*)placeholder_4[((((blockIdx.x*128) + floordiv(threadIdx.x, 4))*stride_10) + (floormod(threadIdx.x, 4)*stride_11))])
      }
    }
  }
}

primfn(placeholder_5: Pointer(float32), T_softmax_norm_1: Pointer(float32), stride_12: int32, stride_13: int32, stride_14: int32, stride_15: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim_5: int32;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_5[((blockIdx.x_1*stride_12) + (threadIdx.x_1*stride_13))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_5[((blockIdx.x_1*stride_12) + (threadIdx.x_1*stride_13))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_1[((blockIdx.x_1*stride_14) + (threadIdx.x_1*stride_15))] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)red_buf0_1[0])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_add_1", "calling_conv": 1} {
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
      @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
      attr [0] "compute_scope" = "fused_add_1_compute_";
      @tir.tvm_call_packed("fused_add_1_kernel0", T_add, placeholder, placeholder_1, any_dim, any_dim_1, stride_1, stride, stride_3, stride_2, floordiv(((max(any_dim, any_dim_1)*4) + 511), 512), 512, dtype=int32)
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
    @tir.tvm_call_packed("__tvm_set_device", 2, dev_id_1, dtype=int32)
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    @tir.tvm_call_packed("fused_nn_softmax_kernel0", placeholder_2, T_softmax_norm, stride_5, stride_4, stride_7, stride_6, any_dim_2, 32, dtype=int32)
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add_1", "calling_conv": 1} {
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
      @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
      attr [0] "compute_scope" = "fused_add_1_compute_";
      @tir.tvm_call_packed("fused_add_1_kernel0", T_add, placeholder, placeholder_1, any_dim, any_dim_1, stride_1, stride, stride_3, stride_2, floordiv(((max(any_dim, any_dim_1)*4) + 511), 512), 512, dtype=int32)
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
    @tir.tvm_call_packed("__tvm_set_device", 2, dev_id_1, dtype=int32)
    attr [0] "compute_scope" = "fused_nn_softmax_compute_";
    @tir.tvm_call_packed("fused_nn_softmax_kernel0", placeholder_2, T_softmax_norm, stride_5, stride_4, stride_7, stride_6, any_dim_2, 32, dtype=int32)
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add_1", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 12, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 12, dtype=handle)
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
       {
        @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
        stack_tcode[0] = 0
        @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
        stack_tcode[1] = 0
        @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
      }
      attr [0] "compute_scope" = "fused_add_1_compute_" {
        @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
        stack_tcode[0] = 3
        @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
        stack_tcode[1] = 3
        @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
        stack_tcode[2] = 3
        @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, any_dim), dtype=int32)
        stack_tcode[3] = 0
        @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, any_dim_1), dtype=int32)
        stack_tcode[4] = 0
        @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, stride_1), dtype=int32)
        stack_tcode[5] = 0
        @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride), dtype=int32)
        stack_tcode[6] = 0
        @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
        stack_tcode[7] = 0
        @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
        stack_tcode[8] = 0
        @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, floordiv(((max(any_dim, any_dim_1)*4) + 511), 512)), dtype=int32)
        stack_tcode[9] = 0
        @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, 512), dtype=int32)
        stack_tcode[10] = 0
        @tir.tvm_call_packed_lowered("fused_add_1_kernel0", stack_value, stack_tcode, 0, 11, dtype=int32)
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 9, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 9, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode_1[0] = 0
      @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
      stack_tcode_1[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
      @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
      stack_tcode_1[0] = 3
      @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
      stack_tcode_1[1] = 3
      @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, stride_5), dtype=int32)
      stack_tcode_1[2] = 0
      @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, stride_4), dtype=int32)
      stack_tcode_1[3] = 0
      @tir.tvm_struct_set(stack_value_1, 4, 12, cast(int64, stride_7), dtype=int32)
      stack_tcode_1[4] = 0
      @tir.tvm_struct_set(stack_value_1, 5, 12, cast(int64, stride_6), dtype=int32)
      stack_tcode_1[5] = 0
      @tir.tvm_struct_set(stack_value_1, 6, 12, cast(int64, any_dim_2), dtype=int32)
      stack_tcode_1[6] = 0
      @tir.tvm_struct_set(stack_value_1, 7, 12, cast(int64, 32), dtype=int32)
      stack_tcode_1[7] = 0
      @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 8, dtype=int32)
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add_1", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 12, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 12, dtype=handle)
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
       {
        @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
        stack_tcode[0] = 0
        @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
        stack_tcode[1] = 0
        @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
      }
      attr [0] "compute_scope" = "fused_add_1_compute_" {
        @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
        stack_tcode[0] = 3
        @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
        stack_tcode[1] = 3
        @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
        stack_tcode[2] = 3
        @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, any_dim), dtype=int32)
        stack_tcode[3] = 0
        @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, any_dim_1), dtype=int32)
        stack_tcode[4] = 0
        @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, stride_1), dtype=int32)
        stack_tcode[5] = 0
        @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride), dtype=int32)
        stack_tcode[6] = 0
        @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
        stack_tcode[7] = 0
        @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
        stack_tcode[8] = 0
        @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, floordiv(((max(any_dim, any_dim_1)*4) + 511), 512)), dtype=int32)
        stack_tcode[9] = 0
        @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, 512), dtype=int32)
        stack_tcode[10] = 0
        @tir.tvm_call_packed_lowered("fused_add_1_kernel0", stack_value, stack_tcode, 0, 11, dtype=int32)
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 9, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 9, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode_1[0] = 0
      @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
      stack_tcode_1[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
      @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
      stack_tcode_1[0] = 3
      @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
      stack_tcode_1[1] = 3
      @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, stride_5), dtype=int32)
      stack_tcode_1[2] = 0
      @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, stride_4), dtype=int32)
      stack_tcode_1[3] = 0
      @tir.tvm_struct_set(stack_value_1, 4, 12, cast(int64, stride_7), dtype=int32)
      stack_tcode_1[4] = 0
      @tir.tvm_struct_set(stack_value_1, 5, 12, cast(int64, stride_6), dtype=int32)
      stack_tcode_1[5] = 0
      @tir.tvm_struct_set(stack_value_1, 6, 12, cast(int64, any_dim_2), dtype=int32)
      stack_tcode_1[6] = 0
      @tir.tvm_struct_set(stack_value_1, 7, 12, cast(int64, 32), dtype=int32)
      stack_tcode_1[7] = 0
      @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 8, dtype=int32)
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add_1", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 12, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 12, dtype=handle)
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
       {
        @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
        stack_tcode[0] = 0
        @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
        stack_tcode[1] = 0
        @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
      }
      attr [0] "compute_scope" = "fused_add_1_compute_" {
        @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
        stack_tcode[0] = 3
        @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
        stack_tcode[1] = 3
        @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
        stack_tcode[2] = 3
        @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, any_dim), dtype=int32)
        stack_tcode[3] = 0
        @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, any_dim_1), dtype=int32)
        stack_tcode[4] = 0
        @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, stride_1), dtype=int32)
        stack_tcode[5] = 0
        @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride), dtype=int32)
        stack_tcode[6] = 0
        @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
        stack_tcode[7] = 0
        @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
        stack_tcode[8] = 0
        @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, @tir.shift_right(((max(any_dim, any_dim_1)*4) + 511), 9, dtype=int32)), dtype=int32)
        stack_tcode[9] = 0
        @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, 512), dtype=int32)
        stack_tcode[10] = 0
        @tir.tvm_call_packed_lowered("fused_add_1_kernel0", stack_value, stack_tcode, 0, 11, dtype=int32)
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 9, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 9, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode_1[0] = 0
      @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
      stack_tcode_1[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
      @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
      stack_tcode_1[0] = 3
      @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
      stack_tcode_1[1] = 3
      @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, stride_5), dtype=int32)
      stack_tcode_1[2] = 0
      @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, stride_4), dtype=int32)
      stack_tcode_1[3] = 0
      @tir.tvm_struct_set(stack_value_1, 4, 12, cast(int64, stride_7), dtype=int32)
      stack_tcode_1[4] = 0
      @tir.tvm_struct_set(stack_value_1, 5, 12, cast(int64, stride_6), dtype=int32)
      stack_tcode_1[5] = 0
      @tir.tvm_struct_set(stack_value_1, 6, 12, cast(int64, any_dim_2), dtype=int32)
      stack_tcode_1[6] = 0
      @tir.tvm_struct_set(stack_value_1, 7, 12, cast(int64, 32), dtype=int32)
      stack_tcode_1[7] = 0
      @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 8, dtype=int32)
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add_1", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 12, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 12, dtype=handle)
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
       {
        @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
        stack_tcode[0] = 0
        @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
        stack_tcode[1] = 0
        @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
      }
      attr [0] "compute_scope" = "fused_add_1_compute_" {
        @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
        stack_tcode[0] = 3
        @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
        stack_tcode[1] = 3
        @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
        stack_tcode[2] = 3
        @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, any_dim), dtype=int32)
        stack_tcode[3] = 0
        @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, any_dim_1), dtype=int32)
        stack_tcode[4] = 0
        @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, stride_1), dtype=int32)
        stack_tcode[5] = 0
        @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride), dtype=int32)
        stack_tcode[6] = 0
        @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
        stack_tcode[7] = 0
        @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
        stack_tcode[8] = 0
        @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, @tir.shift_right(((max(any_dim, any_dim_1)*4) + 511), 9, dtype=int32)), dtype=int32)
        stack_tcode[9] = 0
        @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, 512), dtype=int32)
        stack_tcode[10] = 0
        @tir.tvm_call_packed_lowered("fused_add_1_kernel0", stack_value, stack_tcode, 0, 11, dtype=int32)
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 9, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 9, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode_1[0] = 0
      @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
      stack_tcode_1[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
      @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
      stack_tcode_1[0] = 3
      @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
      stack_tcode_1[1] = 3
      @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, stride_5), dtype=int32)
      stack_tcode_1[2] = 0
      @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, stride_4), dtype=int32)
      stack_tcode_1[3] = 0
      @tir.tvm_struct_set(stack_value_1, 4, 12, cast(int64, stride_7), dtype=int32)
      stack_tcode_1[4] = 0
      @tir.tvm_struct_set(stack_value_1, 5, 12, cast(int64, stride_6), dtype=int32)
      stack_tcode_1[5] = 0
      @tir.tvm_struct_set(stack_value_1, 6, 12, cast(int64, any_dim_2), dtype=int32)
      stack_tcode_1[6] = 0
      @tir.tvm_struct_set(stack_value_1, 7, 12, cast(int64, 32), dtype=int32)
      stack_tcode_1[7] = 0
      @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 8, dtype=int32)
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add_1", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 12, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 12, dtype=handle)
  assert((num_args == 3), "fused_add_1: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let any_dim: int32 = cast(int32, (int64*)arg0.shape[0])
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let stride: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 1, cast(int32, (int64*)arg0.strides[1]), dtype=int32)
  let stride_1: int32 = @tir.if_then_else((any_dim == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides, dtype=bool), 4, cast(int32, (int64*)arg0.strides[0]), dtype=int32), dtype=int32)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let any_dim_1: int32 = cast(int32, (int64*)arg1.shape[0])
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let stride_2: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 1, cast(int32, (int64*)arg1.strides[1]), dtype=int32)
  let stride_3: int32 = @tir.if_then_else((any_dim_1 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides, dtype=bool), 4, cast(int32, (int64*)arg1.strides[0]), dtype=int32), dtype=int32)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add_1: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
  assert((max(any_dim, any_dim_1) == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (max(any_dim, any_dim) == int32(arg2.shape[0]))")
  assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
   {
    if !@tir.isnullptr(arg2.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
     {
       {
        @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
        stack_tcode[0] = 0
        @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
        stack_tcode[1] = 0
        @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
      }
      attr [0] "compute_scope" = "fused_add_1_compute_" {
        @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
        stack_tcode[0] = 3
        @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
        stack_tcode[1] = 3
        @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
        stack_tcode[2] = 3
        @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, any_dim), dtype=int32)
        stack_tcode[3] = 0
        @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, any_dim_1), dtype=int32)
        stack_tcode[4] = 0
        @tir.tvm_struct_set(stack_value, 5, 12, cast(int64, stride_1), dtype=int32)
        stack_tcode[5] = 0
        @tir.tvm_struct_set(stack_value, 6, 12, cast(int64, stride), dtype=int32)
        stack_tcode[6] = 0
        @tir.tvm_struct_set(stack_value, 7, 12, cast(int64, stride_3), dtype=int32)
        stack_tcode[7] = 0
        @tir.tvm_struct_set(stack_value, 8, 12, cast(int64, stride_2), dtype=int32)
        stack_tcode[8] = 0
        @tir.tvm_struct_set(stack_value, 9, 12, cast(int64, @tir.shift_right(((max(any_dim, any_dim_1)*4) + 511), 9, dtype=int32)), dtype=int32)
        stack_tcode[9] = 0
        @tir.tvm_struct_set(stack_value, 10, 12, cast(int64, 512), dtype=int32)
        stack_tcode[10] = 0
        @tir.tvm_call_packed_lowered("fused_add_1_kernel0", stack_value, stack_tcode, 0, 11, dtype=int32)
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 9, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 9, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let any_dim_2: int32 = cast(int32, (int64*)arg0.shape_1[0])
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let stride_4: int32 = @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 1, cast(int32, (int64*)arg0.strides_1[1]), dtype=int32)
  let stride_5: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg0.strides_1, dtype=bool), 4, cast(int32, (int64*)arg0.strides_1[0]), dtype=int32), dtype=int32)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  let stride_6: int32 = @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 1, cast(int32, (int64*)arg1.strides_1[1]), dtype=int32)
  let stride_7: int32 = @tir.if_then_else((any_dim_2 == 1), 0, @tir.if_then_else(@tir.isnullptr(arg1.strides_1, dtype=bool), 4, cast(int32, (int64*)arg1.strides_1[0]), dtype=int32), dtype=int32)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
  assert((any_dim_2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (any_dim == int32(arg1.shape[0]))")
  assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
  assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
   {
     {
      @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
      stack_tcode_1[0] = 0
      @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
      stack_tcode_1[1] = 0
      @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
    }
    attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
      @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
      stack_tcode_1[0] = 3
      @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
      stack_tcode_1[1] = 3
      @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, stride_5), dtype=int32)
      stack_tcode_1[2] = 0
      @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, stride_4), dtype=int32)
      stack_tcode_1[3] = 0
      @tir.tvm_struct_set(stack_value_1, 4, 12, cast(int64, stride_7), dtype=int32)
      stack_tcode_1[4] = 0
      @tir.tvm_struct_set(stack_value_1, 5, 12, cast(int64, stride_6), dtype=int32)
      stack_tcode_1[5] = 0
      @tir.tvm_struct_set(stack_value_1, 6, 12, cast(int64, any_dim_2), dtype=int32)
      stack_tcode_1[6] = 0
      @tir.tvm_struct_set(stack_value_1, 7, 12, cast(int64, 32), dtype=int32)
      stack_tcode_1[7] = 0
      @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 8, dtype=int32)
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32), stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim: int32;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32), any_dim_1: int32, any_dim_2: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_1_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x_1 < floordiv(max(any_dim_1, any_dim_2), 128)) && (blockIdx.x_1 < floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512))) {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
    }
  } else {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (max(any_dim_1, any_dim_2)*4)) {
        T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32), stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim: int32;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32), any_dim_1: int32, any_dim_2: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_1_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x_1 < floordiv(max(any_dim_1, any_dim_2), 128)) && (blockIdx.x_1 < floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512))) {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
    }
  } else {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (max(any_dim_1, any_dim_2)*4)) {
        T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerWarpMemory
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32), stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim: int32;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.exp(((float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] = (@tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32), any_dim_1: int32, any_dim_2: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_1_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x_1 < floordiv(max(any_dim_1, any_dim_2), 128)) && (blockIdx.x_1 < floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512))) {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
    }
  } else {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (max(any_dim_1, any_dim_2)*4)) {
        T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32), stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim: int32;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.exp(((float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] = (@tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32), any_dim_1: int32, any_dim_2: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_1_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x_1 < floordiv(max(any_dim_1, any_dim_2), 128)) && (blockIdx.x_1 < floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512))) {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
    }
  } else {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (max(any_dim_1, any_dim_2)*4)) {
        T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32), stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim: int32;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.exp(((float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] = (@tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32), any_dim_1: int32, any_dim_2: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_1_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x_1 < floordiv(max(any_dim_1, any_dim_2), 128)) && (blockIdx.x_1 < floordiv(((max(any_dim_1, any_dim_2)*4) + 511), 512))) {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
    }
  } else {
    if ((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_1) || (((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4)) < any_dim_2)) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (max(any_dim_1, any_dim_2)*4)) {
        T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_4) + (floormod(threadIdx.x_1, 4)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + floordiv(threadIdx.x_1, 4))*stride_6) + (floormod(threadIdx.x_1, 4)*stride_7))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32), stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim: int32;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.cuda.__activemask(, dtype=uint32)
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.cuda.__shfl_sync((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.call_pure_extern("__expf", ((float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.cuda.__shfl_sync(@tir.cuda.__activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.cuda.__activemask(, dtype=uint32)
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.cuda.__shfl_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] = (@tir.cuda.__shfl_sync(@tir.cuda.__activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32), any_dim_1: int32, any_dim_2: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_1_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right(((max(any_dim_1, any_dim_2)*4) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x_1 < @tir.shift_right(max(any_dim_1, any_dim_2), 7, dtype=int32)) && (blockIdx.x_1 < @tir.shift_right(((max(any_dim_1, any_dim_2)*4) + 511), 9, dtype=int32))) {
    if ((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32)) < any_dim_1) || (((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32)) < any_dim_2)) {
      T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32))*stride_4) + (@tir.bitwise_and(threadIdx.x_1, 3, dtype=int32)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32))*stride_6) + (@tir.bitwise_and(threadIdx.x_1, 3, dtype=int32)*stride_7))])
    }
  } else {
    if ((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32)) < any_dim_1) || (((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32)) < any_dim_2)) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (max(any_dim_1, any_dim_2)*4)) {
        T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32))*stride_4) + (@tir.bitwise_and(threadIdx.x_1, 3, dtype=int32)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32))*stride_6) + (@tir.bitwise_and(threadIdx.x_1, 3, dtype=int32)*stride_7))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32), stride: int32, stride_1: int32, stride_2: int32, stride_3: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = any_dim: int32;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.cuda.__activemask(, dtype=uint32)
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.cuda.__shfl_sync((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.call_pure_extern("__expf", ((float32*)placeholder[((blockIdx.x*stride) + (threadIdx.x*stride_1))] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.cuda.__shfl_sync(@tir.cuda.__activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.cuda.__activemask(, dtype=uint32)
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.cuda.__shfl_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*stride_2) + (threadIdx.x*stride_3))] = (@tir.cuda.__shfl_sync(@tir.cuda.__activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32), any_dim_1: int32, any_dim_2: int32, stride_4: int32, stride_5: int32, stride_6: int32, stride_7: int32) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_1_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = @tir.shift_right(((max(any_dim_1, any_dim_2)*4) + 511), 9, dtype=int32);
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 512;
  if ((blockIdx.x_1 < @tir.shift_right(max(any_dim_1, any_dim_2), 7, dtype=int32)) && (blockIdx.x_1 < @tir.shift_right(((max(any_dim_1, any_dim_2)*4) + 511), 9, dtype=int32))) {
    if ((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32)) < any_dim_1) || (((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32)) < any_dim_2)) {
      T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32))*stride_4) + (@tir.bitwise_and(threadIdx.x_1, 3, dtype=int32)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32))*stride_6) + (@tir.bitwise_and(threadIdx.x_1, 3, dtype=int32)*stride_7))])
    }
  } else {
    if ((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32)) < any_dim_1) || (((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32)) < any_dim_2)) {
      if (((blockIdx.x_1*512) + threadIdx.x_1) < (max(any_dim_1, any_dim_2)*4)) {
        T_add[((blockIdx.x_1*512) + threadIdx.x_1)] = ((float32*)placeholder_1[((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32))*stride_4) + (@tir.bitwise_and(threadIdx.x_1, 3, dtype=int32)*stride_5))] + (float32*)placeholder_2[((((blockIdx.x_1*128) + @tir.shift_right(threadIdx.x_1, 2, dtype=int32))*stride_6) + (@tir.bitwise_and(threadIdx.x_1, 3, dtype=int32)*stride_7))])
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}

primfn(placeholder_7: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder_6: Buffer(placeholder_8: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_7: placeholder_6, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_8[k0])
  }
}

primfn(placeholder_10: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder_9: Buffer(placeholder_11: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_10: placeholder_9, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_11[0]*4i64)
}

primfn(placeholder_13: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder_12: Buffer(placeholder_14: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_13: placeholder_12, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_14[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VerifyMemory
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}

primfn(placeholder_7: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder_6: Buffer(placeholder_8: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_7: placeholder_6, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_8[k0])
  }
}

primfn(placeholder_10: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder_9: Buffer(placeholder_11: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_10: placeholder_9, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_11[0]*4i64)
}

primfn(placeholder_13: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder_12: Buffer(placeholder_14: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_13: placeholder_12, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_14[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}

primfn(placeholder_7: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder_6: Buffer(placeholder_8: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_7: placeholder_6, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_8[k0])
  }
}

primfn(placeholder_10: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder_9: Buffer(placeholder_11: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_10: placeholder_9, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_11[0]*4i64)
}

primfn(placeholder_13: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder_12: Buffer(placeholder_14: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_13: placeholder_12, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_14[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}

primfn(placeholder_7: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder_6: Buffer(placeholder_8: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_7: placeholder_6, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_8[k0])
  }
}

primfn(placeholder_10: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder_9: Buffer(placeholder_11: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_10: placeholder_9, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_11[0]*4i64)
}

primfn(placeholder_13: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder_12: Buffer(placeholder_14: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_13: placeholder_12, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_14[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InferFragment
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}

primfn(placeholder_7: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder_6: Buffer(placeholder_8: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_7: placeholder_6, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_8[k0])
  }
}

primfn(placeholder_10: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder_9: Buffer(placeholder_11: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_10: placeholder_9, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_11[0]*4i64)
}

primfn(placeholder_13: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder_12: Buffer(placeholder_14: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_13: placeholder_12, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_14[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerThreadAllreduce
primfn(placeholder_2: handle, placeholder_3: handle, v_broadcast_shape_func: handle) -> ()
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(int64), int64, [2], []),
             buf__broadcast_shape_func: Buffer(v_broadcast_shape_func_1: Pointer(int64), int64, [2], []),
             placeholder_1: Buffer(placeholder_5: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, v_broadcast_shape_func: buf__broadcast_shape_func} {
  attr [0] "extern_scope" = 0 {
    if ((int64*)placeholder_4[1] == (int64*)placeholder_5[1]) {
      v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
    } else {
      if ((int64*)placeholder_4[1] == 1i64) {
        v_broadcast_shape_func_1[1] = (int64*)placeholder_5[1]
      } else {
        assert(((int64*)placeholder_5[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
        0
        v_broadcast_shape_func_1[1] = (int64*)placeholder_4[1]
      }
    }
    if ((int64*)placeholder_4[0] == (int64*)placeholder_5[0]) {
      v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
    } else {
      if ((int64*)placeholder_4[0] == 1i64) {
        v_broadcast_shape_func_1[0] = (int64*)placeholder_5[0]
      } else {
        assert(((int64*)placeholder_5[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
        0
        v_broadcast_shape_func_1[0] = (int64*)placeholder_4[0]
      }
    }
  }
}

primfn(placeholder_7: handle, placeholder_red_1: handle) -> ()
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {placeholder_red: Buffer(placeholder_red_2: Pointer(int64), int64, [], []),
             placeholder_6: Buffer(placeholder_8: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_7: placeholder_6, placeholder_red_1: placeholder_red} {
  placeholder_red_2[0] = 1i64
  for (k0: int32, 0, 2) {
    placeholder_red_2[0] = ((int64*)placeholder_red_2[0]*(int64*)placeholder_8[k0])
  }
}

primfn(placeholder_10: handle, T_multiply_1: handle) -> ()
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_multiply: Buffer(T_multiply_2: Pointer(int64), int64, [], []),
             placeholder_9: Buffer(placeholder_11: Pointer(int64), int64, [], [])}
  buffer_map = {placeholder_10: placeholder_9, T_multiply_1: T_multiply} {
  T_multiply_2[0] = ((int64*)placeholder_11[0]*4i64)
}

primfn(placeholder_13: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {compute: Buffer(compute_2: Pointer(int64), int64, [2], []),
             placeholder_12: Buffer(placeholder_14: Pointer(int64), int64, [2], [])}
  buffer_map = {placeholder_13: placeholder_12, compute_1: compute} {
  for (i0: int32, 0, 2) {
    compute_2[i0] = (int64*)placeholder_14[i0]
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"global_symbol": "fused_prod", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"global_symbol": "shape_func_nn_softmax_1", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"global_symbol": "fused_multiply", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_2;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"global_symbol": "shape_func_add_1", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id_3;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_2;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "shape_func_add_1", "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id_3;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_2;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "shape_func_add_1", "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id_3;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_2;
  attr ["default"] "device_type" = 1;
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_add_1", "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id_3;
  attr ["default"] "device_type" = 1;
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_add_1", "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_add_1", "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_add_1", "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_add_1", "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_prod", "calling_conv": 1} {
  assert((num_args == 2), "fused_prod: num_args should be 2")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let placeholder: Pointer(int64) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_red: Pointer(int64) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_red] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_prod: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_prod: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((0 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
    assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
    attr [0] "compute_scope" = "fused_prod_compute_" {
      placeholder_red[0] = 1i64
      for (k0: int32, 0, 2) {
        placeholder_red[0] = ((int64*)placeholder_red[0]*(int64*)placeholder[k0])
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_nn_softmax_1", "calling_conv": 1} {
  assert((num_args_1 == 2), "shape_func_nn_softmax_1: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_1: Pointer(int64) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let compute: Pointer(int64) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [compute] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "shape_func_nn_softmax_1: Expect arg[1] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_1[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_1[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      attr [0] "compute_scope" = "shape_func_nn_softmax_1_compute_";
      for (i0: int32, 0, 2) {
        compute[i0] = (int64*)placeholder_1[i0]
      }
    }
  }
}

primfn(args_2: handle, arg_type_ids_2: handle, num_args_2: int32, out_ret_value_2: handle, out_ret_tcode_2: handle, resource_handle_2: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_multiply", "calling_conv": 1} {
  assert((num_args_2 == 2), "fused_multiply: num_args should be 2")
  let arg0_2: handle = @tir.tvm_struct_get(args_2, 0, 12, dtype=handle)
  let arg0.code_2: int32 = (int32*)arg_type_ids_2[0]
  let arg1_2: handle = @tir.tvm_struct_get(args_2, 1, 12, dtype=handle)
  let arg1.code_2: int32 = (int32*)arg_type_ids_2[1]
  let placeholder_2: Pointer(int64) = @tir.tvm_struct_get(arg0_2, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_2: handle = @tir.tvm_struct_get(arg0_2, 0, 2, dtype=handle)
  let arg0.strides_2: handle = @tir.tvm_struct_get(arg0_2, 0, 3, dtype=handle)
  let dev_id_2: int32 = @tir.tvm_struct_get(arg0_2, 0, 9, dtype=int32)
  let T_multiply: Pointer(int64) = @tir.tvm_struct_get(arg1_2, 0, 1, dtype=handle)
  attr [T_multiply] "storage_alignment" = 128;
  let arg1.shape_2: handle = @tir.tvm_struct_get(arg1_2, 0, 2, dtype=handle)
  let arg1.strides_2: handle = @tir.tvm_struct_get(arg1_2, 0, 3, dtype=handle)
  assert(((((arg0.code_2 == 3) || (arg0.code_2 == 13)) || (arg0.code_2 == 7)) || (arg0.code_2 == 4)), "fused_multiply: Expect arg[0] to be pointer")
  assert(((((arg1.code_2 == 3) || (arg1.code_2 == 13)) || (arg1.code_2 == 7)) || (arg1.code_2 == 4)), "fused_multiply: Expect arg[1] to be pointer")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg0_2, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg0_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_2, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg0_2, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg0_2, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((0 == @tir.tvm_struct_get(arg1_2, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 0")
  assert((((@tir.tvm_struct_get(arg1_2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_2, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
  assert((0u64 == @tir.tvm_struct_get(arg1_2, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
  assert((1 == @tir.tvm_struct_get(arg1_2, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
  assert((dev_id_2 == @tir.tvm_struct_get(arg1_2, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
  attr [0] "compute_scope" = "fused_multiply_compute_";
  T_multiply[0] = ((int64*)placeholder_2[0]*4i64)
}

primfn(args_3: handle, arg_type_ids_3: handle, num_args_3: int32, out_ret_value_3: handle, out_ret_tcode_3: handle, resource_handle_3: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "shape_func_add_1", "calling_conv": 1} {
  assert((num_args_3 == 3), "shape_func_add_1: num_args should be 3")
  let arg0_3: handle = @tir.tvm_struct_get(args_3, 0, 12, dtype=handle)
  let arg0.code_3: int32 = (int32*)arg_type_ids_3[0]
  let arg1_3: handle = @tir.tvm_struct_get(args_3, 1, 12, dtype=handle)
  let arg1.code_3: int32 = (int32*)arg_type_ids_3[1]
  let arg2: handle = @tir.tvm_struct_get(args_3, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids_3[2]
  let placeholder_3: Pointer(int64) = @tir.tvm_struct_get(arg0_3, 0, 1, dtype=handle)
  attr [placeholder_3] "storage_alignment" = 128;
  let arg0.shape_3: handle = @tir.tvm_struct_get(arg0_3, 0, 2, dtype=handle)
  let arg0.strides_3: handle = @tir.tvm_struct_get(arg0_3, 0, 3, dtype=handle)
  let dev_id_3: int32 = @tir.tvm_struct_get(arg0_3, 0, 9, dtype=int32)
  let placeholder_4: Pointer(int64) = @tir.tvm_struct_get(arg1_3, 0, 1, dtype=handle)
  attr [placeholder_4] "storage_alignment" = 128;
  let arg1.shape_3: handle = @tir.tvm_struct_get(arg1_3, 0, 2, dtype=handle)
  let arg1.strides_3: handle = @tir.tvm_struct_get(arg1_3, 0, 3, dtype=handle)
  let v_broadcast_shape_func: Pointer(int64) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [v_broadcast_shape_func] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code_3 == 3) || (arg0.code_3 == 13)) || (arg0.code_3 == 7)) || (arg0.code_3 == 4)), "shape_func_add_1: Expect arg[0] to be pointer")
  assert(((((arg1.code_3 == 3) || (arg1.code_3 == 13)) || (arg1.code_3 == 7)) || (arg1.code_3 == 4)), "shape_func_add_1: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "shape_func_add_1: Expect arg[2] to be pointer")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((1 == @tir.tvm_struct_get(arg0_3, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 1")
  assert((((@tir.tvm_struct_get(arg0_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg0_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg0_3, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be int64")
  assert((2 == cast(int32, (int64*)arg0.shape_3[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (2 == int32(arg0.shape[0]))")
   {
    if !@tir.isnullptr(arg0.strides_3, dtype=bool) {
      assert((1 == cast(int32, (int64*)arg0.strides_3[0])), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_3, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((1 == @tir.tvm_struct_get(arg0_3, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((1 == @tir.tvm_struct_get(arg1_3, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 1")
    assert((((@tir.tvm_struct_get(arg1_3, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg1_3, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg1_3, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be int64")
    assert((2 == cast(int32, (int64*)arg1.shape_3[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (2 == int32(arg1.shape[0]))")
     {
      if !@tir.isnullptr(arg1.strides_3, dtype=bool) {
        assert((1 == cast(int32, (int64*)arg1.strides_3[0])), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_3, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((1 == @tir.tvm_struct_get(arg1_3, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_3 == @tir.tvm_struct_get(arg1_3, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((1 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 1")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 0u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 64u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be int64")
      assert((2 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (2 == int32(arg2.shape[0]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert((1 == cast(int32, (int64*)arg2.strides[0])), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((1 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id_3 == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
        attr [0] "compute_scope" = "shape_func_add_1_compute_";
        attr [0] "extern_scope" = 0 {
          if ((int64*)placeholder_3[1] == (int64*)placeholder_4[1]) {
            v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
          } else {
            if ((int64*)placeholder_3[1] == 1i64) {
              v_broadcast_shape_func[1] = (int64*)placeholder_4[1]
            } else {
              assert(((int64*)placeholder_4[1] == 1i64), "Incompatible broadcast type placeholder[1] and placeholder[1]")
              0
              v_broadcast_shape_func[1] = (int64*)placeholder_3[1]
            }
          }
          if ((int64*)placeholder_3[0] == (int64*)placeholder_4[0]) {
            v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
          } else {
            if ((int64*)placeholder_3[0] == 1i64) {
              v_broadcast_shape_func[0] = (int64*)placeholder_4[0]
            } else {
              assert(((int64*)placeholder_4[0] == 1i64), "Incompatible broadcast type placeholder[0] and placeholder[0]")
              0
              v_broadcast_shape_func[0] = (int64*)placeholder_3[0]
            }
          }
        }
      }
    }
  }
}


[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerWarpMemory

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin

[14:32:26] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo

[14:32:26] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1203: CODEGEN END

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32]) {
  %0 = add(%a, %b);
  %1 = nn.softmax(%0);
  add(%1, %b)
}

def @main(%a: Tensor[(?, 4), float32], %b: Tensor[(?, 4), float32], hash="832104fd038093f0") -> Tensor[(?, 4), float32] {
  %0 = fn (%p02: Tensor[(?, 4), float32], %p11: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(?, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(?, 4), float32] */;
  %2 = fn (%p01: Tensor[(?, 4), float32], Primitive=1, hash="ce52580097fd9d78") -> Tensor[(?, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(?, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(?, 4), float32] */;
  %4 = fn (%p0: Tensor[(?, 4), float32], %p1: Tensor[(?, 4), float32], Primitive=1, hash="c61d1b9bc4c25fda") -> Tensor[(?, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(?, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(?, 4), float32] */
}

VM Function[0]: main(a, b)
# reg file size = 52
# instruction count = 63
opcode, fields # inst(text):
 0: 17 0 2   # shape_of $2 $0
 1: 17 1 3   # shape_of $3 $1
 2: 11 0 4   # load_const $4 Const[0]
 3: 16 4 64 0 64 1 1 5   # alloc_storage $5 $4 64 int64 1
 4: 11 1 6   # load_const $6 Const[1]
 5: 5 5 6 0 64 1 1 7 2   # alloc_tensor $7 $5 $6 [2] int64
 6: 4 0 3 1 2 3 7   # invoke_packed PackedFunc[0] (in: $2, $3, out: $7)
 7: 11 2 8   # load_const $8 Const[2]
 8: 16 8 64 0 64 1 1 9   # alloc_storage $9 $8 64 int64 1
 9: 11 3 10   # load_const $10 Const[3]
10: 5 9 10 0 64 1 0 11   # alloc_tensor $11 $9 $10 [] int64
11: 4 1 2 1 7 11   # invoke_packed PackedFunc[1] (in: $7, out: $11)
12: 11 4 12   # load_const $12 Const[4]
13: 16 12 64 0 64 1 1 13   # alloc_storage $13 $12 64 int64 1
14: 11 5 14   # load_const $14 Const[5]
15: 5 13 14 0 64 1 0 15   # alloc_tensor $15 $13 $14 [] int64
16: 4 2 2 1 11 15   # invoke_packed PackedFunc[2] (in: $11, out: $15)
17: 16 15 64 2 32 1 2 16   # alloc_storage $16 $15 64 float32 2
18: 11 6 17   # load_const $17 Const[6]
19: 6 16 17 7 2 32 1 18   # alloc_tensor_reg $18 $16 $17 $7 float32
20: 4 3 3 1 0 1 18   # invoke_packed PackedFunc[3] (in: $0, $1, out: $18)
21: 17 18 19   # shape_of $19 $18
22: 11 7 20   # load_const $20 Const[7]
23: 16 20 64 0 64 1 1 21   # alloc_storage $21 $20 64 int64 1
24: 11 8 22   # load_const $22 Const[8]
25: 5 21 22 0 64 1 1 23 2   # alloc_tensor $23 $21 $22 [2] int64
26: 4 4 2 1 19 23   # invoke_packed PackedFunc[4] (in: $19, out: $23)
27: 11 9 24   # load_const $24 Const[9]
28: 16 24 64 0 64 1 1 25   # alloc_storage $25 $24 64 int64 1
29: 11 10 26   # load_const $26 Const[10]
30: 5 25 26 0 64 1 0 27   # alloc_tensor $27 $25 $26 [] int64
31: 4 1 2 1 23 27   # invoke_packed PackedFunc[1] (in: $23, out: $27)
32: 11 11 28   # load_const $28 Const[11]
33: 16 28 64 0 64 1 1 29   # alloc_storage $29 $28 64 int64 1
34: 11 12 30   # load_const $30 Const[12]
35: 5 29 30 0 64 1 0 31   # alloc_tensor $31 $29 $30 [] int64
36: 4 2 2 1 27 31   # invoke_packed PackedFunc[2] (in: $27, out: $31)
37: 16 31 64 2 32 1 2 32   # alloc_storage $32 $31 64 float32 2
38: 11 13 33   # load_const $33 Const[13]
39: 6 32 33 23 2 32 1 34   # alloc_tensor_reg $34 $32 $33 $23 float32
40: 4 5 2 1 18 34   # invoke_packed PackedFunc[5] (in: $18, out: $34)
41: 17 34 35   # shape_of $35 $34
42: 17 1 36   # shape_of $36 $1
43: 11 14 37   # load_const $37 Const[14]
44: 16 37 64 0 64 1 1 38   # alloc_storage $38 $37 64 int64 1
45: 11 15 39   # load_const $39 Const[15]
46: 5 38 39 0 64 1 1 40 2   # alloc_tensor $40 $38 $39 [2] int64
47: 4 0 3 1 35 36 40   # invoke_packed PackedFunc[0] (in: $35, $36, out: $40)
48: 11 16 41   # load_const $41 Const[16]
49: 16 41 64 0 64 1 1 42   # alloc_storage $42 $41 64 int64 1
50: 11 17 43   # load_const $43 Const[17]
51: 5 42 43 0 64 1 0 44   # alloc_tensor $44 $42 $43 [] int64
52: 4 1 2 1 40 44   # invoke_packed PackedFunc[1] (in: $40, out: $44)
53: 11 18 45   # load_const $45 Const[18]
54: 16 45 64 0 64 1 1 46   # alloc_storage $46 $45 64 int64 1
55: 11 19 47   # load_const $47 Const[19]
56: 5 46 47 0 64 1 0 48   # alloc_tensor $48 $46 $47 [] int64
57: 4 2 2 1 44 48   # invoke_packed PackedFunc[2] (in: $44, out: $48)
58: 16 48 64 2 32 1 2 49   # alloc_storage $49 $48 64 float32 2
59: 11 20 50   # load_const $50 Const[20]
60: 6 49 50 40 2 32 1 51   # alloc_tensor_reg $51 $49 $50 $40 float32
61: 4 3 3 1 34 1 51   # invoke_packed PackedFunc[3] (in: $34, $1, out: $51)
62: 1 51   # ret $51


Relay VM executable statistics:
  Constant shapes (# 21): [scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar, scalar]
  Globals (#1): [("main", 0)]
  Primitive ops (#6): [shape_func_add_1, fused_prod, fused_multiply, fused_add_1, shape_func_nn_softmax_1, fused_nn_softmax]

[[-0.34270352  0.15980093  2.5868502  -0.7814463 ]
 [-0.07369117 -0.65830517 -0.23009798 -0.6175957 ]
 [-0.8238474   0.85554487  0.30383176 -1.4581347 ]]
