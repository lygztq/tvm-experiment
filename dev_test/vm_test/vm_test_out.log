[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass RemoveUnusedFunctions
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) {
  %0 = add(%a, %b);
  %1 = nn.softmax(%0);
  add(%1, %b)
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToBasicBlockNormalForm
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) {
  %0 = add(%a, %b);
  %1 = nn.softmax(%0);
  add(%1, %b)
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass sequential
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass SimplifyInference
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass DynamicToStatic
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass SimplifyExpr
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldScaleAxis
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] {
  %0 = add(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = nn.softmax(%0) /* ty=Tensor[(3, 4), float32] */;
  add(%1, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:914: LOWER START

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass RemoveUnusedFunctions
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToBasicBlockNormalForm
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b);
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1);
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b)
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass sequential
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Legalize
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass EtaExpand
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11)
  };
  %1 = %0(%a, %b);
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01)
  };
  %3 = %2(%1);
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1)
  };
  %4(%3, %b)
}

[14:33:43] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass SimplifyInference
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass SimplifyExpr
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Inline
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b);
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1);
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b)
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass DeadCodeElimination
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InlinePrimitives
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldScaleAxis
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ToANormalForm
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %x = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  let %x1 = %x(%a, %b);
  let %x2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  let %x3 = %x2(%x1);
  let %x4 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  let %x5 = %x4(%x3, %b);
  %x5
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %x: fn (Tensor[(3, 4), float32], Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  let %x1: Tensor[(3, 4), float32] = %x(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  let %x2: fn (Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  let %x3: Tensor[(3, 4), float32] = %x2(%x1) /* ty=Tensor[(3, 4), float32] */;
  let %x4: fn (Tensor[(3, 4), float32], Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  let %x5: Tensor[(3, 4), float32] = %x4(%x3, %b) /* ty=Tensor[(3, 4), float32] */;
  %x5
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass LambdaLift
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %x: fn (Tensor[(3, 4), float32], Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  let %x1: Tensor[(3, 4), float32] = %x(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  let %x2: fn (Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  let %x3: Tensor[(3, 4), float32] = %x2(%x1) /* ty=Tensor[(3, 4), float32] */;
  let %x4: fn (Tensor[(3, 4), float32], Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  let %x5: Tensor[(3, 4), float32] = %x4(%x3, %b) /* ty=Tensor[(3, 4), float32] */;
  %x5
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Inline
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  let %x: fn (Tensor[(3, 4), float32], Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = %0;
  let %x1: Tensor[(3, 4), float32] = %0(%a, %b);
  %1 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  let %x2: fn (Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = %1;
  let %x3: Tensor[(3, 4), float32] = %1(%x1);
  %2 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  let %x4: fn (Tensor[(3, 4), float32], Tensor[(3, 4), float32]) -> Tensor[(3, 4), float32] = %2;
  let %x5: Tensor[(3, 4), float32] = %2(%x3, %b);
  %x5
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass DeadCodeElimination
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  let %x: Tensor[(3, 4), float32] = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  let %x1: Tensor[(3, 4), float32] = %1(%x) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  let %x2: Tensor[(3, 4), float32] = %2(%x1, %b) /* ty=Tensor[(3, 4), float32] */;
  %x2
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InlinePrimitives
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  let %x: Tensor[(3, 4), float32] = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  let %x1: Tensor[(3, 4), float32] = %1(%x) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  let %x2: Tensor[(3, 4), float32] = %2(%x1, %b) /* ty=Tensor[(3, 4), float32] */;
  %x2
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InlineGlobals
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  let %x: Tensor[(3, 4), float32] = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  let %x1: Tensor[(3, 4), float32] = %1(%x) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  let %x2: Tensor[(3, 4), float32] = %2(%x1, %b) /* ty=Tensor[(3, 4), float32] */;
  %x2
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass RemoveUnusedFunctions
def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  let %x: Tensor[(3, 4), float32] = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %1 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  let %x1: Tensor[(3, 4), float32] = %1(%x) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  let %x2: Tensor[(3, 4), float32] = %2(%x1, %b) /* ty=Tensor[(3, 4), float32] */;
  %x2
}

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ManifestAlloc
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ManifestAlloc
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FuseOps
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass ManifestAlloc
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass FoldConstant
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass sequential
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass InferType
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass LabelOps
type Storage {
  
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="6e558a9ad9656503") -> Tensor[(3, 4), float32] {
  let %storage_0: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][0]) /* ty=Storage[] */;
  let %tensor_0: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_0, 0 /* ty=int64 */, meta[relay.Constant][0] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][0]) /* ty=Tensor[(3, 4), float32] */;
  %0 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="013033dbf7e17e61") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = (%a, %b);
  %2 = (%tensor_0,);
  let %x: () = vm.invoke_tvm_op(%0, %1, %2) /* ty=() */;
  let %x1: Tensor[(3, 4), float32] = %tensor_0;
  let %storage_01: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][1]) /* ty=Storage[] */;
  let %tensor_01: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_01, 0 /* ty=int64 */, meta[relay.Constant][1] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][1]) /* ty=Tensor[(3, 4), float32] */;
  %3 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="4fa4108306315193") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %4 = (%x1,);
  %5 = (%tensor_01,);
  let %x2: () = vm.invoke_tvm_op(%3, %4, %5) /* ty=() */;
  let %x3: Tensor[(3, 4), float32] = %tensor_01;
  let %storage_02: Storage[] = memory.alloc_storage(48 /* ty=int64 */, 64 /* ty=int64 */, meta[relay.attrs.AllocStorageAttrs][2]) /* ty=Storage[] */;
  let %tensor_02: Tensor[(3, 4), float32] = memory.alloc_tensor(%storage_02, 0 /* ty=int64 */, meta[relay.Constant][2] /* ty=Tensor[(2), int64] */, meta[relay.attrs.AllocTensorAttrs][2]) /* ty=Tensor[(3, 4), float32] */;
  %6 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="013033dbf7e17e61") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %7 = (%x3, %b);
  %8 = (%tensor_02,);
  let %x4: () = vm.invoke_tvm_op(%6, %7, %8) /* ty=() */;
  let %x5: Tensor[(3, 4), float32] = %tensor_02;
  %x5
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [T_add] "realize_scope" = "";
  realize(T_add, [0:3, 0:4], True {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
    T_add[floordiv(threadIdx.x, 4), floormod(threadIdx.x, 4)] = (placeholder[floordiv(threadIdx.x, 4), floormod(threadIdx.x, 4)] + placeholder_1[floordiv(threadIdx.x, 4), floormod(threadIdx.x, 4)])
  })
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectPrefetch
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [T_softmax_norm] "realize_scope" = "";
  realize(T_softmax_norm, [0:3, 0:4], True {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
    attr [T_softmax_exp: Buffer(T_softmax_exp_1: Pointer(float32), float32, [3, 4], [])] "realize_scope" = "warp";
    realize(T_softmax_exp, [blockIdx.x:(blockIdx.x + 1), 0:4], True {
      attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
      attr [T_softmax_maxelem: Buffer(T_softmax_maxelem_1: Pointer(float32), float32, [3], [])] "realize_scope" = "";
      realize(T_softmax_maxelem, [blockIdx.x:(blockIdx.x + 1)], True {
        attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
        allocate(normal_reduce_temp0, float32, [1]);
        attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
        allocate(reduce_temp0, float32, [1]) {
          normal_reduce_temp0[0] = -3.40282e+38f32
          if (threadIdx.x < 4) {
            if True {
              normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], placeholder[blockIdx.x, threadIdx.x])
            }
          }
          attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
          @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
          if True {
            T_softmax_maxelem[blockIdx.x] = (float32*)reduce_temp0[0]
          }
        }
        if @tir.likely((threadIdx.x < 4), dtype=bool) {
          T_softmax_exp[blockIdx.x, threadIdx.x] = @tir.exp((placeholder[blockIdx.x, threadIdx.x] - T_softmax_maxelem[blockIdx.x]), dtype=float32)
        }
      })
      attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
      attr [T_softmax_expsum: Buffer(T_softmax_expsum_1: Pointer(float32), float32, [3], [])] "realize_scope" = "";
      realize(T_softmax_expsum, [blockIdx.x:(blockIdx.x + 1)], True {
        attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
        allocate(normal_reduce_temp0_1, float32, [1]);
        attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
        allocate(reduce_temp0_1, float32, [1]) {
          normal_reduce_temp0_1[0] = 0f32
          if (threadIdx.x < 4) {
            if True {
              normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + T_softmax_exp[blockIdx.x, threadIdx.x])
            }
          }
          attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
          @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
          if True {
            T_softmax_expsum[blockIdx.x] = (float32*)reduce_temp0_1[0]
          }
        }
        if @tir.likely((threadIdx.x < 4), dtype=bool) {
          T_softmax_norm[blockIdx.x, threadIdx.x] = (T_softmax_exp[blockIdx.x, threadIdx.x] / T_softmax_expsum[blockIdx.x])
        }
      })
    })
  })
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageFlatten
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Promote
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16CastElimination
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16TypeLowering
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.BF16Legalize
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.NarrowDataType
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if True {
          T_softmax_maxelem[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[(blockIdx.x - blockIdx.x)]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          if True {
            normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
          }
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        if True {
          T_softmax_expsum[(blockIdx.x - blockIdx.x)] = (float32*)reduce_temp0_1[0]
        }
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[(blockIdx.x - blockIdx.x)])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if @tir.likely((threadIdx.x < 4), dtype=bool) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LoopPartition
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VectorizeLoop
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectVirtualThread
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InjectDoubleBuffer
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [1, 4]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_maxelem: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_maxelem, float32, [1]) {
      attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0, float32, [1]);
      attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        T_softmax_maxelem[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)T_softmax_maxelem[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
    attr [T_softmax_expsum: Pointer(float32)] "storage_scope" = "local";
    allocate(T_softmax_expsum, float32, [1]) {
      attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(normal_reduce_temp0_1, float32, [1]);
      attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(reduce_temp0_1, float32, [1]) {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        T_softmax_expsum[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)T_softmax_expsum[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.StorageRewrite
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        reduce_temp0[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        reduce_temp0_1[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.UnrollLoop
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        reduce_temp0[0] = (float32*)reduce_temp0[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        reduce_temp0_1[0] = (float32*)reduce_temp0_1[0]
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0[0] = -3.40282e+38f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        0
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
       {
        normal_reduce_temp0_1[0] = 0f32
        if (threadIdx.x < 4) {
          normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
        }
        attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
        0
      }
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RemoveNoOp
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.RewriteUnsafeSelect
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.HoistIfThenElse
primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder_2[((blockIdx.x*4) + threadIdx.x)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x, dtype=handle)
      if (threadIdx.x < 4) {
        T_softmax_norm_2[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:980: LOWER END

[14:33:44] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1155: CODEGEN START

[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*4) + threadIdx.x_1)] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.VerifyMemory
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*4) + threadIdx.x_1)] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*4) + threadIdx.x_1)] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.ThreadSync
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*4) + threadIdx.x_1)] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.InferFragment
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(reduce_temp0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0[0], True, reduce_temp0, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)] - (float32*)reduce_temp0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float32*)normal_reduce_temp0_1[0], True, reduce_temp0_1, threadIdx.x_1, dtype=handle)
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*4) + threadIdx.x_1)] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)reduce_temp0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerThreadAllreduce
primfn(placeholder_2: handle, placeholder_3: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [3, 4], []),
             placeholder: Buffer(placeholder_4: Pointer(float32), float32, [3, 4], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_add_1: T_add} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_2[threadIdx.x] = ((float32*)placeholder_4[threadIdx.x] + (float32*)placeholder_5[threadIdx.x])
}

primfn(placeholder_7: handle, T_softmax_norm_1: handle) -> ()
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0]}
  buffers = {T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [3, 4], []),
             placeholder_6: Buffer(placeholder_8: Pointer(float32), float32, [3, 4], [])}
  buffer_map = {placeholder_7: placeholder_6, T_softmax_norm_1: T_softmax_norm} {
  attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_8[((blockIdx.x_1*4) + threadIdx.x_1)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_2[((blockIdx.x_1*4) + threadIdx.x_1)] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)red_buf0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.MakePackedAPI
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"global_symbol": "fused_add", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
          @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
          attr [0] "compute_scope" = "fused_add_compute_";
          attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
          attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
          T_add[threadIdx.x] = ((float32*)placeholder[threadIdx.x] + (float32*)placeholder_1[threadIdx.x])
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"global_symbol": "fused_nn_softmax", "tir.noalias": True, "target": meta[Target][0], "calling_conv": 1} {
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
        @tir.tvm_call_packed("__tvm_set_device", 2, dev_id_1, dtype=int32)
        attr [0] "compute_scope" = "fused_nn_softmax_compute_";
        attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
        attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
        allocate(normal_reduce_temp0, float32, [1]);
        attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
        allocate(red_buf0, float32, [1]);
        attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
        allocate(T_softmax_exp, float32, [4]);
        attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
        allocate(normal_reduce_temp0_1, float32, [1]);
        attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
        allocate(red_buf0_1, float32, [1]) {
          attr [IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
            normal_reduce_temp0[0] = -3.40282e+38f32
            if (threadIdx.x_1 < 4) {
              normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_2[((blockIdx.x_1*4) + threadIdx.x_1)])
            }
            attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
            attr [mask: Pointer(uint32)] "storage_scope" = "local";
            allocate(mask, uint32, [1]);
            attr [t0: Pointer(float32)] "storage_scope" = "local";
            allocate(t0, float32, [1]) {
              red_buf0[0] = (float32*)normal_reduce_temp0[0]
              mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
              t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
              red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
              t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
              red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
              t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
              red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
              t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
              red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
              t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
              red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
              red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
            }
            if (threadIdx.x_1 < 4) {
              T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_2[((blockIdx.x_1*4) + threadIdx.x_1)] - (float32*)red_buf0[0]), dtype=float32)
            }
          }
          attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
            normal_reduce_temp0_1[0] = 0f32
            if (threadIdx.x_1 < 4) {
              normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
            }
            attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
            attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
            allocate(mask_1, uint32, [1]);
            attr [t0_1: Pointer(float32)] "storage_scope" = "local";
            allocate(t0_1, float32, [1]) {
              red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
              mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
              t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
              red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
              t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
              red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
              t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
              red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
              t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
              red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
              t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
              red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
              red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
            }
            if (threadIdx.x_1 < 4) {
              T_softmax_norm[((blockIdx.x_1*4) + threadIdx.x_1)] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)red_buf0_1[0])
            }
          }
        }
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.SplitHostDevice
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_add", "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
          @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
          attr [0] "compute_scope" = "fused_add_compute_";
          @tir.tvm_call_packed("fused_add_kernel0", T_add, placeholder, placeholder_1, 1, 12, dtype=int32)
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
        @tir.tvm_call_packed("__tvm_set_device", 2, dev_id_1, dtype=int32)
        attr [0] "compute_scope" = "fused_nn_softmax_compute_";
        @tir.tvm_call_packed("fused_nn_softmax_kernel0", placeholder_2, T_softmax_norm, 3, 32, dtype=int32)
      }
    }
  }
}

primfn(T_add_1: Pointer(float32), placeholder_3: Pointer(float32), placeholder_4: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add_1[threadIdx.x] = ((float32*)placeholder_3[threadIdx.x] + (float32*)placeholder_4[threadIdx.x])
}

primfn(placeholder_5: Pointer(float32), T_softmax_norm_1: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder_5[((blockIdx.x_1*4) + threadIdx.x_1)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x_1 < 4) {
        T_softmax_exp[threadIdx.x_1] = @tir.exp(((float32*)placeholder_5[((blockIdx.x_1*4) + threadIdx.x_1)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x_1, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x_1 < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x_1])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x_1 < 4) {
        T_softmax_norm_1[((blockIdx.x_1*4) + threadIdx.x_1)] = ((float32*)T_softmax_exp[threadIdx.x_1] / (float32*)red_buf0_1[0])
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_add", "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
          @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
          attr [0] "compute_scope" = "fused_add_compute_";
          @tir.tvm_call_packed("fused_add_kernel0", T_add, placeholder, placeholder_1, 1, 12, dtype=int32)
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": (nullptr), "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
        @tir.tvm_call_packed("__tvm_set_device", 2, dev_id_1, dtype=int32)
        attr [0] "compute_scope" = "fused_nn_softmax_compute_";
        @tir.tvm_call_packed("fused_nn_softmax_kernel0", placeholder_2, T_softmax_norm, 3, 32, dtype=int32)
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "calling_conv": 1} {
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  attr ["default"] "device_id" = dev_id;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
          @tir.tvm_call_packed("__tvm_set_device", 2, dev_id, dtype=int32)
          attr [0] "compute_scope" = "fused_add_compute_";
          @tir.tvm_call_packed("fused_add_kernel0", T_add, placeholder, placeholder_1, 1, 12, dtype=int32)
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  attr ["default"] "device_id" = dev_id_1;
  attr ["default"] "device_type" = 2;
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
        @tir.tvm_call_packed("__tvm_set_device", 2, dev_id_1, dtype=int32)
        attr [0] "compute_scope" = "fused_nn_softmax_compute_";
        @tir.tvm_call_packed("fused_nn_softmax_kernel0", placeholder_2, T_softmax_norm, 3, 32, dtype=int32)
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerTVMBuiltin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 6, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 6, dtype=handle)
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
           {
            @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
            stack_tcode[0] = 0
            @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
            stack_tcode[1] = 0
            @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
          }
          attr [0] "compute_scope" = "fused_add_compute_" {
            @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
            stack_tcode[0] = 3
            @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
            stack_tcode[1] = 3
            @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
            stack_tcode[2] = 3
            @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, 1), dtype=int32)
            stack_tcode[3] = 0
            @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, 12), dtype=int32)
            stack_tcode[4] = 0
            @tir.tvm_call_packed_lowered("fused_add_kernel0", stack_value, stack_tcode, 0, 5, dtype=int32)
          }
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 5, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 5, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
         {
          @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
          stack_tcode_1[0] = 0
          @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
          stack_tcode_1[1] = 0
          @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
        }
        attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
          @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
          stack_tcode_1[0] = 3
          @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
          stack_tcode_1[1] = 3
          @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, 3), dtype=int32)
          stack_tcode_1[2] = 0
          @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, 32), dtype=int32)
          stack_tcode_1[3] = 0
          @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 4, dtype=int32)
        }
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 6, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 6, dtype=handle)
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
           {
            @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
            stack_tcode[0] = 0
            @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
            stack_tcode[1] = 0
            @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
          }
          attr [0] "compute_scope" = "fused_add_compute_" {
            @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
            stack_tcode[0] = 3
            @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
            stack_tcode[1] = 3
            @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
            stack_tcode[2] = 3
            @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, 1), dtype=int32)
            stack_tcode[3] = 0
            @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, 12), dtype=int32)
            stack_tcode[4] = 0
            @tir.tvm_call_packed_lowered("fused_add_kernel0", stack_value, stack_tcode, 0, 5, dtype=int32)
          }
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 5, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 5, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
         {
          @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
          stack_tcode_1[0] = 0
          @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
          stack_tcode_1[1] = 0
          @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
        }
        attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
          @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
          stack_tcode_1[0] = 3
          @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
          stack_tcode_1[1] = 3
          @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, 3), dtype=int32)
          stack_tcode_1[2] = 0
          @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, 32), dtype=int32)
          stack_tcode_1[3] = 0
          @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 4, dtype=int32)
        }
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 6, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 6, dtype=handle)
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
           {
            @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
            stack_tcode[0] = 0
            @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
            stack_tcode[1] = 0
            @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
          }
          attr [0] "compute_scope" = "fused_add_compute_" {
            @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
            stack_tcode[0] = 3
            @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
            stack_tcode[1] = 3
            @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
            stack_tcode[2] = 3
            @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, 1), dtype=int32)
            stack_tcode[3] = 0
            @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, 12), dtype=int32)
            stack_tcode[4] = 0
            @tir.tvm_call_packed_lowered("fused_add_kernel0", stack_value, stack_tcode, 0, 5, dtype=int32)
          }
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 5, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 5, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
         {
          @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
          stack_tcode_1[0] = 0
          @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
          stack_tcode_1[1] = 0
          @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
        }
        attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
          @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
          stack_tcode_1[0] = 3
          @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
          stack_tcode_1[1] = 3
          @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, 3), dtype=int32)
          stack_tcode_1[2] = 0
          @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, 32), dtype=int32)
          stack_tcode_1[3] = 0
          @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 4, dtype=int32)
        }
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 6, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 6, dtype=handle)
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
           {
            @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
            stack_tcode[0] = 0
            @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
            stack_tcode[1] = 0
            @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
          }
          attr [0] "compute_scope" = "fused_add_compute_" {
            @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
            stack_tcode[0] = 3
            @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
            stack_tcode[1] = 3
            @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
            stack_tcode[2] = 3
            @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, 1), dtype=int32)
            stack_tcode[3] = 0
            @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, 12), dtype=int32)
            stack_tcode[4] = 0
            @tir.tvm_call_packed_lowered("fused_add_kernel0", stack_value, stack_tcode, 0, 5, dtype=int32)
          }
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 5, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 5, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
         {
          @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
          stack_tcode_1[0] = 0
          @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
          stack_tcode_1[1] = 0
          @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
        }
        attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
          @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
          stack_tcode_1[0] = 3
          @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
          stack_tcode_1[1] = 3
          @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, 3), dtype=int32)
          stack_tcode_1[2] = 0
          @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, 32), dtype=int32)
          stack_tcode_1[3] = 0
          @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 4, dtype=int32)
        }
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.CombineContextCall
primfn(args: handle, arg_type_ids: handle, num_args: int32, out_ret_value: handle, out_ret_tcode: handle, resource_handle: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_add", "calling_conv": 1} {
  let stack_tcode: handle = @tir.tvm_stack_alloca("arg_tcode", 6, dtype=handle)
  let stack_value: handle = @tir.tvm_stack_alloca("arg_value", 6, dtype=handle)
  assert((num_args == 3), "fused_add: num_args should be 3")
  let arg0: handle = @tir.tvm_struct_get(args, 0, 12, dtype=handle)
  let arg0.code: int32 = (int32*)arg_type_ids[0]
  let arg1: handle = @tir.tvm_struct_get(args, 1, 12, dtype=handle)
  let arg1.code: int32 = (int32*)arg_type_ids[1]
  let arg2: handle = @tir.tvm_struct_get(args, 2, 12, dtype=handle)
  let arg2.code: int32 = (int32*)arg_type_ids[2]
  let placeholder: Pointer(float32) = @tir.tvm_struct_get(arg0, 0, 1, dtype=handle)
  attr [placeholder] "storage_alignment" = 128;
  let arg0.shape: handle = @tir.tvm_struct_get(arg0, 0, 2, dtype=handle)
  let arg0.strides: handle = @tir.tvm_struct_get(arg0, 0, 3, dtype=handle)
  let dev_id: int32 = @tir.tvm_struct_get(arg0, 0, 9, dtype=int32)
  let placeholder_1: Pointer(float32) = @tir.tvm_struct_get(arg1, 0, 1, dtype=handle)
  attr [placeholder_1] "storage_alignment" = 128;
  let arg1.shape: handle = @tir.tvm_struct_get(arg1, 0, 2, dtype=handle)
  let arg1.strides: handle = @tir.tvm_struct_get(arg1, 0, 3, dtype=handle)
  let T_add: Pointer(float32) = @tir.tvm_struct_get(arg2, 0, 1, dtype=handle)
  attr [T_add] "storage_alignment" = 128;
  let arg2.shape: handle = @tir.tvm_struct_get(arg2, 0, 2, dtype=handle)
  let arg2.strides: handle = @tir.tvm_struct_get(arg2, 0, 3, dtype=handle)
  assert(((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), "fused_add: Expect arg[0] to be pointer")
  assert(((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), "fused_add: Expect arg[1] to be pointer")
  assert(((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), "fused_add: Expect arg[2] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides[1])) && (4 == cast(int32, (int64*)arg0.strides[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides[1])) && (4 == cast(int32, (int64*)arg1.strides[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id == @tir.tvm_struct_get(arg1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((2 == @tir.tvm_struct_get(arg2, 0, 4, dtype=int32)), "arg2.ndim is expected to equal 2")
      assert((((@tir.tvm_struct_get(arg2, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg2, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg2, 0, 7, dtype=uint16) == 1u16)), "arg2.dtype is expected to be float32")
      assert((3 == cast(int32, (int64*)arg2.shape[0])), "Argument arg2.shape[0] has an unsatisfied constraint: (3 == int32(arg2.shape[0]))")
      assert((4 == cast(int32, (int64*)arg2.shape[1])), "Argument arg2.shape[1] has an unsatisfied constraint: (4 == int32(arg2.shape[1]))")
       {
        if !@tir.isnullptr(arg2.strides, dtype=bool) {
          assert(((1 == cast(int32, (int64*)arg2.strides[1])) && (4 == cast(int32, (int64*)arg2.strides[0]))), "arg2.strides: expected to be compact array")
          0
        }
        assert((0u64 == @tir.tvm_struct_get(arg2, 0, 8, dtype=uint64)), "Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))")
        assert((2 == @tir.tvm_struct_get(arg2, 0, 10, dtype=int32)), "Argument arg2.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg2, 0, 10))")
        assert((dev_id == @tir.tvm_struct_get(arg2, 0, 9, dtype=int32)), "Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))")
         {
           {
            @tir.tvm_struct_set(stack_value, 0, 12, cast(int64, 2), dtype=int32)
            stack_tcode[0] = 0
            @tir.tvm_struct_set(stack_value, 1, 12, cast(int64, dev_id), dtype=int32)
            stack_tcode[1] = 0
            @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2, dtype=int32)
          }
          attr [0] "compute_scope" = "fused_add_compute_" {
            @tir.tvm_struct_set(stack_value, 0, 12, T_add, dtype=int32)
            stack_tcode[0] = 3
            @tir.tvm_struct_set(stack_value, 1, 12, placeholder, dtype=int32)
            stack_tcode[1] = 3
            @tir.tvm_struct_set(stack_value, 2, 12, placeholder_1, dtype=int32)
            stack_tcode[2] = 3
            @tir.tvm_struct_set(stack_value, 3, 12, cast(int64, 1), dtype=int32)
            stack_tcode[3] = 0
            @tir.tvm_struct_set(stack_value, 4, 12, cast(int64, 12), dtype=int32)
            stack_tcode[4] = 0
            @tir.tvm_call_packed_lowered("fused_add_kernel0", stack_value, stack_tcode, 0, 5, dtype=int32)
          }
        }
      }
    }
  }
}

primfn(args_1: handle, arg_type_ids_1: handle, num_args_1: int32, out_ret_value_1: handle, out_ret_tcode_1: handle, resource_handle_1: handle) -> int32
  attr = {"target": meta[Target][0], "tir.noalias": True, "global_symbol": "fused_nn_softmax", "calling_conv": 1} {
  let stack_tcode_1: handle = @tir.tvm_stack_alloca("arg_tcode", 5, dtype=handle)
  let stack_value_1: handle = @tir.tvm_stack_alloca("arg_value", 5, dtype=handle)
  assert((num_args_1 == 2), "fused_nn_softmax: num_args should be 2")
  let arg0_1: handle = @tir.tvm_struct_get(args_1, 0, 12, dtype=handle)
  let arg0.code_1: int32 = (int32*)arg_type_ids_1[0]
  let arg1_1: handle = @tir.tvm_struct_get(args_1, 1, 12, dtype=handle)
  let arg1.code_1: int32 = (int32*)arg_type_ids_1[1]
  let placeholder_2: Pointer(float32) = @tir.tvm_struct_get(arg0_1, 0, 1, dtype=handle)
  attr [placeholder_2] "storage_alignment" = 128;
  let arg0.shape_1: handle = @tir.tvm_struct_get(arg0_1, 0, 2, dtype=handle)
  let arg0.strides_1: handle = @tir.tvm_struct_get(arg0_1, 0, 3, dtype=handle)
  let dev_id_1: int32 = @tir.tvm_struct_get(arg0_1, 0, 9, dtype=int32)
  let T_softmax_norm: Pointer(float32) = @tir.tvm_struct_get(arg1_1, 0, 1, dtype=handle)
  attr [T_softmax_norm] "storage_alignment" = 128;
  let arg1.shape_1: handle = @tir.tvm_struct_get(arg1_1, 0, 2, dtype=handle)
  let arg1.strides_1: handle = @tir.tvm_struct_get(arg1_1, 0, 3, dtype=handle)
  assert(((((arg0.code_1 == 3) || (arg0.code_1 == 13)) || (arg0.code_1 == 7)) || (arg0.code_1 == 4)), "fused_nn_softmax: Expect arg[0] to be pointer")
  assert(((((arg1.code_1 == 3) || (arg1.code_1 == 13)) || (arg1.code_1 == 7)) || (arg1.code_1 == 4)), "fused_nn_softmax: Expect arg[1] to be pointer")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((2 == @tir.tvm_struct_get(arg0_1, 0, 4, dtype=int32)), "arg0.ndim is expected to equal 2")
  assert((((@tir.tvm_struct_get(arg0_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg0_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg0_1, 0, 7, dtype=uint16) == 1u16)), "arg0.dtype is expected to be float32")
  assert((3 == cast(int32, (int64*)arg0.shape_1[0])), "Argument arg0.shape[0] has an unsatisfied constraint: (3 == int32(arg0.shape[0]))")
  assert((4 == cast(int32, (int64*)arg0.shape_1[1])), "Argument arg0.shape[1] has an unsatisfied constraint: (4 == int32(arg0.shape[1]))")
   {
    if !@tir.isnullptr(arg0.strides_1, dtype=bool) {
      assert(((1 == cast(int32, (int64*)arg0.strides_1[1])) && (4 == cast(int32, (int64*)arg0.strides_1[0]))), "arg0.strides: expected to be compact array")
      0
    }
    assert((0u64 == @tir.tvm_struct_get(arg0_1, 0, 8, dtype=uint64)), "Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))")
    assert((2 == @tir.tvm_struct_get(arg0_1, 0, 10, dtype=int32)), "Argument arg0.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg0, 0, 10))")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((2 == @tir.tvm_struct_get(arg1_1, 0, 4, dtype=int32)), "arg1.ndim is expected to equal 2")
    assert((((@tir.tvm_struct_get(arg1_1, 0, 5, dtype=uint8) == 2u8) && (@tir.tvm_struct_get(arg1_1, 0, 6, dtype=uint8) == 32u8)) && (@tir.tvm_struct_get(arg1_1, 0, 7, dtype=uint16) == 1u16)), "arg1.dtype is expected to be float32")
    assert((3 == cast(int32, (int64*)arg1.shape_1[0])), "Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))")
    assert((4 == cast(int32, (int64*)arg1.shape_1[1])), "Argument arg1.shape[1] has an unsatisfied constraint: (4 == int32(arg1.shape[1]))")
     {
      if !@tir.isnullptr(arg1.strides_1, dtype=bool) {
        assert(((1 == cast(int32, (int64*)arg1.strides_1[1])) && (4 == cast(int32, (int64*)arg1.strides_1[0]))), "arg1.strides: expected to be compact array")
        0
      }
      assert((0u64 == @tir.tvm_struct_get(arg1_1, 0, 8, dtype=uint64)), "Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))")
      assert((2 == @tir.tvm_struct_get(arg1_1, 0, 10, dtype=int32)), "Argument arg1.device_type has an unsatisfied constraint: (2 == tir.tvm_struct_get(arg1, 0, 10))")
      assert((dev_id_1 == @tir.tvm_struct_get(arg1_1, 0, 9, dtype=int32)), "Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))")
       {
         {
          @tir.tvm_struct_set(stack_value_1, 0, 12, cast(int64, 2), dtype=int32)
          stack_tcode_1[0] = 0
          @tir.tvm_struct_set(stack_value_1, 1, 12, cast(int64, dev_id_1), dtype=int32)
          stack_tcode_1[1] = 0
          @tir.tvm_call_packed_lowered("__tvm_set_device", stack_value_1, stack_tcode_1, 0, 2, dtype=int32)
        }
        attr [0] "compute_scope" = "fused_nn_softmax_compute_" {
          @tir.tvm_struct_set(stack_value_1, 0, 12, placeholder_2, dtype=int32)
          stack_tcode_1[0] = 3
          @tir.tvm_struct_set(stack_value_1, 1, 12, T_softmax_norm, dtype=int32)
          stack_tcode_1[1] = 3
          @tir.tvm_struct_set(stack_value_1, 2, 12, cast(int64, 3), dtype=int32)
          stack_tcode_1[2] = 0
          @tir.tvm_struct_set(stack_value_1, 3, 12, cast(int64, 32), dtype=int32)
          stack_tcode_1[3] = 0
          @tir.tvm_call_packed_lowered("fused_nn_softmax_kernel0", stack_value_1, stack_tcode_1, 0, 4, dtype=int32)
        }
      }
    }
  }
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass Filter
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder[((blockIdx.x*4) + threadIdx.x)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add[threadIdx.x_1] = ((float32*)placeholder_1[threadIdx.x_1] + (float32*)placeholder_2[threadIdx.x_1])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass BindTarget
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "warp";
  allocate(T_softmax_exp, float32, [4]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[threadIdx.x] = @tir.exp(((float32*)placeholder[((blockIdx.x*4) + threadIdx.x)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + (float32*)T_softmax_exp[threadIdx.x])
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*4) + threadIdx.x)] = ((float32*)T_softmax_exp[threadIdx.x] / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add[threadIdx.x_1] = ((float32*)placeholder_1[threadIdx.x_1] + (float32*)placeholder_2[threadIdx.x_1])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerWarpMemory
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.exp(((float32*)placeholder[((blockIdx.x*4) + threadIdx.x)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*4) + threadIdx.x)] = (@tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add[threadIdx.x_1] = ((float32*)placeholder_1[threadIdx.x_1] + (float32*)placeholder_2[threadIdx.x_1])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.Simplify
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.exp(((float32*)placeholder[((blockIdx.x*4) + threadIdx.x)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*4) + threadIdx.x)] = (@tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add[threadIdx.x_1] = ((float32*)placeholder_1[threadIdx.x_1] + (float32*)placeholder_2[threadIdx.x_1])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerCustomDatatypes
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.tvm_warp_shuffle_down((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.tvm_warp_shuffle((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.exp(((float32*)placeholder[((blockIdx.x*4) + threadIdx.x)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.tvm_warp_activemask(, dtype=uint32)
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.tvm_warp_shuffle_down((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.tvm_warp_shuffle((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*4) + threadIdx.x)] = (@tir.tvm_warp_shuffle(@tir.tvm_warp_activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add[threadIdx.x_1] = ((float32*)placeholder_1[threadIdx.x_1] + (float32*)placeholder_2[threadIdx.x_1])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerIntrin
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.cuda.__activemask(, dtype=uint32)
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.cuda.__shfl_sync((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.call_pure_extern("__expf", ((float32*)placeholder[((blockIdx.x*4) + threadIdx.x)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.cuda.__shfl_sync(@tir.cuda.__activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.cuda.__activemask(, dtype=uint32)
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.cuda.__shfl_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*4) + threadIdx.x)] = (@tir.cuda.__shfl_sync(@tir.cuda.__activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add[threadIdx.x_1] = ((float32*)placeholder_1[threadIdx.x_1] + (float32*)placeholder_2[threadIdx.x_1])
}


[14:33:44] /workspace/home/codes/tvm/src/ir/transform.cc:541: After pass tir.LowerDeviceStorageAccessInfo
primfn(placeholder: Pointer(float32), T_softmax_norm: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_nn_softmax_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 3;
  attr [normal_reduce_temp0: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0, float32, [1]);
  attr [red_buf0: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0, float32, [1]);
  attr [T_softmax_exp: Pointer(float32)] "storage_scope" = "local";
  allocate(T_softmax_exp, float32, [1]);
  attr [normal_reduce_temp0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(normal_reduce_temp0_1, float32, [1]);
  attr [red_buf0_1: Pointer(float32)] "storage_scope" = "local";
  allocate(red_buf0_1, float32, [1]) {
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0[0] = -3.40282e+38f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0[0] = max((float32*)normal_reduce_temp0[0], (float32*)placeholder[((blockIdx.x*4) + threadIdx.x)])
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask, uint32, [1]);
      attr [t0: Pointer(float32)] "storage_scope" = "local";
      allocate(t0, float32, [1]) {
        red_buf0[0] = (float32*)normal_reduce_temp0[0]
        mask[0] = @tir.cuda.__activemask(, dtype=uint32)
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 16, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 8, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 4, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 2, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        t0[0] = @tir.cuda.__shfl_down_sync((uint32*)mask[0], (float32*)red_buf0[0], 1, 32, dtype=float32)
        red_buf0[0] = max((float32*)red_buf0[0], (float32*)t0[0])
        red_buf0[0] = @tir.cuda.__shfl_sync((uint32*)mask[0], (float32*)red_buf0[0], 0, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_exp[0] = @tir.call_pure_extern("__expf", ((float32*)placeholder[((blockIdx.x*4) + threadIdx.x)] - (float32*)red_buf0[0]), dtype=float32)
      }
    }
    attr [IterVar(threadIdx.x, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1[0] = 0f32
      if (threadIdx.x < 4) {
        normal_reduce_temp0_1[0] = ((float32*)normal_reduce_temp0_1[0] + @tir.cuda.__shfl_sync(@tir.cuda.__activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, dtype=float32))
      }
      attr [meta[tir.CommReducer][1]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      attr [mask_1: Pointer(uint32)] "storage_scope" = "local";
      allocate(mask_1, uint32, [1]);
      attr [t0_1: Pointer(float32)] "storage_scope" = "local";
      allocate(t0_1, float32, [1]) {
        red_buf0_1[0] = (float32*)normal_reduce_temp0_1[0]
        mask_1[0] = @tir.cuda.__activemask(, dtype=uint32)
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 16, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 8, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 4, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 2, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        t0_1[0] = @tir.cuda.__shfl_down_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 1, 32, dtype=float32)
        red_buf0_1[0] = ((float32*)red_buf0_1[0] + (float32*)t0_1[0])
        red_buf0_1[0] = @tir.cuda.__shfl_sync((uint32*)mask_1[0], (float32*)red_buf0_1[0], 0, 32, dtype=float32)
      }
      if (threadIdx.x < 4) {
        T_softmax_norm[((blockIdx.x*4) + threadIdx.x)] = (@tir.cuda.__shfl_sync(@tir.cuda.__activemask(, dtype=uint32), (float32*)T_softmax_exp[0], threadIdx.x, 32, dtype=float32) / (float32*)red_buf0_1[0])
      }
    }
  }
}

primfn(T_add: Pointer(float32), placeholder_1: Pointer(float32), placeholder_2: Pointer(float32)) -> ()
  attr = {"target": meta[Target][0], "tir.noalias": 1, "global_symbol": "fused_add_kernel0", "tir.device_thread_axis": [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x"), IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")], "calling_conv": 2} {
  attr [IterVar(blockIdx.x_1, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 12;
  T_add[threadIdx.x_1] = ((float32*)placeholder_1[threadIdx.x_1] + (float32*)placeholder_2[threadIdx.x_1])
}


[14:33:44] /workspace/home/codes/tvm/src/relay/backend/vm/compiler.cc:1203: CODEGEN END

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32]) {
  %0 = add(%a, %b);
  %1 = nn.softmax(%0);
  add(%1, %b)
}

def @main(%a: Tensor[(3, 4), float32], %b: Tensor[(3, 4), float32], hash="cb4686cacacc8e04") -> Tensor[(3, 4), float32] {
  %0 = fn (%p02: Tensor[(3, 4), float32], %p11: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p02, %p11) /* ty=Tensor[(3, 4), float32] */
  };
  %1 = %0(%a, %b) /* ty=Tensor[(3, 4), float32] */;
  %2 = fn (%p01: Tensor[(3, 4), float32], Primitive=1, hash="88e813b511b22ea3") -> Tensor[(3, 4), float32] {
    nn.softmax(%p01) /* ty=Tensor[(3, 4), float32] */
  };
  %3 = %2(%1) /* ty=Tensor[(3, 4), float32] */;
  %4 = fn (%p0: Tensor[(3, 4), float32], %p1: Tensor[(3, 4), float32], Primitive=1, hash="be696fa1cec00415") -> Tensor[(3, 4), float32] {
    add(%p0, %p1) /* ty=Tensor[(3, 4), float32] */
  };
  %4(%3, %b) /* ty=Tensor[(3, 4), float32] */
}

VM Function[0]: main(a, b)
# reg file size = 14
# instruction count = 16
opcode, fields # inst(text):
 0: 11 0 2   # load_const $2 Const[0]
 1: 16 2 64 2 32 1 2 3   # alloc_storage $3 $2 64 float32 2
 2: 11 1 4   # load_const $4 Const[1]
 3: 5 3 4 2 32 1 2 5 3 4   # alloc_tensor $5 $3 $4 [3, 4] float32
 4: 4 0 3 1 0 1 5   # invoke_packed PackedFunc[0] (in: $0, $1, out: $5)
 5: 11 2 6   # load_const $6 Const[2]
 6: 16 6 64 2 32 1 2 7   # alloc_storage $7 $6 64 float32 2
 7: 11 3 8   # load_const $8 Const[3]
 8: 5 7 8 2 32 1 2 9 3 4   # alloc_tensor $9 $7 $8 [3, 4] float32
 9: 4 1 2 1 5 9   # invoke_packed PackedFunc[1] (in: $5, out: $9)
10: 11 4 10   # load_const $10 Const[4]
11: 16 10 64 2 32 1 2 11   # alloc_storage $11 $10 64 float32 2
12: 11 5 12   # load_const $12 Const[5]
13: 5 11 12 2 32 1 2 13 3 4   # alloc_tensor $13 $11 $12 [3, 4] float32
14: 4 0 3 1 9 1 13   # invoke_packed PackedFunc[0] (in: $9, $1, out: $13)
15: 1 13   # ret $13


Relay VM executable statistics:
  Constant shapes (# 6): [scalar, scalar, scalar, scalar, scalar, scalar]
  Globals (#1): [("main", 0)]
  Primitive ops (#2): [fused_add, fused_nn_softmax]

[[-1.3589563   1.2950816   0.56617177  0.6727376 ]
 [-1.7214141   2.4560935  -0.9257527  -0.6259286 ]
 [-0.45445174  0.23691672 -0.32476252  0.8503547 ]]
